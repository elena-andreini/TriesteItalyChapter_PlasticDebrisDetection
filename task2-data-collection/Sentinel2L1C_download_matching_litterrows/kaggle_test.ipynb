{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing Multisprectral Satellite Imagery from Copernicus Data Space Ecosystem CDCE via API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sentinel Hub API is a RESTful API interface that provides access to various satellite imagery archives. It allows you to access raw satellite data, rendered images, statistical analysis, and other features. \n",
    "\n",
    "To use the features in this notebook you need to visit https://dataspace.copernicus.eu and create an account with Copernicus, the official governing body of Sentinel Missions for the European Space Agency (ESA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /Users/sara_mac/.kaggle/kaggle.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sara_mac/Desktop/projects/plastic_detection/Sentinel2PlasticDetectionProject/omdena/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import kaggle\n",
    "import json\n",
    "\n",
    "\n",
    "from sentinelhub import (SHConfig,\n",
    "    DataCollection,\n",
    "    SentinelHubCatalog,\n",
    "    SentinelHubRequest,\n",
    "    SentinelHubStatistical,\n",
    "    BBox,\n",
    "    bbox_to_dimensions,\n",
    "    CRS,\n",
    "    MimeType,\n",
    "    Geometry,\n",
    ")\n",
    "from utils import plot_image\n",
    "from dotenv import load_dotenv\n",
    "import requests_oauthlib as requests\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime \n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIRST TIME Credentials\n",
    "\n",
    "Run the following cells the first time you run this notebook after retrieving your credentials for CDSE from their website. The `client_id` & `client_secret` can be obtained in your [Dashboard](https://shapps.dataspace.copernicus.eu/dashboard/#/). In the User Settings you can create a new OAuth Client to generate these credentials. For more detailed instructions, visit the relevant [documentation page](https://documentation.dataspace.copernicus.eu/APIs/SentinelHub/Overview/Authentication.html).\n",
    "\n",
    "Now that you have your `client_id` & `client_secret`, save them in the .env file in the same directory as this script, \n",
    "by simply inserting the following lines:\n",
    "\n",
    "CLIENT_ID= \"your_client_id\"\n",
    "CLIENT_SECRET= \"your_client_secret\"\n",
    "\n",
    "For privacy and security please make sure that \".env\" is included in your .gitignore file\n",
    "Once this is all done, you can proceed from here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() \n",
    "\n",
    "config = SHConfig(use_defaults=True)\n",
    "config.sh_client_id = os.getenv(\"CLIENT_ID\")\n",
    "config.sh_client_secret = os.getenv(\"CLIENT_SECRET\")\n",
    "config.sh_token_url = os.getenv(\"TOKEN_URL\")\n",
    "config.sh_base_url = os.getenv(\"BASE_URL\")\n",
    "config.save(\"cdse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the complete config has also been stored on your local machine for future use,\n",
    "this ensures everything is set correctly and that you can use your cedentials with other \n",
    "virtual machines for as long as your OAuth token is valid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/sara_mac/.config/sentinelhub/config.toml'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SHConfig.get_config_location()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workflow works with credentials and configurations specific to the free CDSE platform, not Sentinel Hub, which requires a subscription after a 30 day free trial. \n",
    "Instructions on how to configure your Sentinel Hub Python package instead can be found [here](https://sentinelhub-py.readthedocs.io/en/latest/configure.html). Using these instructions you can create a profile specific to using the Sentinel Hub package for accessing Copernicus Data Space Ecosystem data collections, if you wish to do so. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RETRIEVE PREVIOUS Credentials\n",
    "skip directly to this is you have already set up the credentials in previous cells before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SHConfig(\"cdse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\n",
      "https://sh.dataspace.copernicus.eu\n"
     ]
    }
   ],
   "source": [
    "# check that the credentials are set correctly \n",
    "if not config.sh_client_id or not config.sh_client_secret:\n",
    "    print(\"Please provide your Sentinel Hub credentials in the .env file.\")\n",
    "    exit(1)\n",
    "\n",
    "# check that the credentials are what you expect (i.e. output is the same & not None)\n",
    "# NOTE: you can also set the credentials directly in the code, \n",
    "# but this is not recommended for security reasons.\n",
    "# do not print the secret credentials 'id' or 'secret' in a public notebook for security reasons either\n",
    "print(config.sh_token_url)\n",
    "print(config.sh_base_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed bounding boxes selected for 3 Areas of Interest\n",
    "\n",
    "Do not change the values for the selected AOIs, but add others if required.\n",
    "Make sure the aoi_size is less than 2500 pixels as the maximum image length/width to request. If larger areas are needed a mosaics approach needs to be implemented and can be added at a later stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape for Po River Plume at 10 m resolution: (2049, 2496) pixels\n"
     ]
    }
   ],
   "source": [
    "# Po River \n",
    "resolution = 10\n",
    "AOI1 = 'Po River Plume'\n",
    "aoi_prp =[12.45, 44.825, 12.7, 45.055]  \n",
    "\n",
    "aoi1_bbox = BBox(bbox=aoi_prp, crs=CRS.WGS84)\n",
    "aoi1_size = bbox_to_dimensions(aoi1_bbox, resolution=resolution)\n",
    "print(f\"Image shape for {AOI1} at {resolution} m resolution: {aoi1_size} pixels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape for North East Corsica at 10 m resolution: (2423, 2298) pixels\n"
     ]
    }
   ],
   "source": [
    "# Corsica\n",
    "resolution = 10\n",
    "AOI2 = 'North East Corsica'\n",
    "aoi2_nec =[9.6, 42.95, 9.9, 43.155]  \n",
    "\n",
    "aoi2_bbox = BBox(bbox=aoi2_nec, crs=CRS.WGS84)\n",
    "aoi2_size = bbox_to_dimensions(aoi2_bbox, resolution=resolution)\n",
    "print(f\"Image shape for {AOI2} at {resolution} m resolution: {aoi2_size} pixels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape for South East Calabria at 10m resolution: (2185, 2314) pixels\n"
     ]
    }
   ],
   "source": [
    "# Calabria\n",
    "resolution = 10\n",
    "AOI3 = 'South East Calabria'\n",
    "aoi3_sec =[16.5, 38.35, 16.755, 38.555]  \n",
    "\n",
    "aoi3_bbox = BBox(bbox=aoi3_sec, crs=CRS.WGS84)\n",
    "aoi3_size = bbox_to_dimensions(aoi3_bbox, resolution=resolution)\n",
    "print(f\"Image shape for {AOI3} at {resolution}m resolution: {aoi3_size} pixels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download workflow \n",
    "- Match full Sentinel Catalog for S2_L1C to AOI & time period (adjust as needed, but within litterrow time period, i.e. between 01/07/2015 - 31/08/2021)\n",
    "- Limit results to images with identified litter windrows\n",
    "- define evalscript with bands and images to download\n",
    "- Iterate through images from filtered results (i.e. with litter rows) and save them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limit Catalog to products with identified & annotated litter rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = SentinelHubCatalog(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of results: 48\n"
     ]
    }
   ],
   "source": [
    "# Retrieve images from the Sentinel Hub Catalog for the specified AOI\n",
    "# and time interval in representative batches\n",
    "\n",
    "aoi_bbox = BBox(bbox=aoi_prp, crs=CRS.WGS84) # switch for aoi_nec or aoi_sec as needed\n",
    "time_interval =  \"2019-07-01\", \"2019-07-31\" # adjust dates as needed with max period:\n",
    "# \"2015-07-01\", \"2021-08-31\" -- total images: 3026 for prp, 754 for nec, 378 for sec\n",
    "\n",
    "search_iterator = catalog.search(\n",
    "    DataCollection.SENTINEL2_L1C,\n",
    "    bbox=aoi_bbox,\n",
    "    time=time_interval,\n",
    "    fields={\"include\": [\"id\", \"properties.datetime\"], \"exclude\": []},\n",
    ")\n",
    "\n",
    "results = list(search_iterator)\n",
    "print(\"Total number of results:\", len(results))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if any of the id's match images with positive pixels listed in LM_centroids.xlxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching IDs: ['S2A_MSIL1C_20190730T100031_N0500_R122_T32TQQ_20230710T202111.SAFE', 'S2A_MSIL1C_20190730T100031_N0500_R122_T33TUK_20230710T202111.SAFE', 'S2A_MSIL1C_20190730T100031_N0500_R122_T32TQR_20230710T202111.SAFE', 'S2A_MSIL1C_20190730T100031_N0500_R122_T33TUL_20230710T202111.SAFE', 'S2B_MSIL1C_20190725T100039_N0500_R122_T32TQQ_20230619T024542.SAFE', 'S2B_MSIL1C_20190725T100039_N0500_R122_T33TUK_20230619T024542.SAFE', 'S2B_MSIL1C_20190725T100039_N0500_R122_T32TQR_20230619T024542.SAFE', 'S2B_MSIL1C_20190725T100039_N0500_R122_T33TUL_20230619T024542.SAFE', 'S2A_MSIL1C_20190723T101031_N0500_R022_T32TQQ_20230718T015529.SAFE', 'S2A_MSIL1C_20190723T101031_N0500_R022_T33TUK_20230718T015529.SAFE', 'S2A_MSIL1C_20190723T101031_N0500_R022_T32TQR_20230718T015529.SAFE', 'S2A_MSIL1C_20190723T101031_N0500_R022_T33TUL_20230718T015529.SAFE', 'S2A_MSIL1C_20190720T100031_N0500_R122_T32TQQ_20230715T235221.SAFE', 'S2A_MSIL1C_20190720T100031_N0500_R122_T33TUK_20230715T235221.SAFE', 'S2A_MSIL1C_20190720T100031_N0500_R122_T32TQR_20230715T235221.SAFE', 'S2A_MSIL1C_20190720T100031_N0500_R122_T33TUL_20230715T235221.SAFE', 'S2B_MSIL1C_20190705T100039_N0500_R122_T32TQQ_20230718T022116.SAFE', 'S2B_MSIL1C_20190705T100039_N0500_R122_T33TUK_20230718T022116.SAFE', 'S2B_MSIL1C_20190705T100039_N0500_R122_T32TQR_20230718T022116.SAFE', 'S2B_MSIL1C_20190705T100039_N0500_R122_T33TUL_20230718T022116.SAFE', 'S2A_MSIL1C_20190703T101031_N0500_R022_T32TQQ_20230717T022229.SAFE', 'S2A_MSIL1C_20190703T101031_N0500_R022_T33TUK_20230717T022229.SAFE', 'S2A_MSIL1C_20190703T101031_N0500_R022_T32TQR_20230717T022229.SAFE', 'S2A_MSIL1C_20190703T101031_N0500_R022_T33TUL_20230717T022229.SAFE']\n"
     ]
    }
   ],
   "source": [
    "# Function to match LW source data to the image ID \n",
    "# Iterate through the results and check for matches by removing S2A_MSIL1C_ \n",
    "# from the 'id' and keeping only the datestr and codeT which are\n",
    "# listed in the LM_centroids.xlsx file based on S2L1C naming convention: \n",
    "# S2A_MSIL1C__YYYYMMDDTXXXXXX....\n",
    "\n",
    "def check_matching_ids(results, lm_centroids_path):\n",
    "    \n",
    "    lm_centroids = pd.read_excel(lm_centroids_path)\n",
    "    if 'Str_time' not in lm_centroids.columns:\n",
    "        raise ValueError(\"The column 'Str_time' is not found in the provided Excel file.\")\n",
    "    \n",
    "    str_time_set = set(lm_centroids['Str_time'])\n",
    "    \n",
    "    matching_ids = []\n",
    "    for result in results:\n",
    "        trimmed_id = result['id'][11:26]\n",
    "        if trimmed_id in str_time_set:\n",
    "            matching_ids.append(result['id'])\n",
    "    \n",
    "    return matching_ids\n",
    "\n",
    "\n",
    "lm_centroids_path = \"../LM_centroids.xlsx\" \n",
    "matching_ids = check_matching_ids(results, lm_centroids_path)\n",
    "print(\"Matching IDs:\", matching_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 matching images found between 2019-07-01 and 2019-07-31 for Po River Plume.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(matching_ids)} matching images found between {time_interval[0]} and {time_interval[1]} for {AOI1}.\") \n",
    "# exchange for AOI2, AOI3, as req."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter images needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_results = [\n",
    "    {key: record[key] for key in record if key in [\"id\", \"properties\"]}\n",
    "    for record in results if record[\"id\"] in matching_ids\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Eval script of bands for download and how to combine the returned outputs \n",
    "(i.e. RGB image with invisible bands, FDI, NVDI, or custom false colour images)\n",
    "This evalscript includes only four bands for workflow trial, but can be extended to include all bands available, as well as calculated indexes such as NVDI etc. More documentation on the components and how to write/edit eval scripts can be found in the [doumentation](https://documentation.dataspace.copernicus.eu/APIs/SentinelHub/Evalscript/V3.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on Booth et al. (2023) (who used Marida + PLP) the following 4 bands were selected:<br>\n",
    "bands: [\"B04\", \"B06\", \"B08\", \"B11\"] , i.e. // red, red edge + NIR + SWIR\n",
    "<br>\n",
    "The authors reported the Map Mapper-Opt \"model trained with only 4 bands (the ones contributing to calculating FDI and NDVI) demonstrated good (better) performance than using all 13 Sentinel-2 bands. However, it is possible that other band combinations could improve model performance. Removing some of the lower resolution bands, as well as bands where wavelengths do not correlate with plastic materials, may reduce noise in the data set.\"\n",
    "\n",
    "Complete list of bands that can be selected from L1C are:\n",
    "[\"B01\", \"B03\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B08a\", \"B09\", \"B10, \"B11, \"B12\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the eval script the function evaluatePixel(sample) return value determines how data from the various multi spectral index (MSI) bands are handled. In this case it simple returns a list with the seperate values for each of the four selected bands. To combine red, green & blue(RGB) (B04, B03, B02) for a real 'true color' composites, these would be combined within the funstions return section of the code. \n",
    "The same goes for the calculations/combinations used to get the Normalized Difference Vegetation Index (NDVI) and/or the Floating Debris Index (FDI). Code snippets to get these indexes & combinations will be included shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 24 requests for true color images.\n"
     ]
    }
   ],
   "source": [
    "evalscript_true_color = \"\"\"\n",
    "//VERSION=3\n",
    "\n",
    "function setup() {\n",
    "    return {\n",
    "        input: [{\n",
    "            bands: [\"B04\", \"B06\", \"B08\", \"B11\"] \n",
    "        }],\n",
    "        output: {\n",
    "            bands: 4\n",
    "        }\n",
    "    };\n",
    "}\n",
    "\n",
    "function evaluatePixel(sample) {\n",
    "    return [sample.B04, sample.B06, sample.B08, sample.B11]; // R + RE + NIR + SWIR in seperate files\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# to change outputs saved to real true colour composites, false composites or indexes, such as NVDI & FDI \n",
    "# the function evaluatePixel(sample) return value needs to be adjusted. Pre-set evals scripts will be added for each option shortly \n",
    "# Generate requests for each matching ID\n",
    "requests_true_color = []\n",
    "for matching_id in matching_ids:\n",
    "    request = SentinelHubRequest(\n",
    "        evalscript=evalscript_true_color,\n",
    "        input_data=[\n",
    "            SentinelHubRequest.input_data(\n",
    "                data_collection=DataCollection.SENTINEL2_L1C,\n",
    "                identifier=matching_id,\n",
    "            time_interval=time_interval,\n",
    "            other_args={\"dataFilter\": {\"mosaickingOrder\": \"leastCC\"}},\n",
    "            )\n",
    "        ],\n",
    "        responses=[SentinelHubRequest.output_response(\"default\", MimeType.TIFF)],\n",
    "        bbox=aoi_bbox, #e xchange for aoi2_bbox for Corsica , aoi3_bbox for Calabria as req.\n",
    "        size=aoi1_size, # exchange for aoi2_size for Corsica , aoi3_size for Calabria as req.\n",
    "        config=config,\n",
    "    )\n",
    "    requests_true_color.append(request)\n",
    "\n",
    "print(f\"Generated {len(requests_true_color)} requests for true color images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate through the Sentinel Products identified and save them to kaggle hub [in progress...]\n",
    "### use this for batch downloads, i.e. to expand the dataset spatially & temporally - more months/other AOIs\n",
    "\n",
    "This option now saves the retrieves Sentinal bands and eval script outputs directly to a central kaggle Dataset, which can be used as the input for other notebooks in the project (satellite correction and various models people are testing/evaluating), as well as for future use by the public"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a folder for the current batch of images you are downloading\n",
    "# NOTE: this will overwrite any existing folder with the same name\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(project_root + '/kaggle_dataset/Po_River_July_2019/', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download the images\n",
    "for i, request in enumerate(requests_true_color):\n",
    "    response = request.get_data(save_data=True)\n",
    "    print(f\"Downloaded image {i + 1}/{len(requests_true_color)}: {response[0]['filename']}\")\n",
    "# Upload the images to Kaggle\n",
    "kaggle.api.dataset_create_version(\n",
    "    '/root/kaggle_dataset/Po_River_March_2020', # replace with your own folder name\n",
    "    version_notes='Initial version',\n",
    "    convert_to_csv=False,\n",
    "    delete_old_versions=False,\n",
    ")\n",
    "\n",
    "# Create a metadata file for your dataset\n",
    "metadata = {\n",
    "    \"title\": \"Po_River_July_2019\", # enter the title adding AOI and timeframe in name\n",
    "    \"id\": \"sarahajbane/litter_rows\",  # replace if you want to use your own and have \n",
    "    # been added as a editing collaborator to the dataset on Kaggle, message me if you want to be added.\n",
    "    \"licenses\": [{\"name\": \"CC0-4.0\"}]  # Creative Commons license with attribution to original authors of the litter_row dataset\n",
    "}\n",
    "with open('/root/kaggle_dataset/dataset-metadata.json', 'w') as file:\n",
    "    json.dump(metadata, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "current_batch = []\n",
    "# Loop through each result to fetch the corresponding image\n",
    "\n",
    "output_dir = \"~/kaggle_datasets/Po_River_July_2019\" # adjust this for kaggle\n",
    "\n",
    "for result in new_results:\n",
    "    timestamp = result[\"properties\"][\"datetime\"]\n",
    "    result_id = result[\"id\"] \n",
    "    print(f\"Fetching image for timestamp: {timestamp}\")\n",
    "\n",
    "    # Request for a single image corresponding to the timestamp\n",
    "    request_selected_bands = SentinelHubRequest(\n",
    "        evalscript=evalscript_true_color,\n",
    "        input_data=[\n",
    "            SentinelHubRequest.input_data(\n",
    "                data_collection=DataCollection.SENTINEL2_L1C.define_from(\n",
    "                    name=\"s2l1c\", service_url=\"https://sh.dataspace.copernicus.eu\"\n",
    "                ),\n",
    "                time_interval=(timestamp, timestamp),  # Use specific timestamp for each result\n",
    "                other_args={\"dataFilter\": {\"mosaickingOrder\": \"leastCC\"}},\n",
    "            )\n",
    "        ],\n",
    "        responses=[SentinelHubRequest.output_response(\"default\", MimeType.TIFF)],\n",
    "        bbox=aoi_bbox, #exchange for aoi2_bbox for Corsica , aoi3_bbox for Calabria as req.\n",
    "        size=aoi1_size, #exchange for aoi2_size for Corsica , aoi3_size for Calabria as req.\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    # Get the data (the image) and append to the list\n",
    "    image = request_selected_bands.get_data()\n",
    "    \n",
    "    # Since get_data() returns a list of images, take the first one\n",
    "    current_batch.append(image[0])  # Append the first image (or modify as needed)\n",
    "    \n",
    "    # Save the image to the \"data\" folder\n",
    "    image_array = image[0]\n",
    "    image_path = os.path.join(output_dir, f\"{result_id}.tiff\")  # Use ID for naming\n",
    "    Image.fromarray(image_array).save(image_path)\n",
    "\n",
    "    print(f\"Fetched and stored image for {timestamp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate through the Sentinel Products identified and save them locally [depracated, soon obsolete]\n",
    "Use this option if you want to download images to your own machine / drive to look at and play around with, \n",
    "but all products used for future steps in the project, i.e. for use in satellite corrections, model test/train, or validation \n",
    "need to be added to kaggle so we are working on the **same data** with same bands, transformations, bounding boxes etc. \n",
    "stored in one central **public** location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_color_imgs = []\n",
    "# Loop through each result to fetch the corresponding image\n",
    "\n",
    "output_dir = \"../data\" # adjust this for kaggle\n",
    "\n",
    "for result in new_results:\n",
    "    timestamp = result[\"properties\"][\"datetime\"]\n",
    "    result_id = result[\"id\"] \n",
    "    print(f\"Fetching image for timestamp: {timestamp}\")\n",
    "\n",
    "    # Request for a single image corresponding to the timestamp\n",
    "    request_true_color = SentinelHubRequest(\n",
    "        evalscript=evalscript_true_color,\n",
    "        input_data=[\n",
    "            SentinelHubRequest.input_data(\n",
    "                data_collection=DataCollection.SENTINEL2_L1C.define_from(\n",
    "                    name=\"s2l1c\", service_url=\"https://sh.dataspace.copernicus.eu\"\n",
    "                ),\n",
    "                time_interval=(timestamp, timestamp),  # Use specific timestamp for each result\n",
    "                other_args={\"dataFilter\": {\"mosaickingOrder\": \"leastCC\"}},\n",
    "            )\n",
    "        ],\n",
    "        responses=[SentinelHubRequest.output_response(\"default\", MimeType.TIFF)],\n",
    "        bbox=aoi_bbox, #exchange for aoi2_bbox for Corsica , aoi3_bbox for Calabria as req.\n",
    "        size=aoi1_size, #exchange for aoi2_size for Corsica , aoi3_size for Calabria as req.\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    # Get the data (the image) and append to the list\n",
    "    image = request_true_color.get_data()\n",
    "    \n",
    "    # Since get_data() returns a list of images, take the first one\n",
    "    true_color_imgs.append(image[0])  # Append the first image (or modify as needed)\n",
    "    \n",
    "    # Save the image to the \"data\" folder\n",
    "    image_array = image[0]\n",
    "    image_path = os.path.join(output_dir, f\"{result_id}.tiff\")  # Use ID for naming\n",
    "    Image.fromarray(image_array).save(image_path)\n",
    "\n",
    "    print(f\"Fetched and stored image for {timestamp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative if images are already saved locally \n",
    " &  you don't want to redownload them \n",
    " <br>\n",
    " <br>\n",
    " !! **PLEASE** !! make sure the bands selected (esp. if not all 13) and eval script return bands are accurate if you did not use this notebook to download L1C products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALT: if you already downloaded images to your local machine, you can move them to the untracked kaggle dataset folder \n",
    "# then uncomment the last line to upload them to Kaggle, i.e. copy or move your files to this directory\n",
    "\n",
    "!cp -r /Users/sara_mac/Desktop/projects/plastic_detection/Sentinel2PlasticDetectionProject/task2-data-collection/data/Po_River_July_2019/ /Users/sara_mac/Desktop/projects/plastic_detection/Sentinel2PlasticDetectionProject/task2-data-collection/kaggle_dataset/Po_River_July_2019/\n",
    "# !kaggle datasets create -p ~/kaggle_datasets/Po_River_July_2019/ --dir-mode zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are workign on a local machine, add your working directory folder to the sys path\n",
    "project_root = \"/Users/sara_mac/Desktop/projects/plastic_detection/Sentinel2PlasticDetectionProject/task2-data-collection/\" \n",
    "# if you are working on colab use the content (default) folder instead:\n",
    "# project_root = \"./content\"\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sara_mac/Desktop/projects/plastic_detection/Sentinel2PlasticDetectionProject/task2-data-collection/\n"
     ]
    }
   ],
   "source": [
    "print(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "KAGGLE_CONFIG_DIR = os.path.expanduser(project_root + \".kaggle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sara_mac/Desktop/projects/plastic_detection/Sentinel2PlasticDetectionProject/task2-data-collection/.kaggle\n"
     ]
    }
   ],
   "source": [
    "print(KAGGLE_CONFIG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = project_root + \"kaggle_dataset/Po_River_July_2019/\"\n",
    "if data_dir not in sys.path:\n",
    "    sys.path.append(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the kaggle.json file is in the correct location\n",
    "kaggle_json_path = os.path.expanduser(project_root + \"/.kaggle/kaggle.json\")\n",
    "if not os.path.exists(kaggle_json_path):\n",
    "\traise FileNotFoundError(\n",
    "\t\tf\"The Kaggle API credentials file is missing. Please place your kaggle.json file at {kaggle_json_path}.\"\n",
    "\t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate a dataset in the directory where you saved the bands/images, this will create a metadata file, which we have to amend in the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data package template written to: /Users/sara_mac/Desktop/projects/plastic_detection/Sentinel2PlasticDetectionProject/task2-data-collection/kaggle_dataset/Po_River_July_2019/dataset-metadata.json\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets init -p {data_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the metadata file to reflect what you are uploading and adding to the dataset collection\n",
    "# Replace metadata from the default that wasa created in the last cell. \n",
    "# This step is not optional, the default will not be accepted by Kaggle.\n",
    "\n",
    "# Amend as needed:\n",
    "metadata = {\n",
    "    \"title\": \"litter_rows: Sentinel2 L1C bands\", # enter the title adding AOI and timeframe in name\n",
    "    \"id\": \"www.kaggle.com/datasets/sarahajbane/litter_rows\",  # replace if you want to use your own and have\n",
    "      \"resources\": [\n",
    "    {\n",
    "      \"name\": \"Po_River_July_2019_4B\",\n",
    "      #\"path\": \"https://www.kaggle.com/datasets/sarahajbane/litter_rows\",\n",
    "      \"description\": \"Sentinel2 L1C bands from CDSE - First for Po River July 2019 - FDI and NDVI bands only\",\n",
    "      \"type\": \"image\",\n",
    "      \"format\": \"tiff\",\n",
    "    }\n",
    "      ],\n",
    "    \"licenses\": [\n",
    "        {\n",
    "      \"name\": \"CC-BY-SA-4.0\",\n",
    "      \"title\": \"Creative Commons Attribution Share-Alike 4.0\",\n",
    "      \"path\": \"https://creativecommons.org/licenses/by-sa/4.0/\"\n",
    "    } # Creative Commons license with proper attribution \n",
    "    ] # to original authors of the litter_row dataset\n",
    "    }\n",
    "with open(data_dir + 'dataset-metadata.json', 'w') as file:\n",
    "    json.dump(metadata, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_path = \"https://www.kaggle.com/datasets/sarahajbane/litter_rows\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/Users/sara_mac/Desktop/projects/plastic_detection/Sentinel2PlasticDetectionProject/omdena/bin/kaggle\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/sara_mac/Desktop/projects/plastic_detection/Sentinel2PlasticDetectionProject/omdena/lib/python3.13/site-packages/kaggle/cli.py\"\u001b[0m, line \u001b[35m68\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "    out = args.func(**command_args)\n",
      "  File \u001b[35m\"/Users/sara_mac/Desktop/projects/plastic_detection/Sentinel2PlasticDetectionProject/omdena/lib/python3.13/site-packages/kaggle/api/kaggle_api_extended.py\"\u001b[0m, line \u001b[35m2073\u001b[0m, in \u001b[35mdataset_create_new_cli\u001b[0m\n",
      "    result = self.dataset_create_new(folder, public, quiet, convert_to_csv,\n",
      "                                     dir_mode)\n",
      "  File \u001b[35m\"/Users/sara_mac/Desktop/projects/plastic_detection/Sentinel2PlasticDetectionProject/omdena/lib/python3.13/site-packages/kaggle/api/kaggle_api_extended.py\"\u001b[0m, line \u001b[35m2013\u001b[0m, in \u001b[35mdataset_create_new\u001b[0m\n",
      "    \u001b[31mself.validate_resources\u001b[0m\u001b[1;31m(folder, resources)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/sara_mac/Desktop/projects/plastic_detection/Sentinel2PlasticDetectionProject/omdena/lib/python3.13/site-packages/kaggle/api/kaggle_api_extended.py\"\u001b[0m, line \u001b[35m4577\u001b[0m, in \u001b[35mvalidate_resources\u001b[0m\n",
      "    \u001b[31mself.validate_files_exist\u001b[0m\u001b[1;31m(folder, resources)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/sara_mac/Desktop/projects/plastic_detection/Sentinel2PlasticDetectionProject/omdena/lib/python3.13/site-packages/kaggle/api/kaggle_api_extended.py\"\u001b[0m, line \u001b[35m4590\u001b[0m, in \u001b[35mvalidate_files_exist\u001b[0m\n",
      "    full_path = os.path.join(folder, file_name)\n",
      "  File \u001b[35m\"<frozen posixpath>\"\u001b[0m, line \u001b[35m90\u001b[0m, in \u001b[35mjoin\u001b[0m\n",
      "  File \u001b[35m\"<frozen genericpath>\"\u001b[0m, line \u001b[35m188\u001b[0m, in \u001b[35m_check_arg_types\u001b[0m\n",
      "\u001b[1;35mTypeError\u001b[0m: \u001b[35mjoin() argument must be str, bytes, or os.PathLike object, not 'NoneType'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets create -p {data_dir} --dir-mode zip \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sara_mac/Desktop/projects/plastic_detection/Sentinel2PlasticDetectionProject/task2-data-collection/kaggle_dataset/Po_River_July_2019/https://www.kaggle.com/datasets/sarahajbane/litter-windrows does not exist\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets version -p {data_dir} -m \"Update data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets version -p /path/to/dataset -m \"Add new satellite bands: PRP 07/2019 - 4 bands\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/Users/sara_mac/Desktop/projects/plastic_detection/Sentinel2PlasticDetectionProject/omdena/bin/kaggle\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/sara_mac/Desktop/projects/plastic_detection/Sentinel2PlasticDetectionProject/omdena/lib/python3.13/site-packages/kaggle/cli.py\"\u001b[0m, line \u001b[35m68\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "    out = args.func(**command_args)\n",
      "  File \u001b[35m\"/Users/sara_mac/Desktop/projects/plastic_detection/Sentinel2PlasticDetectionProject/omdena/lib/python3.13/site-packages/kaggle/api/kaggle_api_extended.py\"\u001b[0m, line \u001b[35m1920\u001b[0m, in \u001b[35mdataset_create_version_cli\u001b[0m\n",
      "    result = self.dataset_create_version(\n",
      "        folder,\n",
      "    ...<3 lines>...\n",
      "        delete_old_versions=delete_old_versions,\n",
      "        dir_mode=dir_mode)\n",
      "  File \u001b[35m\"/Users/sara_mac/Desktop/projects/plastic_detection/Sentinel2PlasticDetectionProject/omdena/lib/python3.13/site-packages/kaggle/api/kaggle_api_extended.py\"\u001b[0m, line \u001b[35m1859\u001b[0m, in \u001b[35mdataset_create_version\u001b[0m\n",
      "    \u001b[31mself.validate_resources\u001b[0m\u001b[1;31m(folder, resources)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/sara_mac/Desktop/projects/plastic_detection/Sentinel2PlasticDetectionProject/omdena/lib/python3.13/site-packages/kaggle/api/kaggle_api_extended.py\"\u001b[0m, line \u001b[35m4577\u001b[0m, in \u001b[35mvalidate_resources\u001b[0m\n",
      "    \u001b[31mself.validate_files_exist\u001b[0m\u001b[1;31m(folder, resources)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/sara_mac/Desktop/projects/plastic_detection/Sentinel2PlasticDetectionProject/omdena/lib/python3.13/site-packages/kaggle/api/kaggle_api_extended.py\"\u001b[0m, line \u001b[35m4590\u001b[0m, in \u001b[35mvalidate_files_exist\u001b[0m\n",
      "    full_path = os.path.join(folder, file_name)\n",
      "  File \u001b[35m\"<frozen posixpath>\"\u001b[0m, line \u001b[35m90\u001b[0m, in \u001b[35mjoin\u001b[0m\n",
      "  File \u001b[35m\"<frozen genericpath>\"\u001b[0m, line \u001b[35m188\u001b[0m, in \u001b[35m_check_arg_types\u001b[0m\n",
      "\u001b[1;35mTypeError\u001b[0m: \u001b[35mjoin() argument must be str, bytes, or os.PathLike object, not 'NoneType'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets version -p {data_dir} -m \"Update data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last task outstanding before complete:\n",
    "- Create a function to save images/bands directly to kagglehub storage regardless where the code is ran, if possible, i.e. https://www.kaggle.com/datasets/sarahajbane/litter-windrows \n",
    "- Create proper true colour image from RGB bands and add transformations for NDVI and FDI from appropriate bands, creating appropriate outputs for MapMapper workflow and other model inputs output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes kaggle API\n",
    " to save outputs directly to kaggle hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section ensures all relevant Sentinel Products (bands/images) relating to the litter_rows will be saved to a public kagglehub [dataset](\"https://www.kaggle.com/datasets/sarahajbane/litter-windrows\") to have one stable centralised data storage location and streamline an input directory path for further use in the Satellite Detection algorithm project.  \n",
    "(i.e. for satellite corrections, training, testing & validation of current and additional model versions), \n",
    "as well as an open source resource for future models or other open-source use cases for the public, similar to MARIDA dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Install the Kaggle API if not already installed. \n",
    "If you are using a virtual environment, you may need to install the Kaggle API with --user outside of it first for it to work\n",
    "with a regular pip install command (i.e. pip install kaggle) within the venv afterwards\n",
    "\n",
    "!pip install --user kaggle\n",
    "\n",
    "make sure you have the /.kaggle/kaggle.json file in the correct location, i.e. your home directory or the current working directory if \n",
    "using colab or jupyter notebook\n",
    "\n",
    "- Linux: $XDG_CONFIG_HOME/kaggle/kaggle.json (defaults to ~/.config/kaggle/kaggle.json). \n",
    "- The path ~/.kaggle/kaggle.json which was used by older versions of the tool is also still supported.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Jayash snippet:\n",
    "# Create the kaggle.json file with your API credentials\n",
    "# You'll need to replace these with your actual Kaggle username and key\n",
    "import json\n",
    "with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
    "    json.dump({\n",
    "        \"username\": \"YOUR_KAGGLE_USERNAME\",\n",
    "        \"key\": \"YOUR_KAGGLE_API_KEY\"\n",
    "    }, file)\n",
    "# Set permissions for the kaggle.json file\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "# Create a directory for the dataset files\n",
    "!mkdir -p ~/kaggle_dataset\n",
    "# Copy or move your files to this directory\n",
    "# For example:\n",
    "# !cp your_file1.csv your_file2.csv ~/kaggle_dataset/\n",
    "# Create a metadata file for your dataset\n",
    "metadata = {\n",
    "    \"title\": \"Your Dataset Title\",\n",
    "    \"id\": \"username/dataset-name\",  # username/dataset-slug-name\n",
    "    \"licenses\": [{\"name\": \"CC0-1.0\"}]  # Choose an appropriate license\n",
    "}\n",
    "with open('/root/kaggle_dataset/dataset-metadata.json', 'w') as file:\n",
    "    json.dump(metadata, file)\n",
    "# Create a new dataset (or update if it already exists)\n",
    "!kaggle datasets create -p ~/kaggle_dataset/ -r zip"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Create a new dataset version\n",
    "\n",
    "usage: kaggle datasets version [-h] -m VERSION_NOTES [-p FOLDER] [-q] [-t]\n",
    "                               [-r {skip,zip,tar}] [-d]\n",
    "\n",
    "required arguments:\n",
    "  -m VERSION_NOTES, --message VERSION_NOTES\n",
    "                        Message describing the new version\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  -p FOLDER, --path FOLDER\n",
    "                        Folder for upload, containing data files and a special dataset-metadata.json file (https://github.com/Kaggle/kaggle-api/wiki/Dataset-Metadata). Defaults to current working directory\n",
    "  -q, --quiet           Suppress printing information about the upload/download progress\n",
    "  -t, --keep-tabular    Do not convert tabular files to CSV (default is to convert)\n",
    "  -r {skip,zip,tar}, --dir-mode {skip,zip,tar}\n",
    "                        What to do with directories: \"skip\" - ignore; \"zip\" - compressed upload; \"tar\" - uncompressed upload\n",
    "  -d, --delete-old-versions\n",
    "                        Delete old versions of this dataset\n",
    "Example:\n",
    "\n",
    "kaggle datasets version -p /path/to/dataset -m \"Updated data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sagar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sara_mac/Desktop/projects/plastic_detection/Sentinel2PlasticDetectionProject/task2-data-collection/\n",
      "/Users/sara_mac/Desktop/projects/plastic_detection/Sentinel2PlasticDetectionProject/task2-data-collection/.kaggle\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[100]\u001b[39m\u001b[32m, line 83\u001b[39m\n\u001b[32m     75\u001b[39m         subprocess.run([\n\u001b[32m     76\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mkaggle\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdatasets\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mversion\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     77\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m-p\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(folder_path),\n\u001b[32m     78\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m-m\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mUpdate data\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     79\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m--dir-mode\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mzip\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     80\u001b[39m         ])\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# Example call to the above function\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m \u001b[43mcreate_kaggle_dataset_from_folder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/Users/sara_mac/Desktop/projects/plastic_detection/Sentinel2PlasticDetectionProject/task2-data-collection/kaggle_dataset/Po_River_July_2019\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPo River July 2019: 4-band Sentinel2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_id\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msarahajbane/litter_windrows\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     87\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[100]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mcreate_kaggle_dataset_from_folder\u001b[39m\u001b[34m(folder_path, title, dataset_id, description, license_name)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_kaggle_dataset_from_folder\u001b[39m(\n\u001b[32m     23\u001b[39m     folder_path,\n\u001b[32m     24\u001b[39m     title,\n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m     license_name=\u001b[33m\"\u001b[39m\u001b[33mCC-BY-SA-4.0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     28\u001b[39m ):\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     folder_path = \u001b[43mPath\u001b[49m(folder_path)\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m folder_path.exists(), \u001b[33m\"\u001b[39m\u001b[33mFolder does not exist!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     32\u001b[39m     metadata_path = folder_path / \u001b[33m\"\u001b[39m\u001b[33mdataset-metadata.json\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "# Set the project root\n",
    "project_root = \"/Users/sara_mac/Desktop/projects/plastic_detection/Sentinel2PlasticDetectionProject/task2-data-collection/\"\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "print(project_root)\n",
    "\n",
    "# Set Kaggle config directory\n",
    "KAGGLE_CONFIG_DIR = os.path.expanduser(project_root + \".kaggle\")\n",
    "print(KAGGLE_CONFIG_DIR)\n",
    "\n",
    "# Path to data directory to Po_River\n",
    "data_dir = project_root + \"kaggle_dataset/Po_River_July_2019/\"\n",
    "if data_dir not in sys.path:\n",
    "    sys.path.append(data_dir)\n",
    "\n",
    "# Check for kaggle.json credentials\n",
    "kaggle_json_path = os.path.expanduser(project_root + \"/.kaggle/kaggle.json\")\n",
    "if not os.path.exists(kaggle_json_path):\n",
    "    raise FileNotFoundError(f\"The Kaggle API credentials file is missing. Please place your kaggle.json file at {kaggle_json_path}.\")\n",
    "\n",
    "# Function to create or update a Kaggle dataset from a local folder\n",
    "def create_kaggle_dataset_from_folder(\n",
    "    folder_path,\n",
    "    title,\n",
    "    dataset_id,\n",
    "    description=\"Sentinel-2 L1C subset\",\n",
    "    license_name=\"CC-BY-SA-4.0\"\n",
    "):\n",
    "    folder_path = Path(folder_path)\n",
    "    assert folder_path.exists(), \"Folder does not exist!\"\n",
    "\n",
    "    metadata_path = folder_path / \"dataset-metadata.json\"\n",
    "    image_files = [f.name for f in folder_path.glob(\"*.tif*\")]\n",
    "\n",
    "    resources = [\n",
    "        {\n",
    "            \"name\": Path(img).stem,\n",
    "            \"path\": img,\n",
    "            \"description\": f\"Image: {img}\",\n",
    "            \"type\": \"image\",\n",
    "            \"format\": \"tiff\"\n",
    "        } for img in image_files\n",
    "    ]\n",
    "\n",
    "    metadata = {\n",
    "        \"title\": title,\n",
    "        \"id\": dataset_id,\n",
    "        \"licenses\": [{\n",
    "            \"name\": license_name,\n",
    "            \"title\": \"Creative Commons Attribution Share-Alike 4.0\",\n",
    "            \"path\": \"https://creativecommons.org/licenses/by-sa/4.0/\"\n",
    "        }],\n",
    "        \"resources\": resources\n",
    "    }\n",
    "\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "\n",
    "    # Initialize if necessary\n",
    "    if not (folder_path / \"dataset-metadata.json\").exists():\n",
    "        subprocess.run([\"kaggle\", \"datasets\", \"init\", \"-p\", str(folder_path)])\n",
    "\n",
    "    # Create or version the dataset\n",
    "    if not any((folder_path / f).exists() for f in [\"dataset-metadata.json\", \"dataset-metadata.yml\"]):\n",
    "        print(\"No metadata found, initializing dataset.\")\n",
    "        subprocess.run([\"kaggle\", \"datasets\", \"init\", \"-p\", str(folder_path)])\n",
    "\n",
    "    try:\n",
    "        subprocess.run([\n",
    "            \"kaggle\", \"datasets\", \"create\",\n",
    "            \"-p\", str(folder_path),\n",
    "            \"--dir-mode\", \"zip\"\n",
    "        ], check=True)\n",
    "    except subprocess.CalledProcessError:\n",
    "        subprocess.run([\n",
    "            \"kaggle\", \"datasets\", \"version\",\n",
    "            \"-p\", str(folder_path),\n",
    "            \"-m\", \"Update data\",\n",
    "            \"--dir-mode\", \"zip\"\n",
    "        ])\n",
    "\n",
    "# Example call to the above function\n",
    "create_kaggle_dataset_from_folder(\n",
    "    folder_path=\"/Users/sara_mac/Desktop/projects/plastic_detection/Sentinel2PlasticDetectionProject/task2-data-collection/kaggle_dataset/Po_River_July_2019\",\n",
    "    title=\"Po River July 2019: 4-band Sentinel2\",\n",
    "    dataset_id=\"sarahajbane/litter_windrows\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omdena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
