{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a016576",
   "metadata": {},
   "source": [
    "#  Download full SAFE archive as .zip and upload to kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d25aba",
   "metadata": {},
   "source": [
    " __To use the features in this notebook you need to visit https://dataspace.copernicus.eu and create an account with Copernicus, the official governing body of Sentinel Missions for the European Space Agency (ESA). This takes about 5 minutes to do.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b61af77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834b6d8c",
   "metadata": {},
   "source": [
    "# Batch setup\n",
    "Steps:\n",
    "- make sure your log in credentials for cdse are appropriately stored in the .env file in this format with quotation marks: <br><br>\n",
    "CDSE_email = 'youremail' <br>\n",
    "CDSE_password = 'yourpassword'<br><br>\n",
    "- change satellite, if S2A or S2B, depending on batch <br><br>\n",
    "- change startDate and endDate to reflect the time period for your batch\n",
    "<br><br>\n",
    "- output_dir to reflect the REgion & time for your batch in folder name<br><br>__!!Keep it strictly in PO_SX_YY format!!__ <br> longer or shorter strings will break the code <br><br>\n",
    "- leave query_satellite and query_tile unchanged\n",
    "- if you find more tiles than written in the table please update as pictured.\n",
    "- occasionally you will find duplicate records/ instances miliseconds - seconds apart (mostly with one image predominantly blank). This is usually easily noticable because one file will be very small compared to the other, but please keep them until you can unzip and verify there is no ocean visible in the scene with possible annotations. Please still record all SAFE archives downloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2867fef4",
   "metadata": {},
   "source": [
    "### ðŸ›°ï¸ Sentinel-2 Data Summary for Po_River_Plume_tile\n",
    "\n",
    "| ðŸ—‚ï¸Â Batch Name | ðŸ›°ï¸ Satellite Type | ðŸ“… From         | ðŸ“… To           | ðŸ“¦ Number of SAFE Files | ðŸ’¾ Estimated Size |\n",
    "| ---------------- | ------------------- | --------------- | --------------- | -------------- | ----------------- |\n",
    "| Batch_01         |  S2A_MSIL1C         | 2015-07-04   | 2016-10-17   | 30                       | Maximum 27 GB ðŸ’½       |\n",
    "| Batch_02  âœ…        | S2A_MSIL1C          | 2017-02-20      | 2017-10-08      | 17                      | Maximum 15 GB ðŸ’½  |\n",
    "| Batch_03     âœ…      | S2A_MSIL1C          | 2018-03-27      | 2018-11-12      | 27                      | Maximum 24 GB ðŸ’½  |\n",
    "| [Batch_04](https://www.kaggle.com/datasets/sarahajbane/litter-windrows-batch-4)   âœ…    | S2A_MSIL1C      | 2019-02-13  | 2019-10-11 | 17                      | 11.7 GB ðŸ’½  |\n",
    "| [Batch_05](https://www.kaggle.com/datasets/sarahajbane/litter-windrows-batch-5)    âœ…   | S2A_MSIL1C      | 2020-03-16      | 2020-11-04    | 18                      | 12.35 GB ðŸ’½  |\n",
    "| [Batch_06](https://kaggle.com/datasets/57998def69485e4499984abeb8f2986e745ada745cb7270c56dd2c7c182c30e5)    âœ…    | S2A_MSIL1C          | 2021-03-01      | 2021-08-18      | 18                      | 12.1 GB ðŸ’½  |\n",
    "| Batch_07         | S2B_MSIL1C          | 2017-07-05      | 2018-11-17      | 37                      | Maximum 33 GB ðŸ’½  |\n",
    "| Batch_08         | S2B_MSIL1C          | 2019-02-08      | 2019-10-16      | 14                      | Maximum 12 GB ðŸ’½  |\n",
    "| Batch_09         | S2B_MSIL1C          | 2020-02-20      | 2020-11-19      | 21                      | Maximum 18 GB ðŸ’½  |\n",
    "| Batch_10         | S2B_MSIL1C          | 2021-03-29      | 2021-06-17      | 10                      | Maximum 9 GB ðŸ’½   |\n",
    "### ðŸ›°ï¸ Sentinel-2 Data Summary for Northern_Corsica_tile\n",
    "\n",
    "| ðŸ—‚ï¸Â Batch Name | ðŸ›°ï¸ Satellite Type | ðŸ“… From    | ðŸ“… To      | ðŸ“¦ Number of SAFE Files | ðŸ’¾ Estimated Size |\n",
    "| ---------------- | ------------------- | ---------- | ---------- | ----------------------- | ----------------- |\n",
    "| [Batch_11](https://kaggle.com/datasets/978813a46e0c302b6ef579cf5406f7f0e783afe4a2d998009593091fd5f00b34) âœ…      | S2A_MSIL1C          | 2015-07-24 | 2021-06-22 | 18                      | 10.5 GB ðŸ’½  |\n",
    "| [Batch_12](https://www.kaggle.com/datasets/sarahajbane/litter-windrows-corsib)     âœ…     | S2B_MSIL1C          | 2017-07-08 | 2020-08-01 | 8                       | 4.9 GB ðŸ’½   |\n",
    "\n",
    "### ðŸ›°ï¸ Sentinel-2 Data Summary for Calabria_tile\n",
    "\n",
    "| ðŸ—‚ï¸Â Batch Name | ðŸ›°ï¸ Satellite Type | ðŸ“… From    | ðŸ“… To      | ðŸ“¦ Number of SAFE Files | ðŸ’¾ Estimated Size |\n",
    "| ---------------- | ------------------- | ---------- | ---------- | ----------------------- | ----------------- |\n",
    "| [Batch_13](https://www.kaggle.com/datasets/sarahajbane/litter-windrows-batch-cala)   âœ…       | S2A_MSIL1C          | 2015-08-14 | 2020-12-28 | 13                      | 6.1 GB ðŸ’½  |\n",
    "| [Batch_14](https://kaggle.com/datasets/f6906f09cfd4e0b0d046fd7c330064c9dc0f838ab14b9bcb79fdbc3776a680e4)    âœ…      | S2B_MSIL1C          | 2017-07-09 | 2020-10-21 | 9                       | 6.21 GB ðŸ’½   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29b61131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Required satellite category\n",
    "query_satellite = 'SENTINEL-2'\n",
    "\n",
    "# 2 Strings to be included in query for retrieval of specific product by name, \n",
    "# i.e S2A vs S2B, and code for AOI tile name\n",
    "query_product = 'S2A_MSIL1C_' # change to S2B_MSIL1C_\n",
    "query_tile = 'T33TUL' # stays the same\n",
    "\n",
    "# 3 Enter a start and end date\n",
    "query_startDate = '2019-02-13'   # change as per table above\n",
    "query_endDate = '2019-10-12'     # change as per table above\n",
    "\n",
    "# 4 Load your credentials from .env\n",
    "load_dotenv()\n",
    "username=os.getenv(\"CDSE_email\")\n",
    "password=os.getenv(\"CDSE_password\")\n",
    "# if not already in .env config, insert them as 'string' \n",
    "# values in the following format to the .env file:\n",
    "CDSE_email = username\n",
    "CDSE_password = password\n",
    "\n",
    "# 5 Set output file:\n",
    "output_dir = './SAFE/PO_2A_19' #edit folder name within SAFE/ as appropriate to add batch folders\n",
    "# i.e. keep format like: \n",
    "# ./SAFE/PO_2A_17 for Sentinel 2A until 2017\n",
    "# ./SAFE/PO_2B_18 for Sentinel 2B until 2018 etc. \n",
    "# =============================================== \n",
    "# ! DO NOT CHANGE THE LENGTH OF THE FOLDER NAME! \n",
    "# =============================================== \n",
    "# This is important for the download script to work properly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e36f2c0",
   "metadata": {},
   "source": [
    "## Run as is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2c45810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_access_token(username: str, password: str) -> str:\n",
    "    data = {\n",
    "        \"client_id\": \"cdse-public\",\n",
    "        \"username\":username,\n",
    "        \"password\":password,\n",
    "        \"grant_type\": \"password\",\n",
    "        }\n",
    "    try:\n",
    "        r = requests.post(\n",
    "            \"https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\",\n",
    "            data=data,\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "    except Exception as e:\n",
    "        raise Exception(\n",
    "            f\"Access token creation failed. Reponse from the server was: {r.json()}\"\n",
    "            )\n",
    "    print(\"Access token created successfully!\")\n",
    "    return r.json()[\"access_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b281cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_https_request(satellite, product, tile, start_date, end_date):\n",
    "    \n",
    "    base_prefix = \"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=\"\n",
    "    collection = \"Collection/Name eq '\" + satellite + \"' and startswith(Name,'\" + product + \"') and contains(Name,'\" + tile + \"')\"\n",
    "    content_date = (\n",
    "            \"ContentDate/Start gt \" + start_date + \"T00:00:00.000Z and \" +\n",
    "            \"ContentDate/Start lt \" + end_date + \"T00:00:00.000Z\"\n",
    "    )\n",
    "    https_request = (base_prefix + collection +  \" and \" + content_date) \n",
    "    print(\"Query URL:\", https_request)\n",
    "    return https_request\n",
    "\n",
    "\n",
    "def download_data(token, id, name, length, output):\n",
    "    url = f\"https://download.dataspace.copernicus.eu/odata/v1/Products({id})/$value\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    session = requests.Session()\n",
    "    session.headers.update(headers)\n",
    "    response = session.get(url, headers=headers, stream=True)\n",
    "    try:\n",
    "        print('[', datetime.datetime.strftime(datetime.datetime.now(), '%H:%M:%S'), '] '+'Downloading: '+name)\n",
    "        with open(output, \"wb\") as file:\n",
    "            if length is not None:\n",
    "                pbar = tqdm(total=length, unit=\"B\", unit_scale=True, desc=name)\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        file.write(chunk)\n",
    "                        pbar.update(len(chunk))\n",
    "                pbar.close()\n",
    "        print('[', datetime.datetime.strftime(datetime.datetime.now(), '%H:%M:%S'), '] '+'Download complete: '+name)\n",
    "        response.close()\n",
    "    except Exception as e:\n",
    "        print('[', datetime.datetime.strftime(datetime.datetime.now(), '%H:%M:%S'), '] '+'Download failed: '+name)\n",
    "        print(f\"An exception occured: {e}\")\n",
    "\n",
    "\n",
    "def get_file_name(name):\n",
    "    file_name = ''\n",
    "    if query_satellite == 'SENTINEL-1':\n",
    "        file_name = name.replace(\".SAFE\", \".zip\")\n",
    "    elif query_satellite == 'SENTINEL-2':\n",
    "        file_name = name.replace(\".SAFE\", \".zip\")\n",
    "    elif query_satellite == 'SENTINEL-3':\n",
    "        file_name = name.replace(\".SEN3\", \".zip\")\n",
    "    elif query_satellite == 'SENTINEL-5P':\n",
    "        file_name = name.replace(\".nc\", \".zip\")\n",
    "    elif query_satellite == 'SENTINEL-6':\n",
    "        file_name = name.replace(\".SEN6\", \".zip\")\n",
    "    return file_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8e7617",
   "metadata": {},
   "source": [
    "# Download non-duplicate tiles matched to litter row data. \n",
    "Make sure the path  for ```litterrows = pd.read_csv('../files/s2_product_unique.csv')```is reflected in your folder structure or \n",
    "if using colab, changed to './s2_product_unique.csv' and the file added to content folder (current working dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3610dee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query URL: https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq 'SENTINEL-2' and startswith(Name,'S2A_MSIL1C_') and contains(Name,'T33TUL') and ContentDate/Start gt 2019-02-13T00:00:00.000Z and ContentDate/Start lt 2019-10-12T00:00:00.000Z\n",
      "S2A_MSIL1C_20190312T100021_N0500_R122_T33TUL_20221129T200337.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190312T100021_N0500_R122_T33TUL_20221129T200337.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190325T101021_N0500_R022_T33TUL_20221117T131900.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190325T101021_N0500_R022_T33TUL_20221117T131900.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190315T101021_N0500_R022_T33TUL_20221116T052920.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190315T101021_N0500_R022_T33TUL_20221116T052920.zip this file already exists\n",
      "S2A_MSIL1C_20190322T100031_N0500_R122_T33TUL_20221119T025646.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190322T100031_N0500_R122_T33TUL_20221119T025646.zip this file already exists\n",
      "S2A_MSIL1C_20190312T100021_N0500_R122_T33TUL_20221115T172347.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190312T100021_N0500_R122_T33TUL_20221115T172347.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190213T101131_N0500_R022_T33TUL_20221128T153324.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190213T101131_N0500_R022_T33TUL_20221128T153324.zip this file already exists\n",
      "S2A_MSIL1C_20190220T100031_N0500_R122_T33TUL_20221125T152706.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190220T100031_N0500_R122_T33TUL_20221125T152706.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190223T101021_N0500_R022_T33TUL_20221127T011157.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190223T101021_N0500_R022_T33TUL_20221127T011157.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190302T100021_N0500_R122_T33TUL_20221112T030944.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190302T100021_N0500_R122_T33TUL_20221112T030944.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190305T101021_N0500_R022_T33TUL_20221117T172715.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190305T101021_N0500_R022_T33TUL_20221117T172715.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190521T100031_N0500_R122_T33TUL_20221215T032340.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190521T100031_N0500_R122_T33TUL_20221215T032340.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190610T100031_N0500_R122_T33TUL_20230605T081058.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190610T100031_N0500_R122_T33TUL_20230605T081058.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190421T100031_N0500_R122_T33TUL_20221101T004514.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190421T100031_N0500_R122_T33TUL_20221101T004514.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190524T101031_N0500_R022_T33TUL_20221209T052220.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190524T101031_N0500_R022_T33TUL_20221209T052220.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190411T100031_N0500_R122_T33TUL_20221102T182917.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190411T100031_N0500_R122_T33TUL_20221102T182917.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190424T101031_N0500_R022_T33TUL_20221024T205552.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190424T101031_N0500_R022_T33TUL_20221024T205552.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190623T101031_N0500_R022_T33TUL_20230619T042008.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190623T101031_N0500_R022_T33TUL_20230619T042008.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190620T100031_N0500_R122_T33TUL_20230722T153818.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190620T100031_N0500_R122_T33TUL_20230722T153818.zip this file already exists\n",
      "S2A_MSIL1C_20190504T101031_N0500_R022_T33TUL_20221215T170240.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190504T101031_N0500_R022_T33TUL_20221215T170240.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190514T101031_N0500_R022_T33TUL_20221217T045717.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190514T101031_N0500_R022_T33TUL_20221217T045717.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190630T100031_N0500_R122_T33TUL_20230723T095540.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190630T100031_N0500_R122_T33TUL_20230723T095540.zip this file already exists\n",
      "S2A_MSIL1C_20190404T101031_N0500_R022_T33TUL_20221025T065055.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190404T101031_N0500_R022_T33TUL_20221025T065055.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190414T101031_N0500_R022_T33TUL_20221110T004946.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190414T101031_N0500_R022_T33TUL_20221110T004946.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190501T100031_N0500_R122_T33TUL_20221219T071444.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190501T100031_N0500_R122_T33TUL_20221219T071444.zip this file already exists\n",
      "S2A_MSIL1C_20190531T100031_N0500_R122_T33TUL_20221213T082018.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190531T100031_N0500_R122_T33TUL_20221213T082018.zip this file already exists\n",
      "S2A_MSIL1C_20190401T100031_N0500_R122_T33TUL_20221019T010704.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190401T100031_N0500_R122_T33TUL_20221019T010704.zip this file already exists\n",
      "S2A_MSIL1C_20190603T101031_N0500_R022_T33TUL_20221221T193704.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190603T101031_N0500_R022_T33TUL_20221221T193704.zip this file already exists\n",
      "S2A_MSIL1C_20190511T100031_N0500_R122_T33TUL_20230617T185046.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190511T100031_N0500_R122_T33TUL_20230617T185046.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190613T101031_N0500_R022_T33TUL_20230721T210220.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190613T101031_N0500_R022_T33TUL_20230721T210220.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190812T101031_N0500_R022_T33TUL_20230708T105921.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190812T101031_N0500_R022_T33TUL_20230708T105921.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190822T101031_N0500_R022_T33TUL_20230705T195754.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190822T101031_N0500_R022_T33TUL_20230705T195754.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190908T100031_N0500_R122_T33TUL_20230502T043538.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190908T100031_N0500_R122_T33TUL_20230502T043538.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190802T101031_N0500_R022_T33TUL_20230507T065150.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190802T101031_N0500_R022_T33TUL_20230507T065150.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190918T100031_N0500_R122_T33TUL_20230628T191245.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190918T100031_N0500_R122_T33TUL_20230628T191245.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190829T100031_N0500_R122_T33TUL_20230523T105445.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190829T100031_N0500_R122_T33TUL_20230523T105445.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190928T100031_N0500_R122_T33TUL_20230628T222625.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190928T100031_N0500_R122_T33TUL_20230628T222625.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190901T101031_N0500_R022_T33TUL_20230701T230615.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190901T101031_N0500_R022_T33TUL_20230701T230615.zip this file already exists\n",
      "S2A_MSIL1C_20190723T101031_N0500_R022_T33TUL_20230718T015529.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190723T101031_N0500_R022_T33TUL_20230718T015529.zip this file already exists\n",
      "S2A_MSIL1C_20190921T101031_N0500_R022_T33TUL_20230629T021320.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190921T101031_N0500_R022_T33TUL_20230629T021320.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190911T101021_N0500_R022_T33TUL_20230703T071049.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190911T101021_N0500_R022_T33TUL_20230703T071049.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190730T100031_N0500_R122_T33TUL_20230710T202111.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190730T100031_N0500_R122_T33TUL_20230710T202111.zip this file already exists\n",
      "S2A_MSIL1C_20190713T101031_N0500_R022_T33TUL_20230722T213217.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190713T101031_N0500_R022_T33TUL_20230722T213217.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190703T101031_N0500_R022_T33TUL_20230717T022229.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190703T101031_N0500_R022_T33TUL_20230717T022229.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190720T100031_N0500_R122_T33TUL_20230715T235221.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190720T100031_N0500_R122_T33TUL_20230715T235221.zip this file already exists\n",
      "S2A_MSIL1C_20190710T100031_N0500_R122_T33TUL_20230624T125001.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190710T100031_N0500_R122_T33TUL_20230624T125001.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20190809T100031_N0500_R122_T33TUL_20230515T073846.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190809T100031_N0500_R122_T33TUL_20230515T073846.zip this file already exists\n",
      "S2A_MSIL1C_20190819T100031_N0500_R122_T33TUL_20230719T223350.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20190819T100031_N0500_R122_T33TUL_20230719T223350.zip this file already exists\n",
      "S2A_MSIL1C_20191001T101031_N0500_R022_T33TUL_20230710T230419.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20191001T101031_N0500_R022_T33TUL_20230710T230419.zip has no recorded litter rows\n",
      "S2A_MSIL1C_20191008T100031_N0500_R122_T33TUL_20230613T122804.SAFE\n",
      "./SAFE/PO_2A_19/S2A_MSIL1C_20191008T100031_N0500_R122_T33TUL_20230613T122804.zip this file already exists\n",
      "S2A_MSIL1C_20191011T101031_N0500_R022_T33TUL_20230708T021625.SAFE\n",
      "Access token created successfully!\n",
      "[ 01:41:27 ] Downloading: S2A_MSIL1C_20191011T101031_N0500_R022_T33TUL_20230708T021625.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S2A_MSIL1C_20191011T101031_N0500_R022_T33TUL_20230708T021625.zip: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 641M/641M [02:56<00:00, 3.64MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 01:44:24 ] Download complete: S2A_MSIL1C_20191011T101031_N0500_R022_T33TUL_20230708T021625.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "request_url = get_https_request(\n",
    "    query_satellite, query_product, query_tile, query_startDate, query_endDate #, map_geojson, \n",
    ")\n",
    "def get_all_results(url):\n",
    "    all_results = []\n",
    "    while url:\n",
    "        response = requests.get(url).json()\n",
    "        if 'value' in response:\n",
    "            all_results.extend(response['value'])\n",
    "        else:\n",
    "            print('Unexpected API response structure.')\n",
    "            break\n",
    "        url = response.get('@odata.nextLink')  # Move to next page if exists\n",
    "        if url:\n",
    "            time.sleep(1)  # Optional: small delay between pages\n",
    "    return all_results\n",
    "\n",
    "results = get_all_results(request_url)\n",
    "\n",
    "if not results:\n",
    "    print('No data found')\n",
    "    sys.exit()\n",
    "\n",
    "df = pd.DataFrame.from_dict(results)\n",
    "data_id_list = df.Id\n",
    "data_name_list = df.Name\n",
    "date_content_length = df.ContentLength\n",
    "\n",
    "for i in range(len(data_id_list)):\n",
    "    print(data_name_list[i])\n",
    "    data_id = data_id_list[i]\n",
    "    data_name = get_file_name(data_name_list[i])\n",
    "    data_length = date_content_length[i]\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    output_file = os.path.join(output_dir, data_name)\n",
    "# Check if the file has been downloaded before or it has no recorded windrows, \n",
    "# in either case, skip it and do not download it (again). If you have a partial or \n",
    "# corrupted download, you can delete the file and re-run the script.\n",
    "# adjust to your path if necessary:\n",
    "    litterrows = pd.read_csv('../files/s2_product_unique.csv')\n",
    "    samples_set = set()\n",
    "    for name in litterrows['s2_product']:\n",
    "        product_type = name[0:11]\n",
    "        date_str = name[11:26]\n",
    "        tile = name[39:45]\n",
    "        key = product_type + '_' + date_str + '_' + tile\n",
    "        samples_set.add(key)\n",
    "\n",
    "    file_name = os.path.basename(output_file).replace('.zip', '')\n",
    "    product_type = file_name[0:11]\n",
    "    date_str = file_name[11:26]\n",
    "    tile = file_name[39:45]\n",
    "    key = product_type + '_' + date_str + '_' + tile\n",
    "\n",
    "    if os.path.exists(output_file) and os.path.getsize(output_file) == data_length:\n",
    "        print(output_file + ' this file already exists')   \n",
    "    elif key not in samples_set:\n",
    "        print(output_file + ' has no recorded litter rows')\n",
    "    else:\n",
    "        access_token = get_access_token(CDSE_email, CDSE_password)\n",
    "        download_data(access_token, data_id, data_name, data_length, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66c73f3",
   "metadata": {},
   "source": [
    "# Upload your batch to a new kaggle dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67ae7ef",
   "metadata": {},
   "source": [
    "## edit here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "476c7e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Amend you project root to point to /notebooks \n",
    "## or your current directory where .kaggle/ and SAFE/ folders \n",
    "## must also be located\n",
    "\n",
    "project_root = \"/Users/sara_mac/Desktop/projects/plastic_detection/Sentinel2PlasticDetectionProject/task2-data-collection/notebooks\"\n",
    "dataset_main = Path(project_root) / \"dataset_main\"\n",
    "kaggle_json_path = Path(project_root) / \".kaggle/kaggle.json\"\n",
    "kaggle_config_dir = kaggle_json_path.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f9cb47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_folders = [\n",
    "    Path(project_root) / \"SAFE/PO_2A_19\"  # add multiple if multiple batches\n",
    "]\n",
    "dataset_title = \"Litter Rows Italy Batch 04\" # change only the number to appropriate batch #\n",
    "dataset_id = \"sarahajbane/litter-windrows-batch-4\" # change only the number to appropriate batch #\n",
    "license_name = \"CC-BY-SA-4.0\"\n",
    "\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = str(kaggle_config_dir)\n",
    "os.chmod(kaggle_json_path, 0o600)\n",
    "\n",
    "dataset_main.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e071a1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied: S2A_MSIL1C_20190809T100031_N0500_R122_T33TUL_20230515T073846.zip â†’ PO_2A_19\n",
      "Copied: S2A_MSIL1C_20191008T100031_N0500_R122_T33TUL_20230613T122804.zip â†’ PO_2A_19\n",
      "Copied: S2A_MSIL1C_20190603T101031_N0500_R022_T33TUL_20221221T193704.zip â†’ PO_2A_19\n",
      "Copied: S2A_MSIL1C_20190723T101031_N0500_R022_T33TUL_20230718T015529.zip â†’ PO_2A_19\n",
      "Copied: S2A_MSIL1C_20190630T100031_N0500_R122_T33TUL_20230723T095540.zip â†’ PO_2A_19\n",
      "Copied: S2A_MSIL1C_20190401T100031_N0500_R122_T33TUL_20221019T010704.zip â†’ PO_2A_19\n",
      "Copied: S2A_MSIL1C_20191011T101031_N0500_R022_T33TUL_20230708T021625.zip â†’ PO_2A_19\n",
      "Copied: S2A_MSIL1C_20190720T100031_N0500_R122_T33TUL_20230715T235221.zip â†’ PO_2A_19\n",
      "Copied: S2A_MSIL1C_20190501T100031_N0500_R122_T33TUL_20221219T071444.zip â†’ PO_2A_19\n",
      "Copied: S2A_MSIL1C_20190322T100031_N0500_R122_T33TUL_20221119T025646.zip â†’ PO_2A_19\n",
      "Copied: S2A_MSIL1C_20190819T100031_N0500_R122_T33TUL_20230719T223350.zip â†’ PO_2A_19\n",
      "Copied: S2A_MSIL1C_20190730T100031_N0500_R122_T33TUL_20230710T202111.zip â†’ PO_2A_19\n",
      "Copied: S2A_MSIL1C_20190213T101131_N0500_R022_T33TUL_20221128T153324.zip â†’ PO_2A_19\n",
      "Copied: S2A_MSIL1C_20190620T100031_N0500_R122_T33TUL_20230722T153818.zip â†’ PO_2A_19\n",
      "Copied: S2A_MSIL1C_20190901T101031_N0500_R022_T33TUL_20230701T230615.zip â†’ PO_2A_19\n",
      "Copied: S2A_MSIL1C_20190531T100031_N0500_R122_T33TUL_20221213T082018.zip â†’ PO_2A_19\n",
      "Copied: S2A_MSIL1C_20190315T101021_N0500_R022_T33TUL_20221116T052920.zip â†’ PO_2A_19\n"
     ]
    }
   ],
   "source": [
    "for src_folder in batch_folders:\n",
    "    batch_name = src_folder.name\n",
    "    dest_folder = dataset_main / batch_name\n",
    "    dest_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for zip_file in src_folder.glob(\"*.zip\"):\n",
    "        dest_file = dest_folder / zip_file.name\n",
    "        if not dest_file.exists():\n",
    "            shutil.copy2(zip_file, dest_file)\n",
    "            print(f\"Copied: {zip_file.name} â†’ {batch_name}\")\n",
    "        else:\n",
    "            print(f\"Skipped (already exists): {zip_file.name} in {batch_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "556817ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sara_mac/Desktop/projects/plastic_detection/Sentinel2PlasticDetectionProject/task2-data-collection/notebooks\n"
     ]
    }
   ],
   "source": [
    "## LEAVE AS IS\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "print(project_root)\n",
    "\n",
    "# Function to create or update a Kaggle dataset from a local folder\n",
    "def create_kaggle_dataset_from_folder(\n",
    "    folder_path,\n",
    "    title,\n",
    "    dataset_id,\n",
    "    description=\"Sentinel-2 L1C subset\",\n",
    "    license_name=\"CC-BY-SA-4.0\"\n",
    "):\n",
    "    folder_path = Path(folder_path)\n",
    "    assert folder_path.exists(), \"Folder does not exist!\"\n",
    "\n",
    "    metadata_path = folder_path / \"dataset-metadata.json\"\n",
    "    safe_files = [f.name for f in folder_path.glob(\"*.zip*\")]\n",
    "\n",
    "    resources = [\n",
    "        {\n",
    "            \"name\": Path(zipf).stem,\n",
    "            \"path\": zipf,\n",
    "            \"description\": f\"Zipped .SAFE Sentinel-2: {zipf}\",\n",
    "                \"type\": \"other\",\n",
    "                \"format\": \"zip\"\n",
    "        } for zipf in safe_files\n",
    "    ]\n",
    "\n",
    "    metadata = {\n",
    "        \"title\": title,\n",
    "        \"id\": dataset_id,\n",
    "        \"licenses\": [{\n",
    "            \"name\": license_name,\n",
    "            \"title\": \"Creative Commons Attribution Share-Alike 4.0\",\n",
    "            \"path\": \"https://creativecommons.org/licenses/by-sa/4.0/\"\n",
    "        }],\n",
    "        \"resources\": resources\n",
    "    }\n",
    "\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "\n",
    "    # Initialize if necessary\n",
    "    if not (folder_path / \"dataset-metadata.json\").exists():\n",
    "        subprocess.run([\"kaggle\", \"datasets\", \"init\", \"-u\", str(folder_path)])\n",
    "\n",
    "    # Create or version the dataset\n",
    "    if not any((folder_path / f).exists() for f in [\"dataset-metadata.json\", \"dataset-metadata.yml\"]):\n",
    "        print(\"No metadata found, initializing dataset.\")\n",
    "        subprocess.run([\"kaggle\", \"datasets\", \"init\", \"-u\", str(folder_path)])\n",
    "\n",
    "    try:\n",
    "        subprocess.run([\n",
    "            \"kaggle\", \"datasets\", \"create\",\n",
    "            \"-p\", str(folder_path),\n",
    "            \"--dir-mode\", \"zip\"\n",
    "        ], check=True)\n",
    "    except subprocess.CalledProcessError:\n",
    "        subprocess.run([\n",
    "            \"kaggle\", \"datasets\", \"version\",\n",
    "            \"-p\", str(folder_path),\n",
    "            \"-m\", \"Update data\",\n",
    "            \"--dir-mode\", \"zip\"\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "781e0aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting upload for file S2A_MSIL1C_20190809T100031_N0500_R122_T33TUL_20230515T073846.zip\n",
      "Error while trying to load upload info: ApiStartBlobUploadRequest.__init__() got an unexpected keyword argument 'type'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 658M/658M [04:25<00:00, 2.60MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: S2A_MSIL1C_20190809T100031_N0500_R122_T33TUL_20230515T073846.zip (658MB)\n",
      "Starting upload for file S2A_MSIL1C_20191008T100031_N0500_R122_T33TUL_20230613T122804.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 780M/780M [05:15<00:00, 2.59MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: S2A_MSIL1C_20191008T100031_N0500_R122_T33TUL_20230613T122804.zip (780MB)\n",
      "Starting upload for file S2A_MSIL1C_20190603T101031_N0500_R022_T33TUL_20221221T193704.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 629M/629M [04:13<00:00, 2.60MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: S2A_MSIL1C_20190603T101031_N0500_R022_T33TUL_20221221T193704.zip (629MB)\n",
      "Starting upload for file S2A_MSIL1C_20190723T101031_N0500_R022_T33TUL_20230718T015529.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 607M/607M [04:05<00:00, 2.59MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: S2A_MSIL1C_20190723T101031_N0500_R022_T33TUL_20230718T015529.zip (607MB)\n",
      "Starting upload for file S2A_MSIL1C_20190630T100031_N0500_R122_T33TUL_20230723T095540.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 687M/687M [04:37<00:00, 2.60MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: S2A_MSIL1C_20190630T100031_N0500_R122_T33TUL_20230723T095540.zip (687MB)\n",
      "Starting upload for file S2A_MSIL1C_20190401T100031_N0500_R122_T33TUL_20221019T010704.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 661M/661M [04:27<00:00, 2.59MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: S2A_MSIL1C_20190401T100031_N0500_R122_T33TUL_20221019T010704.zip (661MB)\n",
      "Starting upload for file S2A_MSIL1C_20191011T101031_N0500_R022_T33TUL_20230708T021625.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 612M/612M [04:07<00:00, 2.59MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: S2A_MSIL1C_20191011T101031_N0500_R022_T33TUL_20230708T021625.zip (612MB)\n",
      "Starting upload for file S2A_MSIL1C_20190720T100031_N0500_R122_T33TUL_20230715T235221.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 665M/665M [04:33<00:00, 2.55MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: S2A_MSIL1C_20190720T100031_N0500_R122_T33TUL_20230715T235221.zip (665MB)\n",
      "Starting upload for file S2A_MSIL1C_20190501T100031_N0500_R122_T33TUL_20221219T071444.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 653M/653M [04:24<00:00, 2.59MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: S2A_MSIL1C_20190501T100031_N0500_R122_T33TUL_20221219T071444.zip (653MB)\n",
      "Starting upload for file S2A_MSIL1C_20190322T100031_N0500_R122_T33TUL_20221119T025646.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 671M/671M [04:31<00:00, 2.59MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: S2A_MSIL1C_20190322T100031_N0500_R122_T33TUL_20221119T025646.zip (671MB)\n",
      "Starting upload for file S2A_MSIL1C_20190819T100031_N0500_R122_T33TUL_20230719T223350.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 661M/661M [04:28<00:00, 2.58MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: S2A_MSIL1C_20190819T100031_N0500_R122_T33TUL_20230719T223350.zip (661MB)\n",
      "Starting upload for file S2A_MSIL1C_20190730T100031_N0500_R122_T33TUL_20230710T202111.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 661M/661M [04:28<00:00, 2.58MB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: S2A_MSIL1C_20190730T100031_N0500_R122_T33TUL_20230710T202111.zip (661MB)\n",
      "Starting upload for file S2A_MSIL1C_20190213T101131_N0500_R022_T33TUL_20221128T153324.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 612M/612M [04:06<00:00, 2.60MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: S2A_MSIL1C_20190213T101131_N0500_R022_T33TUL_20221128T153324.zip (612MB)\n",
      "Starting upload for file S2A_MSIL1C_20190620T100031_N0500_R122_T33TUL_20230722T153818.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 709M/709M [04:45<00:00, 2.60MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: S2A_MSIL1C_20190620T100031_N0500_R122_T33TUL_20230722T153818.zip (709MB)\n",
      "Starting upload for file S2A_MSIL1C_20190901T101031_N0500_R022_T33TUL_20230701T230615.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 611M/611M [04:06<00:00, 2.60MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: S2A_MSIL1C_20190901T101031_N0500_R022_T33TUL_20230701T230615.zip (611MB)\n",
      "Starting upload for file S2A_MSIL1C_20190531T100031_N0500_R122_T33TUL_20221213T082018.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 690M/690M [04:39<00:00, 2.59MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: S2A_MSIL1C_20190531T100031_N0500_R122_T33TUL_20221213T082018.zip (690MB)\n",
      "Starting upload for file S2A_MSIL1C_20190315T101021_N0500_R022_T33TUL_20221116T052920.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 598M/598M [04:01<00:00, 2.60MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: S2A_MSIL1C_20190315T101021_N0500_R022_T33TUL_20221116T052920.zip (598MB)\n",
      "Your private Dataset is being created. Please check progress at https://www.kaggle.com/datasets/sarahajbane/litter-windrows-batch-4\n"
     ]
    }
   ],
   "source": [
    "# Run your function\n",
    "\n",
    "folder_path= str(Path(dataset_main / \"PO_2A_19\"))  ### CHANGE FOLDERNAME HERE\n",
    "\n",
    "create_kaggle_dataset_from_folder(\n",
    "    folder_path = folder_path,    \n",
    "    title=dataset_title,\n",
    "    dataset_id= dataset_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b1bb5a",
   "metadata": {},
   "source": [
    " Once you have confirmed the upload to the kaggle dataset, which you can see with the link once completed! Please run the final code block, update the notebook and push your changes to github, or let us know on slack that you have finished the upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f26856b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created report in /Users/sara_mac/Desktop/projects/plastic_detection/Sentinel2PlasticDetectionProject/task2-data-collection/notebooks/SAFE/CALAB_2B/processing_report.txt\n",
      "All tiles uploaded successfully!\n"
     ]
    }
   ],
   "source": [
    "for folder_path in batch_folders:\n",
    "    file_path = os.path.join(folder_path, 'processing_report.txt')\n",
    "    tile_count = len([\n",
    "            entry for entry in os.listdir(folder_path) \n",
    "            if os.path.isfile(os.path.join(folder_path, entry))\n",
    "        ])\n",
    "    \n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(f'Upload finished, {tile_count} tiles processed and uploaded to {dataset_id}')\n",
    "    print(f'Created report in {file_path}')\n",
    "\n",
    "print(\"All tiles uploaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4c27b4",
   "metadata": {},
   "source": [
    "# Finished! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68755f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete files if you want to remove them from your local machine\n",
    "def delete_all_files_in_directory(directory_path):\n",
    "    for file_name in os.listdir(directory_path):\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted: {file_path}\")\n",
    "\n",
    "# This will delete all files in the specified dataset_main subfolder\n",
    "# run this only if you have finished your upload and no longer want the files stored locally\n",
    "# if you want to delete them from the safe folder as well, change \n",
    "# Path(dataset_main) to Path(src_folder) in the line below\n",
    "# Proceed with caution\n",
    "\n",
    "directory_path = Path(dataset_main) / \"PO_2A_19\" # change to appropriate folder name\n",
    "delete_all_files_in_directory(directory_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omdena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
