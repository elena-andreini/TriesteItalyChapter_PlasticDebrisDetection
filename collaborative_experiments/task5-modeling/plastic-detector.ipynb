{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9438808,"sourceType":"datasetVersion","datasetId":5735509},{"sourceId":11795381,"sourceType":"datasetVersion","datasetId":7401788}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n! pip install rasterio\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:13:01.160445Z","iopub.execute_input":"2025-05-18T20:13:01.160714Z","iopub.status.idle":"2025-05-18T20:13:06.219186Z","shell.execute_reply.started":"2025-05-18T20:13:01.160693Z","shell.execute_reply":"2025-05-18T20:13:06.218403Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport sys\nimport shutil\nimport re\nfrom PIL import Image\nimport rasterio\nimport matplotlib.pyplot as plt\nimport dask.array as da\nfrom scipy.ndimage import binary_dilation\nfrom skimage.morphology import disk  # For circular structuring elements\nimport torch\nfrom torchvision import transforms\nimport torchvision.transforms.functional as vF\nimport torch.nn.functional as F\nimport gdown\nfrom tqdm import tqdm\nimport random\n\nfrom sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, jaccard_score, hamming_loss, label_ranking_loss, coverage_error, classification_report\nimport sklearn.metrics as metr\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:13:06.220643Z","iopub.execute_input":"2025-05-18T20:13:06.220944Z","iopub.status.idle":"2025-05-18T20:13:21.253648Z","shell.execute_reply.started":"2025-05-18T20:13:06.220922Z","shell.execute_reply":"2025-05-18T20:13:21.252882Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport random\nfrom torch.utils.data import DataLoader, Dataset\n\ndef set_seed(seed):\n    \"\"\"\n    Set random seeds for NumPy, PyTorch (CPU and GPU), and Python's random module.\n    \n    Args:\n        seed (int): Seed value for RNGs\n    \"\"\"\n    # Python random\n    random.seed(seed)\n    \n    # NumPy\n    np.random.seed(seed)\n    \n    # PyTorch CPU\n    torch.manual_seed(seed)\n    \n    # PyTorch GPU (CUDA)\n    torch.cuda.manual_seed(seed)  # Current GPU\n    torch.cuda.manual_seed_all(seed)  # All GPUs\n    \n    # Ensure deterministic behavior\n    #torch.use_deterministic_algorithms(True)\n    #torch.backends.cudnn.deterministic = True\n    #torch.backends.cudnn.benchmark = False\n\ndef worker_init_fn(worker_id):\n    \"\"\"\n    Initialize random seed for DataLoader workers.\n    Ensures each worker has a unique but reproducible RNG state.\n    \n    Args:\n        worker_id (int): Worker ID\n    \"\"\"\n    max_seed = 2**32 - 1  # NumPy seed limit\n    worker_seed = (torch.initial_seed() + worker_id) % max_seed\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:13:21.254521Z","iopub.execute_input":"2025-05-18T20:13:21.255003Z","iopub.status.idle":"2025-05-18T20:13:21.260428Z","shell.execute_reply.started":"2025-05-18T20:13:21.254982Z","shell.execute_reply":"2025-05-18T20:13:21.259681Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def create_LR_dataframe(splits_path, mode='train'):\n    split_images_files = {'train' : 'train_X.txt', 'val' : 'val_X.txt', 'test' : 'test_X.txt'}\n    split_masks_files = {'train' : 'train_masks.txt', 'val' : 'val_masks.txt', 'test' : 'test_masks.txt'}  \n    with open(os.path.join(splits_path, split_images_files[mode]), \"r\") as file:\n        images = file.readlines()  # Reads all lines into a list\n        images = [image.strip() for image in images]  # Remove any trailing newline characters\n    with open(os.path.join(splits_path, split_masks_files[mode]), \"r\") as file:\n        masks = file.readlines()  # Reads all lines into a list\n        masks = [mask.strip() for mask in masks]  # Remove any trailing newline characters\n    df = pd.DataFrame({'image' : images, 'mask' : masks})\n    return df\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:13:21.261904Z","iopub.execute_input":"2025-05-18T20:13:21.262128Z","iopub.status.idle":"2025-05-18T20:13:21.285252Z","shell.execute_reply.started":"2025-05-18T20:13:21.262111Z","shell.execute_reply":"2025-05-18T20:13:21.284498Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# from Sagar and Navodita's code\ndef compute_fdi_from_tiff(tiff_path):\n    with rasterio.open(tiff_path) as src:\n        # Assuming band order follows your stacked TIFF (B1â€“B12, skipping B10 if needed)\n        # Band indices are 1-based in rasterio\n        R665 = src.read(4)    # B4\n        R859 = src.read(9)    # B8A\n        R1610 = src.read(10)  # B11\n        # Convert to float and mask invalid values\n        R665 = R665.astype(np.float32)\n        R859 = R859.astype(np.float32)\n        R1610 = R1610.astype(np.float32)\n        # Calculate FDI\n        FDI = R859 - (R665 + ((R1610 - R665) * (859 - 665) / (1610 - 665)))\n        return FDI\n\ndef cvt_to_fdi(images):\n    fdi_images = []\n    batch = images.copy()\n    if len(images.shape) == 3 : \n        batch = batch[None, :]\n    for i in range(batch.shape[0]):\n        im = batch[i]\n        R665 = im[3]   # B4\n        R859 = im[8]   # B8A\n        R1610 = im[0]  # B11\n        # Convert to float and mask invalid values\n        R665 = R665.astype(np.float32)\n        R859 = R859.astype(np.float32)\n        R1610 = R1610.astype(np.float32)\n        # Calculate FDI\n        FDI = R859 - (R665 + ((R1610 - R665) * (859 - 665) / (1610 - 665)))\n        fdi_images.append(FDI)\n    return np.array(fdi_images)\n    \ndef compute_ndwi(tiff_path):\n    with rasterio.open(tiff_path) as src:\n        Rgreen = src.read(3).astype(np.float32)  # Band 3 (Green)\n        Rnir = src.read(8).astype(np.float32)    # Band 8 (NIR)\n        ndwi = (Rgreen - Rnir) / (Rgreen + Rnir + 1e-6)  # avoid divide-by-zero\n    return ndwi\ndef plot_fdi(fdi_array, ndwi, img_path, mask_path):\n    with rasterio.open(img_path) as src:\n        rgb = src.read([4, 3, 2])\n        rgb = np.transpose(rgb, (1, 2, 0))\n    # Normalization\n    rgb = rgb.astype(np.float32)\n    rgb = (rgb - rgb.min()) / (rgb.max() - rgb.min())\n    with rasterio.open(mask_path) as src:\n        mask = src.read(1)\n    # Create binary mask\n    mask_binary = mask > 0\n    # Plot side-by-side\n    fig, axs = plt.subplots(1, 4, figsize=(15, 5))\n    axs[0].imshow(rgb)\n    axs[0].set_title(\"RGB Patch\")\n    axs[1].imshow(mask_binary)  #, cmap='gray')\n    axs[1].set_title(\"Binary Mask (._cl.tif)\")\n    axs[2].imshow(fdi_array)\n    axs[2].set_title(\"FDI\")\n    axs[3].imshow(ndwi)\n    axs[3].set_title(\"NDWI\")\n    for ax in axs:\n        ax.axis('off')\n\n    # with rasterio.open(patch_path) as patch_src:\n    #     rgb = patch_src.read([4, 3, 2])  # Use bands B4, B3, B2 for RGB\n    #     rgb = np.transpose(rgb, (1, 2, 0))\n    #     rgb = (rgb - np.min(rgb)) / (np.max(rgb) - np.min(rgb) + 1e-6)\n    import matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\n\n# List of image and mask file paths (replace with your file paths)\nimage_mask_pairs = [\n    ('path_to_image1.jpg', 'path_to_mask1.png'),\n    ('path_to_image2.jpg', 'path_to_mask2.png'),\n    # Add more pairs as needed\n]\n\n\ndef cvt_RGB(images):\n    rgb_images = []\n    for i in range(images.shape[0]):\n        rgb = images[i][[4-1, 3-1, 2-1]] # Use bands B4, B3, B2 for RGB\n        rgb = np.transpose(rgb, (1, 2, 0))\n        rgb = (rgb - np.min(rgb)) / (np.max(rgb) - np.min(rgb) + 1e-6)\n        rgb_images.append(rgb)\n    return np.array(rgb_images)\n\ndef display(images, masks):\n    # Determine the number of pairs\n    num_pairs = images.shape[0]\n\n    # Calculate layout: use 2 columns per pair (image + mask), adjust rows dynamically\n    cols = 2  # One column for image, one for mask\n    rows = num_pairs  # One row per pair\n\n    # Create a figure with subplots\n    fig, axes = plt.subplots(rows, cols, figsize=(5 * cols, 5 * rows))\n\n    # Handle case of single pair (axes is not a 2D array)\n    if num_pairs == 1:\n        axes = np.array([axes]).reshape(1, -1)\n\n    # Iterate through each pair and display image and mask\n    for idx, (image, mask) in enumerate(zip(images, masks)):\n\n        # Display the original image\n        axes[idx, 0].imshow(image)\n        axes[idx, 0].set_title(f'Image {idx + 1}')\n        axes[idx, 0].axis('off')  # Hide axes\n    \n        # Display the segmentation mask\n        axes[idx, 1].imshow(mask, cmap='gray')  # Adjust cmap if needed\n        axes[idx, 1].set_title(f'Mask {idx + 1}')\n        axes[idx, 1].axis('off')  # Hide axes\n\n    # Adjust layout to prevent overlap\n    plt.tight_layout()\n    \n    # Show the plot\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:14:31.202261Z","iopub.execute_input":"2025-05-18T20:14:31.202851Z","iopub.status.idle":"2025-05-18T20:14:31.218162Z","shell.execute_reply.started":"2025-05-18T20:14:31.202829Z","shell.execute_reply":"2025-05-18T20:14:31.217512Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"\ndef extract_date_tile(filename):\n    \"\"\"Extract date and tile from filename using regex.\"\"\"\n    pattern = r'^(\\d{1,2}-\\d{1,2}-\\d{2})_([A-Z0-9]+)_\\d+$'\n    match = re.match(pattern, filename)\n    if not match:\n        raise ValueError(f\"Invalid filename format: {filename}\")\n    return match.groups()  # Returns tuple (date, tile)\n\ndef create_marida_df(data_path, mode='train'):\n    \"\"\"Create DataFrame from MARIDA dataset files.\"\"\"\n    # Determine split file based on mode\n    split_files = {'train': 'train_X.txt', 'val': 'val_X.txt', 'test': 'test_X.txt'}\n    items_list_path = os.path.join(data_path, 'splits', split_files[mode])\n\n    # Read items list\n    with open(items_list_path, 'r') as file:\n        items = [item.strip() for item in file]\n\n    # Base path for patches\n    items_path = os.path.join(data_path, 'patches')\n\n    # Prepare data lists\n    data = {\n        'image': [],\n        'mask': [],\n        'confidence': [],\n        'date': [],\n        'tile': []\n    }\n\n    # Process each item\n    for item in items:\n        tile = \"_\".join(item.split(\"_\")[:-1])\n        tile_path = os.path.join(items_path, f\"S2_{tile}\")\n\n        # Define file paths\n        base_name = f'S2_{item}'\n        paths = {\n            'image': os.path.join(tile_path, f'{base_name}.tif'),\n            'mask': os.path.join(tile_path, f'{base_name}_cl.tif'),\n            'confidence': os.path.join(tile_path, f'{base_name}_conf.tif')\n        }\n\n        # Check if all files exist\n        if all(os.path.exists(p) for p in paths.values()):\n            data['image'].append(paths['image'])\n            data['mask'].append(paths['mask'])\n            data['confidence'].append(paths['confidence'])\n            date, tile = extract_date_tile(item)\n            data['date'].append(date)\n            data['tile'].append(tile)\n\n    return pd.DataFrame(data)\n\n# MARIDA labels dictionary\nMARIDA_LABELS = {\n    i: label for i, label in enumerate([\n        'Marine Debris', 'Dense Sargassum', 'Sparse Sargassum', 'Natural Organic Material',\n        'Ship', 'Clouds', 'Marine Water', 'Sediment-Laden Water', 'Foam', 'Turbid Water',\n        'Shallow Water', 'Waves', 'Cloud Shadows', 'Wakes', 'Mixed Water'\n    ], 1)\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:14:31.419304Z","iopub.execute_input":"2025-05-18T20:14:31.419585Z","iopub.status.idle":"2025-05-18T20:14:31.427696Z","shell.execute_reply.started":"2025-05-18T20:14:31.419565Z","shell.execute_reply":"2025-05-18T20:14:31.426937Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import rasterio\nimport numpy as np\n\ndef compute_invalid_pixels(image_paths, mask_paths):\n    \"\"\"\n    Compute per-band statistics for Sentinel-2 L1C ACOLITE-processed images using segmentation masks.\n    Creates a mask to exclude invalid pixels (NaNs, negative values, specified no-data value).\n    \n    Parameters:\n    - image_paths: List of paths to image files (e.g., GeoTIFF with 11 bands).\n    - mask_paths: List of paths to segmentation mask files (single-band, integer class labels).\n    - class_labels: List of mask class labels to include (e.g., [1, 2] for vegetation and water).\n                   If None, include all non-zero labels (excluding background).\n    - invalid_value: Optional value to treat as invalid in images (e.g., -9999).\n    \n    Returns:\n    - mean_per_band: List of per-band means for each image.\n    - std_per_band: List of per-band standard deviations for each image.\n    \"\"\"\n    mean_per_band = []  # Initialize as list\n    std_per_band = []   # Initialize as list\n    positive_pixels = []\n    tot_pixels = [];\n    images_with_invalid_pixels = []\n    black_list = []\n    accumulator = None\n    no_data_pixels = []\n    neg_pixels = []\n    nan_pixels = []\n    gt1_pixels = []\n    imgs_with_invalid = []\n    positive_pixels = []\n    min_vals = []\n    max_vals = []\n    for img_path, mask_path in zip(image_paths, mask_paths):\n        # Load image and mask\n        with rasterio.open(img_path) as src_img, rasterio.open(mask_path) as src_mask:\n            image = src_img.read()  # Shape: (bands, height, width)\n            mask = src_mask.read(1)  # Shape: (height, width)\n            \n            # Convert image to float for NaN handling\n            image = image.astype(float)\n\n            nan_mask = np.isnan(image)\n            neg_mask = (image < 0)\n            too_big_mask = (image > 1)\n            no_data_mask = (image == src_img.nodata)\n            nan_pixels.append(np.sum(nan_mask))\n            neg_pixels.append(np.sum(neg_mask))\n            gt1_pixels.append(np.sum(too_big_mask))\n            no_data_pixels.append(np.sum(no_data_mask))\n            imgs_with_invalid.append(img_path)\n            positive_pixels.append(np.sum(mask > 0))\n            min_vals.append(np.min(image))\n            max_vals.append(np.max(image))\n    df = pd.DataFrame({'image' : imgs_with_invalid, 'no data pixels' : no_data_pixels, 'negative pixels' : neg_pixels,\n                      'nan pixels' : nan_pixels, 'high value pixels' :  gt1_pixels, 'debris pixels' : positive_pixels,\n                      'min values' : min_vals, 'max values' : max_vals})\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:14:31.602598Z","iopub.execute_input":"2025-05-18T20:14:31.602888Z","iopub.status.idle":"2025-05-18T20:14:31.610520Z","shell.execute_reply.started":"2025-05-18T20:14:31.602867Z","shell.execute_reply":"2025-05-18T20:14:31.609704Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nbatch_size=16","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:14:31.721093Z","iopub.execute_input":"2025-05-18T20:14:31.721759Z","iopub.status.idle":"2025-05-18T20:14:31.785150Z","shell.execute_reply.started":"2025-05-18T20:14:31.721726Z","shell.execute_reply":"2025-05-18T20:14:31.784428Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def compute_stats(image_files, discard_negatives = False, discard_gt_1 = False):\n    bands_std = []\n    bands_mean = []\n    valid_pixels = []\n\n    for band_idx in range(11):\n        arrays = [da.from_array(rasterio.open(f).read(band_idx + 1), chunks='auto')\n                  for f in image_files]\n        stack = da.stack(arrays)\n        #valid = (stack != rasterio.open(image_files[0]).nodata) & (stack >= 0)\n        if discard_negatives and  discard_gt_1: \n            valid = da.stack([da.from_array(rasterio.open(f).read(band_idx + 1) != rasterio.open(f).nodata, chunks='auto')\n                              & (da.from_array(rasterio.open(f).read(band_idx + 1), chunks='auto') >= 0) & \n                              (da.from_array(rasterio.open(f).read(band_idx + 1), chunks='auto') <= 1) \n                              for f in image_files])\n        elif discard_gt_1 :\n            valid = da.stack([da.from_array(rasterio.open(f).read(band_idx + 1) != rasterio.open(f).nodata, chunks='auto')\n                              & (da.from_array(rasterio.open(f).read(band_idx + 1), chunks='auto') <= 1)  \n                              for f in image_files])\n        elif discard_negatives:\n            valid = da.stack([da.from_array(rasterio.open(f).read(band_idx + 1) != rasterio.open(f).nodata, chunks='auto')\n                  & (da.from_array(rasterio.open(f).read(band_idx + 1), chunks='auto') >= 0) \n                  for f in image_files])\n        else :\n            valid = da.stack([da.from_array(rasterio.open(f).read(band_idx + 1) != rasterio.open(f).nodata, chunks='auto')\n                  for f in image_files])\n                         \n        # Compute number of valid pixels\n        valid_count = da.sum(valid).compute()\n        valid_pixels.append(valid_count)\n        mean = da.nanmean(stack[valid]).compute()\n        std = da.nanstd(stack[valid]).compute()\n        bands_mean.append(mean)\n        bands_std.append(std)\n        print(f\"Band {band_idx} - Mean: {mean}, Std: {std}\")\n    return {'mean' : np.array(bands_mean), 'std': np.array(bands_std),'valid pixels' : np.array(valid_pixels) }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:14:31.851888Z","iopub.execute_input":"2025-05-18T20:14:31.852265Z","iopub.status.idle":"2025-05-18T20:14:31.861165Z","shell.execute_reply.started":"2025-05-18T20:14:31.852243Z","shell.execute_reply":"2025-05-18T20:14:31.860405Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def computing_labeled_pixels_stats(mask_paths):\n    arrays = [da.from_array(rasterio.open(f).read(1), chunks='auto')\n                  for f in mask_paths]\n    stack = da.stack(arrays)\n    valid = stack > 0\n    labeled_count = da.sum(valid).compute()\n    return labeled_count","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:14:37.515710Z","iopub.execute_input":"2025-05-18T20:14:37.515996Z","iopub.status.idle":"2025-05-18T20:14:37.520700Z","shell.execute_reply.started":"2025-05-18T20:14:37.515977Z","shell.execute_reply":"2025-05-18T20:14:37.520014Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def compute_invalid_mask(path):\n    with rasterio.open(path) as src:\n        image = src.read()\n        \n        invalid_mask = image == src.nodata\n        invalid_mask |= np.isnan(image)\n        invalid_mask |= image < 0\n        invalid_mask |= image > 1\n        invalid_mask = np.any(invalid_mask, axis=0)\n        return invalid_mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:14:37.713405Z","iopub.execute_input":"2025-05-18T20:14:37.713678Z","iopub.status.idle":"2025-05-18T20:14:37.717987Z","shell.execute_reply.started":"2025-05-18T20:14:37.713659Z","shell.execute_reply":"2025-05-18T20:14:37.717296Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def get_invalid_mask(image, no_data):\n    invalid_mask = image == no_data\n    invalid_mask |= np.isnan(image)\n    #invalid_mask |= image < 0\n    invalid_mask |= image > 1\n    #invalid_mask = np.any(invalid_mask, axis=0)\n    return invalid_mask  #torch.fromnumpy(invalid_mask)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:14:37.913412Z","iopub.execute_input":"2025-05-18T20:14:37.913929Z","iopub.status.idle":"2025-05-18T20:14:37.917539Z","shell.execute_reply.started":"2025-05-18T20:14:37.913908Z","shell.execute_reply":"2025-05-18T20:14:37.916898Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def select_bg_pixels(image, debris_mask, r1=5, r2=20, target_ratio=5):\n    H, W = debris_mask.shape\n    \n    #target_ratio = 5  # Debris-to-background ratio (1:5)\n\n    # Create structuring elements (circular or square)\n    se_r1 = disk(r1) if r1 > 0 else np.ones((1, 1))  # Inner dilation kernel\n    se_r2 = disk(r2)                         # Outer dilation kernel\n    #print('before binary dilation')\n    # Dilate debris mask with r1 and r2\n    dilated_r1 = binary_dilation(debris_mask, structure=se_r1)\n    dilated_r2 = binary_dilation(debris_mask, structure=se_r2)\n    #print('before anular mask')\n    # Compute annular region: pixels in dilated_r2 but not in dilated_r1\n    annular_mask = dilated_r2 & ~dilated_r1\n\n    # Sample background pixels from annular region\n    valid_background_coords = np.where(annular_mask)\n    num_debris = np.sum(debris_mask)\n    num_background = min(len(valid_background_coords[0]), num_debris * target_ratio)\n    if num_background > 0:\n        sample_idx = np.random.choice(len(valid_background_coords[0]), size=num_background, replace=False)\n        background_coords = [(valid_background_coords[0][i], valid_background_coords[1][i]) for i in sample_idx]\n    else:\n        print(\"Warning: No valid background pixels in annular region. Increase r2 or check mask.\")\n\n    # Create background mask (optional, for visualization or training)\n    background_mask = np.zeros_like(debris_mask)\n    for x, y in background_coords:\n        background_mask[x, y] = 1\n    return background_mask\n\n# Optional: Filter by features (e.g., RGB values for water-like pixels)\n# Example: If image is RGB, filter pixels with low green channel (common for water)\n# image = ...  # Your RGB or multispectral image\n# valid_background = [coord for coord in background_coords if image[coord[0], coord[1], 1] < threshold]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:14:38.073978Z","iopub.execute_input":"2025-05-18T20:14:38.074686Z","iopub.status.idle":"2025-05-18T20:14:38.080351Z","shell.execute_reply.started":"2025-05-18T20:14:38.074663Z","shell.execute_reply":"2025-05-18T20:14:38.079640Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def batch_process_marida_masks(masks, dataset_ids, device='cpu'):\n    \"\"\"\n    Process masks for dataset_id == 0 (MARIDA) at the batch level.\n    - Set classes [1, 2, 3, 4, 9] to 2 (debris).\n    - Set class 0 to 0 (unlabeled), other classes to 1 (non-debris).\n    \n    Args:\n        masks: Tensor [batch_size, H, W] (integer-valued masks)\n        dataset_ids: Tensor [batch_size] (dataset IDs)\n        device: Device for PyTorch operations ('cpu' or 'cuda')\n    \n    Returns:\n        marida_masks: Tensor [batch_size, H, W] with values 0, 1, 2\n    \"\"\"\n    batch_size, H, W = masks.shape\n    marida_masks = torch.zeros_like(masks, dtype=torch.int64, device=device)\n    \n    # Identify masks with dataset_id == 0\n    marida_mask = (dataset_ids == 0)  # [batch_size], boolean\n    if not marida_mask.any():\n        return marida_masks\n    \n    # Select masks for dataset_id == 0\n    selected_masks = masks[marida_mask]  # [num_marida, H, W]\n    \n    # Set classes [1, 2, 3, 4, 9] to 2\n    debris_classes = torch.tensor([1, 2, 3, 4, 9], device=device)\n    is_debris = torch.isin(selected_masks, debris_classes)  # [num_marida, H, W]\n    marida_masks[marida_mask] = torch.where(\n        is_debris,\n        torch.tensor(2, dtype=torch.int64, device=device),\n        selected_masks  # Temporarily keep original values\n    )\n    # for idx in range( marida_masks[marida_mask].shape[0]):\n    #     print(f' {idx} has {torch.sum(is_debris[idx])} : {torch.unique(marida_masks[marida_mask][idx])}')\n    # Set non-zero, non-debris pixels to 1\n    marida_masks[marida_mask] = torch.where(\n        (marida_masks[marida_mask] != 0) & (marida_masks[marida_mask] != 2),\n        torch.tensor(1, dtype=torch.int64, device=device),\n        marida_masks[marida_mask]\n    )\n    # print('only 3 values : ')\n    # for idx in range( marida_masks[marida_mask].shape[0]):\n    #     print(f' {idx} has {torch.sum(is_debris[idx])} : {torch.unique(marida_masks[marida_mask][idx])}')\n    marida_masks[marida_mask] = marida_masks[marida_mask] - 1\n    #print('after subtr')\n    # for idx in range( marida_masks[marida_mask].shape[0]):\n    #     print(f' {idx} has {torch.sum(is_debris[idx])} : {torch.unique(marida_masks[marida_mask][idx])}')\n    return marida_masks\n\n\n\n# # Custom collate function\n# def custom_collate_fn(batch):\n#     images, masks, dataset_ids = zip(*batch)\n#     images = torch.stack(images)\n#     masks = torch.stack(masks)\n#     dataset_ids = torch.tensor(dataset_ids, dtype=torch.long)\n    \n#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n#     images, masks, dataset_ids = images.to(device), masks.to(device), dataset_ids.to(device)\n    \n#     final_masks = batch_select_bg_pixels(images, masks, dataset_ids, r1=5, r2=20, \n#                                          target_ratio=5, threshold=0.5, device=device)\n    \n#     return images, masks, final_masks, dataset_ids\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:14:38.229417Z","iopub.execute_input":"2025-05-18T20:14:38.230069Z","iopub.status.idle":"2025-05-18T20:14:38.235852Z","shell.execute_reply.started":"2025-05-18T20:14:38.230036Z","shell.execute_reply":"2025-05-18T20:14:38.235258Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"\ndef torch_dilate(mask, kernel_size, device='cpu'):\n    \"\"\"Apply dilation to a batch of masks using PyTorch convolution.\"\"\"\n    kernel = torch.ones(1, 1, kernel_size, kernel_size, device=device, dtype=torch.float32)\n    mask = mask.float().unsqueeze(1)  # [batch_size, 1, H, W]\n    dilated = torch.nn.functional.conv2d(mask, kernel, padding=kernel_size // 2) > 0\n    return dilated.squeeze(1).bool()  # [batch_size, H, W]\n\ndef batch_select_bg_pixels(images, masks, dataset_ids, r1=5, r2=20, target_ratio=5, threshold=None, device='cpu'):\n    \"\"\"\n    Compute annular background masks for a batch of masks, only for dataset_id == 1.\n    - Set debris pixels (masks == 1) to 2 in bg_masks.\n    - Set randomly sampled annular pixels to 1 in bg_masks.\n    \n    Args:\n        images: Tensor [batch_size, C, H, W] \n        masks: Tensor [batch_size, H, W] (binary debris masks)\n        dataset_ids: Tensor [batch_size] (dataset IDs)\n        r1, r2: Radii for inner and outer dilation\n        target_ratio: Debris-to-background pixel ratio\n        threshold: Optional threshold for filtering (e.g., green channel)\n        device: Device for PyTorch operations ('cpu' or 'cuda')\n    \n    Returns:\n        bg_masks: Tensor [batch_size, H, W] with values 0 (default), 1 (background), 2 (debris)\n    \"\"\"\n\n    batch_size, H, W = masks.shape\n    # Initialize bg_masks with zeros (int64 to support values 0, 1, 2)\n    bg_masks = torch.zeros_like(masks, dtype=torch.int64, device=device)\n    \n    # Identify masks to process (dataset_id == 1)\n    valid_mask = (dataset_ids == 1)  # [batch_size], boolean{\n    #print(f'LR indices {valid_mask}')\n    if not valid_mask.any():\n        return bg_masks  # Return zeros if no masks need processing\n    \n    # Select masks for dataset_id == 1\n    selected_masks = masks[valid_mask]  # [num_valid, H, W]\n    # for idx in range(selected_masks.shape[0]):\n    #     print(f'num debris pixels : {torch.sum(selected_masks[idx])}')\n    # Set debris pixels to 2 for selected masks\n    bg_masks[valid_mask] = selected_masks * 2  # Where selected_masks == 1, set bg_masks to 2\n    \n    # Perform dilation on selected masks\n    dilated_r1 = torch_dilate(selected_masks, 2 * r1 + 1, device=device)  # [num_valid, H, W]\n    dilated_r2 = torch_dilate(selected_masks, 2 * r2 + 1, device=device)  # [num_valid, H, W]\n    annular_masks = dilated_r2 & ~dilated_r1  # [num_valid, H, W]\n    \n    # Sample background pixels for each selected mask\n    for idx in range(annular_masks.shape[0]):\n        valid_coords = torch.where(annular_masks[idx])  # Tuple of (row, col) indices\n        #print(f'unique values in mask {idx} : {torch.unique(selected_masks[idx])}')\n        num_debris = torch.sum(selected_masks[idx] > 0).item()\n        #print(f'num debris for index {idx} : {num_debris}')\n        num_background = min(len(valid_coords[0]), int(num_debris * target_ratio))\n        \n        if num_background > 0:\n            # Randomly sample indices and set to 1\n            sample_indices = torch.randperm(len(valid_coords[0]), device=device)[:num_background]\n            bg_masks[valid_mask.nonzero(as_tuple=True)[0][idx], \n                     valid_coords[0][sample_indices], \n                     valid_coords[1][sample_indices]] = 1\n        else :\n            print(f'no background selected for index {idx}. Num debrid : {num_debris} Num background : {num_background}')\n            print(f'valid coords {len(valid_coords)}')\n            print(f'unique valus : {torch.unique(selected_masks[idx])}')\n    \n    # # Optional: Filter by image features (e.g., green channel) for dataset_id == 1\n    # if threshold is not None and images is not None:\n    #     valid_pixels = images[valid_mask, 1, :, :] < threshold  # Green channel\n    #     # Only apply filtering to background pixels (value 1), preserve debris pixels (value 2)\n    #     bg_masks[valid_mask] = torch.where(\n    #         bg_masks[valid_mask] == 1,\n    #         bg_masks[valid_mask] & valid_pixels,\n    #         bg_masks[valid_mask]\n    #     )\n    bg_masks[valid_mask] = bg_masks[valid_mask] - 1\n    return bg_masks\n\n# Custom collate function\ndef custom_collate_fn(batch):\n    # print(f'custom collate function batch {len(batch)}')\n    # print(f'custom collate function batch type {type(batch)}')\n    # print(f'custom collate function batch[1] type {type(batch[1])}')\n    # print(f'custom collate function batch[1] len  {len(batch[1])}')\n    images, masks, dataset_ids = zip(*batch)\n    images = torch.stack(images)  # [batch_size, C, H, W]\n    masks = torch.stack(masks)    # [batch_size, H, W]\n    dataset_ids = torch.tensor(dataset_ids, dtype=torch.long)  # [batch_size]\n    \n    # Move to GPU if available\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    images, masks, dataset_ids = images.to(device), masks.to(device), dataset_ids.to(device)\n    \n    # Compute background masks\n    lr_masks = batch_select_bg_pixels(images, masks, dataset_ids, r1=5, r2=20, \n                                      target_ratio=40, device=device)\n    marida_masks = batch_process_marida_masks(masks, dataset_ids, device=device)\n    masks = lr_masks + marida_masks\n    \n    return images, masks, dataset_ids\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:14:42.888631Z","iopub.execute_input":"2025-05-18T20:14:42.888916Z","iopub.status.idle":"2025-05-18T20:14:42.899512Z","shell.execute_reply.started":"2025-05-18T20:14:42.888896Z","shell.execute_reply":"2025-05-18T20:14:42.898819Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Seeding for reproducibility\nseed = 42\nset_seed(seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:14:43.111634Z","iopub.execute_input":"2025-05-18T20:14:43.112208Z","iopub.status.idle":"2025-05-18T20:14:43.120606Z","shell.execute_reply.started":"2025-05-18T20:14:43.112183Z","shell.execute_reply":"2025-05-18T20:14:43.120002Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"%%capture\n# Download some pre-computed data \n\n\nfile_id = \"1NvgyeN-k-pRXF114IFhB-AcW86M9e7km\"\ngdown.download(f'https://drive.google.com/uc?id={file_id}', '/kaggle/working/marida_stats.npz', quiet=False)\nfile_id = \"160mw3xELYG_44yemtf2vrBg6n-Q_e8LO\"\ngdown.download(f'https://drive.google.com/uc?id={file_id}', '/kaggle/working/marida_df_invalid_info.csv', quiet=False)\nfile_id = \"1a-4sJZ4NUZsNHuaSBRKoAG7Dy2O4kHWj\"\ngdown.download(f'https://drive.google.com/uc?id={file_id}', '/kaggle/working/litter_rows_df_invalid_info.csv', quiet=False)\nfile_id = \"1sEiP73c4I3S4KK-58c6J02wgmayu3EYa\"\ngdown.download(f'https://drive.google.com/uc?id={file_id}', '/kaggle/working/litter_rows_stats.npz', quiet=False)\nfile_id = \"1wrD41CDQud69AMOyHigw0-DR85Id4zDM\"\ngdown.download(f'https://drive.google.com/uc?id={file_id}', '/kaggle/working/global_stats.npz', quiet=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:14:43.289730Z","iopub.execute_input":"2025-05-18T20:14:43.290413Z","iopub.status.idle":"2025-05-18T20:15:02.468103Z","shell.execute_reply.started":"2025-05-18T20:14:43.290392Z","shell.execute_reply":"2025-05-18T20:15:02.467459Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# check that the \n! ls /kaggle/input/litter-windrows-patches\n# add the lr dataset to path to import code to prepare the dataset\nsys.path.append('/kaggle/input/litter-windrows-patches')\n# import functions to prepare dataset\nfrom prepare_dataset import  get_image_and_mask_paths, split_and_save_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:15:02.469212Z","iopub.execute_input":"2025-05-18T20:15:02.469462Z","iopub.status.idle":"2025-05-18T20:15:02.637934Z","shell.execute_reply.started":"2025-05-18T20:15:02.469438Z","shell.execute_reply":"2025-05-18T20:15:02.637019Z"}},"outputs":[{"name":"stdout","text":"annotations  prepare_dataset.ipynb  README.md\npatches      prepare_dataset.py     splits\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"#! git clone https://github.com/sheikhazhanmohammed/SADMA.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:15:02.639000Z","iopub.execute_input":"2025-05-18T20:15:02.639297Z","iopub.status.idle":"2025-05-18T20:15:02.643201Z","shell.execute_reply.started":"2025-05-18T20:15:02.639272Z","shell.execute_reply":"2025-05-18T20:15:02.642495Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"#sys.path.append('/kaggle/working/SADMA')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:15:02.644944Z","iopub.execute_input":"2025-05-18T20:15:02.645349Z","iopub.status.idle":"2025-05-18T20:15:02.657622Z","shell.execute_reply.started":"2025-05-18T20:15:02.645325Z","shell.execute_reply":"2025-05-18T20:15:02.656859Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# define a variable for the lr dataset\nLW_path = '/kaggle/input/litter-windrows-patches'\nlr_images, lr_masks = get_image_and_mask_paths(LW_path)\n! mkdir ./LR_splits\nsplit_and_save_data(lr_images, lr_masks, './LR_splits' )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:15:02.658475Z","iopub.execute_input":"2025-05-18T20:15:02.658707Z","iopub.status.idle":"2025-05-18T20:15:05.076844Z","shell.execute_reply.started":"2025-05-18T20:15:02.658691Z","shell.execute_reply":"2025-05-18T20:15:05.076045Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"! ls ./LR_splits/splits\nLR_splits_path = '/kaggle/working/LR_splits/splits'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:15:05.078032Z","iopub.execute_input":"2025-05-18T20:15:05.078327Z","iopub.status.idle":"2025-05-18T20:15:05.216005Z","shell.execute_reply.started":"2025-05-18T20:15:05.078302Z","shell.execute_reply":"2025-05-18T20:15:05.215220Z"}},"outputs":[{"name":"stdout","text":"test_masks.txt\ttrain_masks.txt  val_masks.txt\ntest_X.txt\ttrain_X.txt\t val_X.txt\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# from IPython.display import display\n\n# with open(LR_splits_path+'/train_X.txt', \"r\") as file:\n#     display(file.read())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:15:05.217097Z","iopub.execute_input":"2025-05-18T20:15:05.217369Z","iopub.status.idle":"2025-05-18T20:15:05.221503Z","shell.execute_reply.started":"2025-05-18T20:15:05.217334Z","shell.execute_reply":"2025-05-18T20:15:05.220750Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"! ls /kaggle/input/marida-marine-debrish-dataset\nMARIDA_path = '/kaggle/input/marida-marine-debrish-dataset'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:15:05.222332Z","iopub.execute_input":"2025-05-18T20:15:05.222580Z","iopub.status.idle":"2025-05-18T20:15:05.379817Z","shell.execute_reply.started":"2025-05-18T20:15:05.222559Z","shell.execute_reply":"2025-05-18T20:15:05.379102Z"}},"outputs":[{"name":"stdout","text":"labels_mapping.txt  patches  shapefiles  splits\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"lr_df = create_LR_dataframe(LR_splits_path)\npd.set_option(\"display.max_colwidth\", None)\nlr_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:15:05.381379Z","iopub.execute_input":"2025-05-18T20:15:05.381670Z","iopub.status.idle":"2025-05-18T20:15:05.408935Z","shell.execute_reply.started":"2025-05-18T20:15:05.381638Z","shell.execute_reply":"2025-05-18T20:15:05.408290Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"                                                                                                                                            image  \\\n0  /kaggle/input/litter-windrows-patches/patches/S2A_MSIL1C_20161126T094332_R036_T33SXC/S2A_MSIL1C_20161126T094332_R036_T33SXC_646080_4256500.tif   \n1  /kaggle/input/litter-windrows-patches/patches/S2A_MSIL1C_20180615T100031_R122_T33TUL/S2A_MSIL1C_20180615T100031_R122_T33TUL_353760_5056480.tif   \n2  /kaggle/input/litter-windrows-patches/patches/S2A_MSIL1C_20160814T100032_R122_T33TUL/S2A_MSIL1C_20160814T100032_R122_T33TUL_374240_5056480.tif   \n3  /kaggle/input/litter-windrows-patches/patches/S2A_MSIL1C_20210331T100021_R122_T33TUL/S2A_MSIL1C_20210331T100021_R122_T33TUL_379360_5020640.tif   \n4  /kaggle/input/litter-windrows-patches/patches/S2A_MSIL1C_20150830T100016_R122_T33TUL/S2A_MSIL1C_20150830T100016_R122_T33TUL_358880_5066720.tif   \n\n                                                                                                                                                mask  \n0  /kaggle/input/litter-windrows-patches/patches/S2A_MSIL1C_20161126T094332_R036_T33SXC/S2A_MSIL1C_20161126T094332_R036_T33SXC_646080_4256500_cl.tif  \n1  /kaggle/input/litter-windrows-patches/patches/S2A_MSIL1C_20180615T100031_R122_T33TUL/S2A_MSIL1C_20180615T100031_R122_T33TUL_353760_5056480_cl.tif  \n2  /kaggle/input/litter-windrows-patches/patches/S2A_MSIL1C_20160814T100032_R122_T33TUL/S2A_MSIL1C_20160814T100032_R122_T33TUL_374240_5056480_cl.tif  \n3  /kaggle/input/litter-windrows-patches/patches/S2A_MSIL1C_20210331T100021_R122_T33TUL/S2A_MSIL1C_20210331T100021_R122_T33TUL_379360_5020640_cl.tif  \n4  /kaggle/input/litter-windrows-patches/patches/S2A_MSIL1C_20150830T100016_R122_T33TUL/S2A_MSIL1C_20150830T100016_R122_T33TUL_358880_5066720_cl.tif  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>mask</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/litter-windrows-patches/patches/S2A_MSIL1C_20161126T094332_R036_T33SXC/S2A_MSIL1C_20161126T094332_R036_T33SXC_646080_4256500.tif</td>\n      <td>/kaggle/input/litter-windrows-patches/patches/S2A_MSIL1C_20161126T094332_R036_T33SXC/S2A_MSIL1C_20161126T094332_R036_T33SXC_646080_4256500_cl.tif</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/litter-windrows-patches/patches/S2A_MSIL1C_20180615T100031_R122_T33TUL/S2A_MSIL1C_20180615T100031_R122_T33TUL_353760_5056480.tif</td>\n      <td>/kaggle/input/litter-windrows-patches/patches/S2A_MSIL1C_20180615T100031_R122_T33TUL/S2A_MSIL1C_20180615T100031_R122_T33TUL_353760_5056480_cl.tif</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/litter-windrows-patches/patches/S2A_MSIL1C_20160814T100032_R122_T33TUL/S2A_MSIL1C_20160814T100032_R122_T33TUL_374240_5056480.tif</td>\n      <td>/kaggle/input/litter-windrows-patches/patches/S2A_MSIL1C_20160814T100032_R122_T33TUL/S2A_MSIL1C_20160814T100032_R122_T33TUL_374240_5056480_cl.tif</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/litter-windrows-patches/patches/S2A_MSIL1C_20210331T100021_R122_T33TUL/S2A_MSIL1C_20210331T100021_R122_T33TUL_379360_5020640.tif</td>\n      <td>/kaggle/input/litter-windrows-patches/patches/S2A_MSIL1C_20210331T100021_R122_T33TUL/S2A_MSIL1C_20210331T100021_R122_T33TUL_379360_5020640_cl.tif</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/litter-windrows-patches/patches/S2A_MSIL1C_20150830T100016_R122_T33TUL/S2A_MSIL1C_20150830T100016_R122_T33TUL_358880_5066720.tif</td>\n      <td>/kaggle/input/litter-windrows-patches/patches/S2A_MSIL1C_20150830T100016_R122_T33TUL/S2A_MSIL1C_20150830T100016_R122_T33TUL_358880_5066720_cl.tif</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"marida_df = create_marida_df(MARIDA_path)\nmarida_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:15:05.411562Z","iopub.execute_input":"2025-05-18T20:15:05.411830Z","iopub.status.idle":"2025-05-18T20:15:12.249219Z","shell.execute_reply.started":"2025-05-18T20:15:05.411815Z","shell.execute_reply":"2025-05-18T20:15:12.248483Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"                                                                                         image  \\\n0  /kaggle/input/marida-marine-debrish-dataset/patches/S2_1-12-19_48MYU/S2_1-12-19_48MYU_0.tif   \n1  /kaggle/input/marida-marine-debrish-dataset/patches/S2_1-12-19_48MYU/S2_1-12-19_48MYU_1.tif   \n2  /kaggle/input/marida-marine-debrish-dataset/patches/S2_1-12-19_48MYU/S2_1-12-19_48MYU_2.tif   \n3  /kaggle/input/marida-marine-debrish-dataset/patches/S2_1-12-19_48MYU/S2_1-12-19_48MYU_3.tif   \n4  /kaggle/input/marida-marine-debrish-dataset/patches/S2_11-1-19_19QDA/S2_11-1-19_19QDA_0.tif   \n\n                                                                                             mask  \\\n0  /kaggle/input/marida-marine-debrish-dataset/patches/S2_1-12-19_48MYU/S2_1-12-19_48MYU_0_cl.tif   \n1  /kaggle/input/marida-marine-debrish-dataset/patches/S2_1-12-19_48MYU/S2_1-12-19_48MYU_1_cl.tif   \n2  /kaggle/input/marida-marine-debrish-dataset/patches/S2_1-12-19_48MYU/S2_1-12-19_48MYU_2_cl.tif   \n3  /kaggle/input/marida-marine-debrish-dataset/patches/S2_1-12-19_48MYU/S2_1-12-19_48MYU_3_cl.tif   \n4  /kaggle/input/marida-marine-debrish-dataset/patches/S2_11-1-19_19QDA/S2_11-1-19_19QDA_0_cl.tif   \n\n                                                                                         confidence  \\\n0  /kaggle/input/marida-marine-debrish-dataset/patches/S2_1-12-19_48MYU/S2_1-12-19_48MYU_0_conf.tif   \n1  /kaggle/input/marida-marine-debrish-dataset/patches/S2_1-12-19_48MYU/S2_1-12-19_48MYU_1_conf.tif   \n2  /kaggle/input/marida-marine-debrish-dataset/patches/S2_1-12-19_48MYU/S2_1-12-19_48MYU_2_conf.tif   \n3  /kaggle/input/marida-marine-debrish-dataset/patches/S2_1-12-19_48MYU/S2_1-12-19_48MYU_3_conf.tif   \n4  /kaggle/input/marida-marine-debrish-dataset/patches/S2_11-1-19_19QDA/S2_11-1-19_19QDA_0_conf.tif   \n\n      date   tile  \n0  1-12-19  48MYU  \n1  1-12-19  48MYU  \n2  1-12-19  48MYU  \n3  1-12-19  48MYU  \n4  11-1-19  19QDA  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>mask</th>\n      <th>confidence</th>\n      <th>date</th>\n      <th>tile</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/marida-marine-debrish-dataset/patches/S2_1-12-19_48MYU/S2_1-12-19_48MYU_0.tif</td>\n      <td>/kaggle/input/marida-marine-debrish-dataset/patches/S2_1-12-19_48MYU/S2_1-12-19_48MYU_0_cl.tif</td>\n      <td>/kaggle/input/marida-marine-debrish-dataset/patches/S2_1-12-19_48MYU/S2_1-12-19_48MYU_0_conf.tif</td>\n      <td>1-12-19</td>\n      <td>48MYU</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/marida-marine-debrish-dataset/patches/S2_1-12-19_48MYU/S2_1-12-19_48MYU_1.tif</td>\n      <td>/kaggle/input/marida-marine-debrish-dataset/patches/S2_1-12-19_48MYU/S2_1-12-19_48MYU_1_cl.tif</td>\n      <td>/kaggle/input/marida-marine-debrish-dataset/patches/S2_1-12-19_48MYU/S2_1-12-19_48MYU_1_conf.tif</td>\n      <td>1-12-19</td>\n      <td>48MYU</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/marida-marine-debrish-dataset/patches/S2_1-12-19_48MYU/S2_1-12-19_48MYU_2.tif</td>\n      <td>/kaggle/input/marida-marine-debrish-dataset/patches/S2_1-12-19_48MYU/S2_1-12-19_48MYU_2_cl.tif</td>\n      <td>/kaggle/input/marida-marine-debrish-dataset/patches/S2_1-12-19_48MYU/S2_1-12-19_48MYU_2_conf.tif</td>\n      <td>1-12-19</td>\n      <td>48MYU</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/marida-marine-debrish-dataset/patches/S2_1-12-19_48MYU/S2_1-12-19_48MYU_3.tif</td>\n      <td>/kaggle/input/marida-marine-debrish-dataset/patches/S2_1-12-19_48MYU/S2_1-12-19_48MYU_3_cl.tif</td>\n      <td>/kaggle/input/marida-marine-debrish-dataset/patches/S2_1-12-19_48MYU/S2_1-12-19_48MYU_3_conf.tif</td>\n      <td>1-12-19</td>\n      <td>48MYU</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/marida-marine-debrish-dataset/patches/S2_11-1-19_19QDA/S2_11-1-19_19QDA_0.tif</td>\n      <td>/kaggle/input/marida-marine-debrish-dataset/patches/S2_11-1-19_19QDA/S2_11-1-19_19QDA_0_cl.tif</td>\n      <td>/kaggle/input/marida-marine-debrish-dataset/patches/S2_11-1-19_19QDA/S2_11-1-19_19QDA_0_conf.tif</td>\n      <td>11-1-19</td>\n      <td>19QDA</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"print(f'litter rows length {len(lr_df)}')\nprint(f'marida length {len(marida_df)}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:15:12.250065Z","iopub.execute_input":"2025-05-18T20:15:12.250325Z","iopub.status.idle":"2025-05-18T20:15:12.254819Z","shell.execute_reply.started":"2025-05-18T20:15:12.250305Z","shell.execute_reply":"2025-05-18T20:15:12.254109Z"}},"outputs":[{"name":"stdout","text":"litter rows length 1221\nmarida length 694\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"marida_val_df = create_marida_df(MARIDA_path, 'val')\nlr_val_df = create_LR_dataframe(LR_splits_path, 'val')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:15:12.255778Z","iopub.execute_input":"2025-05-18T20:15:12.256004Z","iopub.status.idle":"2025-05-18T20:15:15.483633Z","shell.execute_reply.started":"2025-05-18T20:15:12.255978Z","shell.execute_reply":"2025-05-18T20:15:15.483098Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"lr_val_df_invalid = compute_invalid_pixels(lr_val_df['image'].tolist(), lr_val_df['mask'].tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:15:15.484304Z","iopub.execute_input":"2025-05-18T20:15:15.484508Z","iopub.status.idle":"2025-05-18T20:15:35.338184Z","shell.execute_reply.started":"2025-05-18T20:15:15.484492Z","shell.execute_reply":"2025-05-18T20:15:35.337407Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"lr_test_df = create_LR_dataframe(LR_splits_path, 'test')\nlr_test_df_invalid = compute_invalid_pixels(lr_test_df['image'].tolist(), lr_test_df['mask'].tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:15:35.339016Z","iopub.execute_input":"2025-05-18T20:15:35.339285Z","iopub.status.idle":"2025-05-18T20:15:55.523496Z","shell.execute_reply.started":"2025-05-18T20:15:35.339263Z","shell.execute_reply":"2025-05-18T20:15:55.522926Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"#marida_df_invalid = compute_invalid_pixels(marida_df['image'].tolist(), marida_df['mask'].tolist())\n#marida_df_invalid.to_csv('/kaggle/working/marida_with_invalid.csv')\nmarida_df_invalid = pd.read_csv('/kaggle/working/marida_df_invalid_info.csv')\nmarida_df_invalid.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:15:55.524216Z","iopub.execute_input":"2025-05-18T20:15:55.524396Z","iopub.status.idle":"2025-05-18T20:15:55.543258Z","shell.execute_reply.started":"2025-05-18T20:15:55.524381Z","shell.execute_reply":"2025-05-18T20:15:55.542320Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0  \\\n0           0   \n1           1   \n2           2   \n3           3   \n4           4   \n\n                                                                                         image  \\\n0  /kaggle/input/marida-marine-debrish-dataset/patches/S2_1-12-19_48MYU/S2_1-12-19_48MYU_0.tif   \n1  /kaggle/input/marida-marine-debrish-dataset/patches/S2_1-12-19_48MYU/S2_1-12-19_48MYU_1.tif   \n2  /kaggle/input/marida-marine-debrish-dataset/patches/S2_1-12-19_48MYU/S2_1-12-19_48MYU_2.tif   \n3  /kaggle/input/marida-marine-debrish-dataset/patches/S2_1-12-19_48MYU/S2_1-12-19_48MYU_3.tif   \n4  /kaggle/input/marida-marine-debrish-dataset/patches/S2_11-1-19_19QDA/S2_11-1-19_19QDA_0.tif   \n\n   no data pixels  negative pixels  nan pixels  high value pixels  \\\n0               0                0           0                  0   \n1               0                0           0                  0   \n2               0                0           0                  0   \n3               0                0           0                  0   \n4               0                0           0                  0   \n\n   debris pixels  min values  max values  \n0            529    0.014372    0.271291  \n1             34    0.013171    0.097217  \n2             33    0.014072    0.102946  \n3            408    0.012871    0.177958  \n4             20    0.000247    0.102157  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>image</th>\n      <th>no data pixels</th>\n      <th>negative pixels</th>\n      <th>nan pixels</th>\n      <th>high value pixels</th>\n      <th>debris pixels</th>\n      <th>min values</th>\n      <th>max values</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>/kaggle/input/marida-marine-debrish-dataset/patches/S2_1-12-19_48MYU/S2_1-12-19_48MYU_0.tif</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>529</td>\n      <td>0.014372</td>\n      <td>0.271291</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>/kaggle/input/marida-marine-debrish-dataset/patches/S2_1-12-19_48MYU/S2_1-12-19_48MYU_1.tif</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>34</td>\n      <td>0.013171</td>\n      <td>0.097217</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>/kaggle/input/marida-marine-debrish-dataset/patches/S2_1-12-19_48MYU/S2_1-12-19_48MYU_2.tif</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>33</td>\n      <td>0.014072</td>\n      <td>0.102946</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>/kaggle/input/marida-marine-debrish-dataset/patches/S2_1-12-19_48MYU/S2_1-12-19_48MYU_3.tif</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>408</td>\n      <td>0.012871</td>\n      <td>0.177958</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>/kaggle/input/marida-marine-debrish-dataset/patches/S2_11-1-19_19QDA/S2_11-1-19_19QDA_0.tif</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>20</td>\n      <td>0.000247</td>\n      <td>0.102157</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"##lr_df_invalid = compute_invalid_pixels(lr_df['image'].tolist(), lr_df['mask'].tolist())\n#lr_df_invalid.to_csv('/kaggle/working/lr_with_invalid.csv')\nlr_df_invalid = pd.read_csv('/kaggle/working/litter_rows_df_invalid_info.csv')\nlr_df_invalid.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:15:55.544134Z","iopub.execute_input":"2025-05-18T20:15:55.544445Z","iopub.status.idle":"2025-05-18T20:15:55.562928Z","shell.execute_reply.started":"2025-05-18T20:15:55.544423Z","shell.execute_reply":"2025-05-18T20:15:55.562232Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0  \\\n0           0   \n1           1   \n2           2   \n3           3   \n4           4   \n\n                                                                                                                                            image  \\\n0  /kaggle/input/litter-windrows-patches/patches/S2A_MSIL1C_20161126T094332_R036_T33SXC/S2A_MSIL1C_20161126T094332_R036_T33SXC_646080_4256500.tif   \n1  /kaggle/input/litter-windrows-patches/patches/S2A_MSIL1C_20180615T100031_R122_T33TUL/S2A_MSIL1C_20180615T100031_R122_T33TUL_353760_5056480.tif   \n2  /kaggle/input/litter-windrows-patches/patches/S2A_MSIL1C_20160814T100032_R122_T33TUL/S2A_MSIL1C_20160814T100032_R122_T33TUL_374240_5056480.tif   \n3  /kaggle/input/litter-windrows-patches/patches/S2A_MSIL1C_20210331T100021_R122_T33TUL/S2A_MSIL1C_20210331T100021_R122_T33TUL_379360_5020640.tif   \n4  /kaggle/input/litter-windrows-patches/patches/S2A_MSIL1C_20150830T100016_R122_T33TUL/S2A_MSIL1C_20150830T100016_R122_T33TUL_358880_5066720.tif   \n\n   no data pixels  negative pixels  nan pixels  high value pixels  \\\n0               0                0           0                  0   \n1               0            22493           0                  0   \n2               0             3533           0                  0   \n3               0              382           0                  0   \n4               0            26933           0                  0   \n\n   debris pixels  min values  max values  \n0             30    0.000256    0.061144  \n1            102   -0.001337    0.087866  \n2             45   -0.001495    0.369590  \n3             27   -0.000738    0.042183  \n4            235   -0.001922    0.487958  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>image</th>\n      <th>no data pixels</th>\n      <th>negative pixels</th>\n      <th>nan pixels</th>\n      <th>high value pixels</th>\n      <th>debris pixels</th>\n      <th>min values</th>\n      <th>max values</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>/kaggle/input/litter-windrows-patches/patches/S2A_MSIL1C_20161126T094332_R036_T33SXC/S2A_MSIL1C_20161126T094332_R036_T33SXC_646080_4256500.tif</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30</td>\n      <td>0.000256</td>\n      <td>0.061144</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>/kaggle/input/litter-windrows-patches/patches/S2A_MSIL1C_20180615T100031_R122_T33TUL/S2A_MSIL1C_20180615T100031_R122_T33TUL_353760_5056480.tif</td>\n      <td>0</td>\n      <td>22493</td>\n      <td>0</td>\n      <td>0</td>\n      <td>102</td>\n      <td>-0.001337</td>\n      <td>0.087866</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>/kaggle/input/litter-windrows-patches/patches/S2A_MSIL1C_20160814T100032_R122_T33TUL/S2A_MSIL1C_20160814T100032_R122_T33TUL_374240_5056480.tif</td>\n      <td>0</td>\n      <td>3533</td>\n      <td>0</td>\n      <td>0</td>\n      <td>45</td>\n      <td>-0.001495</td>\n      <td>0.369590</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>/kaggle/input/litter-windrows-patches/patches/S2A_MSIL1C_20210331T100021_R122_T33TUL/S2A_MSIL1C_20210331T100021_R122_T33TUL_379360_5020640.tif</td>\n      <td>0</td>\n      <td>382</td>\n      <td>0</td>\n      <td>0</td>\n      <td>27</td>\n      <td>-0.000738</td>\n      <td>0.042183</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>/kaggle/input/litter-windrows-patches/patches/S2A_MSIL1C_20150830T100016_R122_T33TUL/S2A_MSIL1C_20150830T100016_R122_T33TUL_358880_5066720.tif</td>\n      <td>0</td>\n      <td>26933</td>\n      <td>0</td>\n      <td>0</td>\n      <td>235</td>\n      <td>-0.001922</td>\n      <td>0.487958</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"lr_blacklist = lr_df_invalid[lr_df_invalid['high value pixels']>0]\n#lr_blacklist.head()\nlr_df_filt = lr_df.drop(lr_blacklist.index)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:15:55.563746Z","iopub.execute_input":"2025-05-18T20:15:55.564019Z","iopub.status.idle":"2025-05-18T20:15:55.573118Z","shell.execute_reply.started":"2025-05-18T20:15:55.563997Z","shell.execute_reply":"2025-05-18T20:15:55.572445Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# lr valid = 79495168","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:15:55.573928Z","iopub.execute_input":"2025-05-18T20:15:55.574215Z","iopub.status.idle":"2025-05-18T20:15:55.588632Z","shell.execute_reply.started":"2025-05-18T20:15:55.574197Z","shell.execute_reply":"2025-05-18T20:15:55.587907Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"#lr_stats = compute_stats(lr_df_filt['image'].tolist())\n#np.savez(\"/kaggle/working/lr_stats.npz\", first=lr_stats['mean'], second=lr_stats['std'])\n#marida_stats = compute_stats(marida_df['image'].tolist())\n#np.savez(\"/kaggle/working/my_marida_stats.npz\", first=marida_stats['mean'], second=marida_stats['std'])\n#global_stats = compute_stats(marida_df['image'].tolist() + lr_df_filt['image'].to_list())\n#np.savez(\"/kaggle/working/global_stats.npz\", first=global_stats['mean'], second=global_stats['std'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:15:55.589335Z","iopub.execute_input":"2025-05-18T20:15:55.589571Z","iopub.status.idle":"2025-05-18T20:15:55.604780Z","shell.execute_reply.started":"2025-05-18T20:15:55.589545Z","shell.execute_reply":"2025-05-18T20:15:55.604085Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"global_stats = np.load('/kaggle/working/global_stats.npz')\nglobal_bands_mean = global_stats['first']\nglobal_bands_std = global_stats['second']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:15:55.605583Z","iopub.execute_input":"2025-05-18T20:15:55.605830Z","iopub.status.idle":"2025-05-18T20:15:55.621382Z","shell.execute_reply.started":"2025-05-18T20:15:55.605809Z","shell.execute_reply":"2025-05-18T20:15:55.620852Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# global_bands_mean =np.array([0.03721786, 0.03547978, 0.03033651, 0.01722546, 0.01574046,\n#         0.01738895, 0.01939084, 0.01724032, 0.01895351, 0.0109694 ,\n#         0.00784716])\n# global_bands_std = np.array([0.03185222, 0.03198375, 0.03251331, 0.03379553, 0.03407218,\n#         0.04551132, 0.05334419, 0.05064404, 0.0578197 , 0.03721222,\n#         0.02560836])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:15:55.622047Z","iopub.execute_input":"2025-05-18T20:15:55.622302Z","iopub.status.idle":"2025-05-18T20:15:55.634728Z","shell.execute_reply.started":"2025-05-18T20:15:55.622280Z","shell.execute_reply":"2025-05-18T20:15:55.634107Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"#computing_labeled_pixels_stats(lr_df_filt['mask'].tolist())\n#computing_labeled_pixels_stats(marida_df['mask'].tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:15:55.635387Z","iopub.execute_input":"2025-05-18T20:15:55.635645Z","iopub.status.idle":"2025-05-18T20:15:55.653035Z","shell.execute_reply.started":"2025-05-18T20:15:55.635628Z","shell.execute_reply":"2025-05-18T20:15:55.652247Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"marida_classes_distr = np.array([0.00452, 0.00203, 0.00254, 0.00168, 0.00766, 0.15206, 0.20232,\n 0.35941, 0.00109, 0.20218, 0.03226, 0.00693, 0.01322, 0.01158, 0.00052])\nlr_debris_pixels = 92090\nmarida_pixels = 429412\nmarida_debris_pixels = np.sum(marida_classes_distr[[0,1,2,3,8]]) * marida_pixels\nprint(f'marida debris pixels {marida_debris_pixels}')\ntot_glob_pixels = (len(lr_df_filt) + len(marida_df))*256**2\nmarida_debris_fraction = np.sum(marida_classes_distr[[0,1,2,3,8]])\n#debris_fraction = (lr_debris_pixels + marida_debris_pixels)/tot_glob_pixels\nprint(f'marida_debris_fraction : {marida_debris_fraction}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:15:55.653975Z","iopub.execute_input":"2025-05-18T20:15:55.654254Z","iopub.status.idle":"2025-05-18T20:15:55.670083Z","shell.execute_reply.started":"2025-05-18T20:15:55.654232Z","shell.execute_reply":"2025-05-18T20:15:55.669450Z"}},"outputs":[{"name":"stdout","text":"marida debris pixels 5092.826320000001\nmarida_debris_fraction : 0.011860000000000002\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"# Computing here the percentage of debris pixels across the two datasets\n# This will be used as class distribution to generate weights for the loss function\nLR_ratio = 40 # for 1 debrix pixel, choose 90 bk pixels\n\n# For MARIDA the loss function uses only pixels in the 15 classes \n# The fraction of classes assimilated to marine debris is \nmarida_debrix_pixels_distr = np.sum(marida_classes_distr[[0,1,2,3,8]])\n# For LR the DataSet will sample backgroung pixels with a given ratio, stored in the variable LR_ratio\n# Then the effective ratio \neffective_ratio = (1/40 * len(lr_df_filt) + 0.011860000000000002 * len(marida_df))/(len(lr_df_filt) + len(marida_df))\n#print(f'effective global ratio {effective_ratio}')\nclass_distribution = np.array([1 - effective_ratio, effective_ratio])\nprint(f'class distribution {class_distribution}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:15:55.670868Z","iopub.execute_input":"2025-05-18T20:15:55.671138Z","iopub.status.idle":"2025-05-18T20:15:55.684443Z","shell.execute_reply.started":"2025-05-18T20:15:55.671117Z","shell.execute_reply":"2025-05-18T20:15:55.683681Z"}},"outputs":[{"name":"stdout","text":"class distribution [0.97978194 0.02021806]\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"# MARIDA statistics\n\nclass_distr = np.array([0.00452, 0.00203, 0.00254, 0.00168, 0.00766, 0.15206, 0.20232,\n 0.35941, 0.00109, 0.20218, 0.03226, 0.00693, 0.01322, 0.01158, 0.00052])\n\nbands_mean = np.array([0.05197577, 0.04783991, 0.04056812, 0.03163572, 0.02972606, 0.03457443,\n 0.03875053, 0.03436435, 0.0392113,  0.02358126, 0.01588816]).astype(np.float32)\n\nbands_std = np.array([0.04725893, 0.04743808, 0.04699043, 0.04967381, 0.04946782, 0.06458357,\n 0.07594915, 0.07120246, 0.08251058, 0.05111466, 0.03524419]).astype(np.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:15:55.685262Z","iopub.execute_input":"2025-05-18T20:15:55.685474Z","iopub.status.idle":"2025-05-18T20:15:55.698791Z","shell.execute_reply.started":"2025-05-18T20:15:55.685449Z","shell.execute_reply":"2025-05-18T20:15:55.698092Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"# Other code references  \n# https://github.com/MarcCoru/marinedebrisdetector","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:15:55.699456Z","iopub.execute_input":"2025-05-18T20:15:55.699680Z","iopub.status.idle":"2025-05-18T20:15:55.718349Z","shell.execute_reply.started":"2025-05-18T20:15:55.699654Z","shell.execute_reply":"2025-05-18T20:15:55.717690Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"# MARIDA CLASSES\n# {\n#  1: \"Marine Debris\",\n#  2: \"Dense Sargassum\", \n#  3: \"Sparse Sargassum\", \n#  4: \"Natural Organic Material\", \n#  5: \"Ship\", \n#  6: \"Clouds\", \n#  7: \"Marine Water\", \n#  8: \"Sediment-Laden Water\", \n#  9: \"Foam\", \n#  10: \"Turbid Water\", \n#  11: \"Shallow Water\", \n#  12: \"Waves\", \n#  13: \"Cloud Shadows\", \n#  14: \"Wakes\", \n#  15: \"Mixed Water\"\n# }\n\n\n# From marinedebrisdetector \n# DEBRIS_CLASSES = [1,2,3,4,9]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:15:55.719022Z","iopub.execute_input":"2025-05-18T20:15:55.719302Z","iopub.status.idle":"2025-05-18T20:15:55.731750Z","shell.execute_reply.started":"2025-05-18T20:15:55.719285Z","shell.execute_reply":"2025-05-18T20:15:55.731105Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"# https://drive.google.com/drive/folders/1rntiw5BvOs80eIbpOu7dk9g1BfOVw61-?usp=drive_link","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:15:55.734948Z","iopub.execute_input":"2025-05-18T20:15:55.735320Z","iopub.status.idle":"2025-05-18T20:15:55.745618Z","shell.execute_reply.started":"2025-05-18T20:15:55.735303Z","shell.execute_reply":"2025-05-18T20:15:55.744992Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"\nclass RandomRotationTransform:\n    \"\"\"Rotate by one of the given angles.\"\"\"\n\n    def __init__(self, angles):\n        self.angles = angles\n\n    def __call__(self, x):\n        angle = random.choice(self.angles)\n        return vF.rotate(x, angle)\n    \ndef gen_weights(class_distribution, c = 1.02):\n    return 1/torch.log(c + class_distribution)\n    \ntransformTrain = transforms.Compose([transforms.ToTensor(),\n                                    RandomRotationTransform([-90, 0, 90, 180]),\n                                    transforms.RandomHorizontalFlip()])\n    \ntransformTest = transforms.Compose([transforms.ToTensor()])\n    \nstandardization = transforms.Normalize(global_bands_mean, global_bands_std) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:15:55.746646Z","iopub.execute_input":"2025-05-18T20:15:55.746813Z","iopub.status.idle":"2025-05-18T20:15:55.760689Z","shell.execute_reply.started":"2025-05-18T20:15:55.746800Z","shell.execute_reply":"2025-05-18T20:15:55.760091Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"def gen_weights(class_distribution, c = 1.02):\n    return 1/torch.log(c + class_distribution)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:15:55.761347Z","iopub.execute_input":"2025-05-18T20:15:55.761514Z","iopub.status.idle":"2025-05-18T20:15:55.774614Z","shell.execute_reply.started":"2025-05-18T20:15:55.761501Z","shell.execute_reply":"2025-05-18T20:15:55.773977Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"import os\nimport argparse\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom torchvision import transforms\n# model import UNet, AttentionUNet, ResidualAttentionUNet  # From original script\n#f#rom dataloader import bands_mean, bands_std, RandomRotationTransform, class_distr, gen_weights\n#from metrics import Evaluation\n#from customLosses import FocalLoss\nimport pandas as pd\nfrom torch.utils.data import Dataset\n\nclass MergedSegmentationDataset(Dataset):\n    \"\"\"\n    df_dataset1 : MARIDA dataset\n    df_dataset2 : LR dataset\n    \"\"\"\n    def __init__(self, df_dataset1, df_dataset2, bands_mean, bands_std, transform=None, standardization=None):\n        \"\"\"\n        df_dataset1 : MARIDA\n        df_dataset2 : Litter Windrows\n        \"\"\"\n        self.bands_mean = bands_mean\n        self.bands_std = bands_std\n        self.transform = transform\n        self.standardization = standardization\n        self.image_paths = []\n        self.mask_paths = []\n        self.dataset_ids = []\n        self.image_paths = df_dataset1['image'].tolist() + df_dataset2['image'].tolist() \n        self.mask_paths =  df_dataset1['mask'].tolist() + df_dataset2['mask'].tolist() \n        self.dataset_ids = [0] * len(df_dataset1['image']) + [1] * len(df_dataset2['image'])\n        # Generate shuffled indices\n        indices = np.random.permutation(len(self.image_paths))\n        self.image_paths = np.array(self.image_paths)[indices]\n        self.mask_paths = np.array(self.mask_paths)[indices]\n        self.dataset_ids = np.array(self.dataset_ids)[indices]        \n        #print(self.dataset_ids)\n        if self.transform is None:\n            self.transform = transforms.Compose([transforms.ToTensor()])\n        ## preloading images in memory \n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        #print(f'idx {idx}')\n        # Load Classsification Mask\n        dataset_id = self.dataset_ids[idx]\n        # Open t#he GeoTIFF image file\n        #print(f'image path {self.image_paths[idx]}')\n        #print(f'mask path {self.mask_paths[idx]}')\n        with rasterio.open(self.image_paths[idx]) as src:\n            #print(f#\"Number of bands: {dataset.count}\")  # Check the number of bands\n            # Read all bands as a NumPy array\n            image = src.read()\n            #print(f'image shape {image.shape}')\n            invalid_mask = get_invalid_mask(image, src.nodata)\n            #print(bands.shape)  # Shape will be (bands, height, width)\n            #print(f'invalid mask shape {invalid_mask.shape}')\n            with rasterio.open(self.mask_paths[idx]) as src_mask:\n                mask = src_mask.read().astype(int)\n            # if dataset_id == 0: #MARIDA\n            #     #print(f'sample from marida')\n            #     temp = mask.copy()\n            #     #assimilate several classes to marine debris\n            #     temp[temp==1]=2\n            #     temp[temp==2]=2          \n            #     temp[temp==3]=2          \n            #     temp[temp==4]=2          \n            #     temp[temp==9]=2          \n            #     # Leaving unlabeled pixels to 0 and pixels in classes not in [1,2,3, 4,9] to 1\n            #     temp[(temp != 0) & (temp != 2)] = 1\n                \n            #     # Categories from 1 to 0\n            #     mask = np.copy(temp)\n            # else : #LR\n            #     #print('sample from litter rows')\n            #     bg_mask = select_bg_pixels(image, mask[0], target_ratio=40)\n            #     #print(f'bg mask shape {bg_mask.shape}')\n            #     mask[mask==1] = 2\n            #     mask[bg_mask[None,...].astype(bool)] = 1\n            #print(f'mask before inputing {mask.shape}')\n            debris_before_invalid = np.sum(mask)\n            invalid_pixels = np.sum(np.any(invalid_mask, axis=0))\n            mask[np.any(invalid_mask.astype(bool), axis=0, keepdims=True)] = 0 #I guess it makes sense not to feed invalid pixels to the loss function\n            #print(f'before inputing 2')\n            image[invalid_mask.astype(bool)] = np.tile(self.bands_mean[:, np.newaxis, np.newaxis], (1, 256, 256))[invalid_mask.astype(bool)]\n            #print(f'after inputing')\n            ## Since the model sees unvalid pixels anyway, it's better (?) to replace those with mean values ? \n            #print(f'mask type before transh {type(mask)} - {mask.dtype}')\n            #print(f'image type before transh {type(image)} - {image.dtype}')\n            #############\n            debris_after_invalid = np.sum(mask)\n            #############\n            if self.transform is not None:\n                # applying the same rotation on the image-mask pair\n                #print(f'transform - image shape {image.shape}')\n                #print(f'transform - mask shape {mask.shape}')\n                stack = np.concatenate([image, mask], axis=0).astype(np.float32) \n                stack = np.transpose(stack,(1, 2, 0)) #to channel last\n                #print(f'stack shape before transfrom {stack.shape}')\n                stack = self.transform(stack) #expects channel last, returns channel first\n               \n                #print(f'stack shape after transfrom {stack.shape}')\n                image = stack[:-1,:,:]\n                mask = stack[-1,:,:].long()\n                #print(f'image type {image.dtype}')\n                #print(f'image shape after transform {image.shape}')\n                #print(f'mask shape after transform {mask.shape}')\n\n                   \n            \n            if self.standardization is not None:\n                image = self.standardization(image)\n                \n            #mask = mask - 1 Moved to collate function\n            if isinstance(mask, np.ndarray):\n                mask = torch.from_numpy(mask).to(torch.long)\n            else:\n                mask = mask.to(torch.long)\n            if isinstance(image, np.ndarray):\n                image = torch.from_numpy(image).to(torch.float32)\n            else:\n                im = image.to(torch.float32)\n            if torch.sum(mask) == 0 :\n                print(f'{self.mask_paths[idx]} has no debris pixels')\n                print(f'debris pixels before invalid mask : {debris_before_invalid}')\n                print(f'debris pixels after invalid mask : {debris_after_invalid}')\n                print(f'invalid pixels : {invalid_pixels}')\n           \n        ## Add logic for transform\n\n            return image, mask, dataset_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:15:55.775362Z","iopub.execute_input":"2025-05-18T20:15:55.775590Z","iopub.status.idle":"2025-05-18T20:15:59.952303Z","shell.execute_reply.started":"2025-05-18T20:15:55.775565Z","shell.execute_reply":"2025-05-18T20:15:59.951609Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"def conv3x3(in_channels, out_channels, stride=1):\n    \"3x3 convolution with padding\"\n    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\nclass ChannelAttention(nn.Module):\n    def __init__(self, channels, ratio=16):\n        super().__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n\n        self.mlp = nn.Sequential(nn.Conv2d(channels, channels // 16, 1, bias=False),\n                               nn.ReLU(),\n                               nn.Conv2d(channels // 16, channels, 1, bias=False))\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = self.mlp(self.avg_pool(x))\n        max_out = self.mlp(self.max_pool(x))\n        out = avg_out + max_out\n        return self.sigmoid(out)\n\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super().__init__()\n\n        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x = torch.cat([avg_out, max_out], dim=1)\n        x = self.conv1(x)\n        return self.sigmoid(x)\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, inputChannel, outputChannel, stride=1, downsample=None):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = conv3x3(inputChannel, outputChannel, stride)\n        self.bn1 = nn.BatchNorm2d(outputChannel)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(outputChannel, outputChannel)\n        self.bn2 = nn.BatchNorm2d(outputChannel)\n        self.downsample = downsample\n        self.ca = ChannelAttention(outputChannel)\n        self.sa = SpatialAttention()\n        \n    # def forward(self, x):\n    #     residual = x\n    #     out = self.conv1(x)\n    #     out = self.bn1(out)\n    #     out = self.relu(out)\n    #     out = self.conv2(out)\n    #     out = self.bn2(out)\n    #     if self.downsample:\n    #         residual = self.downsample(x)\n    #     out += residual\n    #     out = self.relu(out)\n    #     caOutput = self.ca(out)\n    #     out = caOutput * out\n    #     saOutput = self.sa(out)\n    #     out = saOutput * out\n    #     return out, saOutput\n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        if self.downsample:\n            residual = self.downsample(x)\n        out += residual\n        out = self.bn2(out)\n        out = self.relu(out)\n        caOutput = self.ca(out)\n        out = caOutput * out\n        saOutput = self.sa(out)\n        out = saOutput * out\n        return out, saOutput\n\n\nclass DownSampleWithAttention(nn.Module):\n    def __init__(self, inputChannel, outputChannel):\n        super().__init__()\n        self.convolution = nn.Sequential(\n            nn.Conv2d(inputChannel, outputChannel, kernel_size=3, padding=1),\n            nn.BatchNorm2d(outputChannel),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(outputChannel, outputChannel, kernel_size=3, padding=1),\n            nn.BatchNorm2d(outputChannel),\n            nn.LeakyReLU(0.2),\n            nn.AvgPool2d(2)\n        )\n        self.ca = ChannelAttention(outputChannel)\n        self.sa = SpatialAttention()\n    \n    def forward(self,x):\n        x = self.convolution(x)\n        caOutput = self.ca(x)\n        x = caOutput * x\n        saOutput = self.sa(x)\n        x = saOutput * x\n        return x, saOutput\n\n    \nclass UpSampleWithAttention(nn.Module):\n    def __init__(self, inputChannel, outputChannel):\n        super().__init__()\n        self.convolution = nn.Sequential(\n            nn.Conv2d(inputChannel, outputChannel, kernel_size=3, padding=1),\n            nn.BatchNorm2d(outputChannel),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(outputChannel, outputChannel, kernel_size=3, padding=1),\n            nn.BatchNorm2d(outputChannel),\n            nn.LeakyReLU(0.2)\n        )\n        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        self.ca = ChannelAttention(outputChannel)\n        self.sa = SpatialAttention()\n    \n    def forward(self, x):\n        x = self.upsample(x)\n        x = self.convolution(x)\n        caOutput = self.ca(x)\n        x = caOutput * x\n        saOutput = self.sa(x)\n        x = saOutput * x\n        return x, saOutput\n\nclass ResidualAttentionUNet(nn.Module):\n  def __init__(self, inputChannel, outputChannel):\n    super().__init__()\n    self.downsample1 = DownSampleWithAttention(inputChannel, 32)\n    self.downsample2 = DownSampleWithAttention(32, 64)\n    self.downsample3 = DownSampleWithAttention(64, 128)\n    self.downsample4 = DownSampleWithAttention(128, 256)\n    self.downsample5 = DownSampleWithAttention(256, 512)\n\n    self.residualBlock1 = ResidualBlock(512, 512)\n    self.residualBlock2 = ResidualBlock(512, 512)\n    self.residualBlock3 = ResidualBlock(512, 512)\n\n    self.upsample1 = UpSampleWithAttention(512, 256)\n    self.upsample2 = UpSampleWithAttention(512, 128)\n    self.upsample3 = UpSampleWithAttention(256, 64)\n    self.upsample4 = UpSampleWithAttention(128, 32)\n    self.upsample5 = UpSampleWithAttention(64, 32)\n    self.classification = nn.Sequential(\n            nn.Conv2d(32, outputChannel, kernel_size=1),\n        )\n\n  def forward(self, x):\n    scale128, sa128down = self.downsample1(x)\n    scale64, sa64down = self.downsample2(scale128)\n    scale32, sa32down = self.downsample3(scale64)\n    scale16, sa64down = self.downsample4(scale32)\n    scale8, sa8down = self.downsample5(scale16)\n    scale8, sa8down = self.residualBlock1(scale8)\n    scale8, sa8down = self.residualBlock2(scale8)\n    scale8, sa8down = self.residualBlock3(scale8)\n    upscale16, sa16up = self.upsample1(scale8)\n    upscale16 = torch.cat([upscale16, scale16], dim=1)\n    upscale32, sa32up = self.upsample2(upscale16)\n    upscale32 = torch.cat([upscale32, scale32], dim=1)\n    upscale64, sa64up = self.upsample3(upscale32)\n    upscale64 = torch.cat([upscale64, scale64], dim=1)\n    upscale128, sa128up = self.upsample4(upscale64)\n    upscale128 = torch.cat([upscale128, scale128], dim=1)\n    upscale256, sa256up = self.upsample5(upscale128)\n    finaloutput = self.classification(upscale256)\n    return finaloutput","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:15:59.953142Z","iopub.execute_input":"2025-05-18T20:15:59.953653Z","iopub.status.idle":"2025-05-18T20:15:59.972510Z","shell.execute_reply.started":"2025-05-18T20:15:59.953622Z","shell.execute_reply":"2025-05-18T20:15:59.971772Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"def Evaluation(y_predicted, y_true):\n\n    micro_prec = precision_score(y_true, y_predicted, average='micro')\n    macro_prec = precision_score(y_true, y_predicted, average='macro')\n    weight_prec = precision_score(y_true, y_predicted, average='weighted')\n    \n    micro_rec = recall_score(y_true, y_predicted, average='micro')\n    macro_rec = recall_score(y_true, y_predicted, average='macro')\n    weight_rec = recall_score(y_true, y_predicted, average='weighted')\n        \n    macro_f1 = f1_score(y_true, y_predicted, average=\"macro\")\n    micro_f1 = f1_score(y_true, y_predicted, average=\"micro\")\n    weight_f1 = f1_score(y_true, y_predicted, average=\"weighted\")\n        \n    subset_acc = accuracy_score(y_true, y_predicted)\n    \n    iou_acc = jaccard_score(y_true, y_predicted, average='macro')\n\n    info = {\n            \"macroPrec\" : macro_prec,\n            \"microPrec\" : micro_prec,\n            \"weightPrec\" : weight_prec,\n            \"macroRec\" : macro_rec,\n            \"microRec\" : micro_rec,\n            \"weightRec\" : weight_rec,\n            \"macroF1\" : macro_f1,\n            \"microF1\" : micro_f1,\n            \"weightF1\" : weight_f1,\n            \"subsetAcc\" : subset_acc,\n            \"IoU\": iou_acc\n            }\n    \n    return info","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:15:59.973367Z","iopub.execute_input":"2025-05-18T20:15:59.973699Z","iopub.status.idle":"2025-05-18T20:16:00.008956Z","shell.execute_reply.started":"2025-05-18T20:15:59.973672Z","shell.execute_reply":"2025-05-18T20:16:00.008138Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"transformTrain = transforms.Compose([transforms.ToTensor(),\n                                    RandomRotationTransform([-90, 0, 90, 180]),\n                                    transforms.RandomHorizontalFlip()])\n    \ntransformTest = transforms.Compose([transforms.ToTensor()])\n    \nstandardization = transforms.Normalize(global_bands_mean.tolist(), global_bands_std.tolist())\nmerged_ds = MergedSegmentationDataset(marida_df, lr_df_filt, global_bands_mean, global_bands_std, transform=transformTrain, standardization= standardization)\nval_ds = MergedSegmentationDataset(marida_val_df, lr_val_df, global_bands_mean, global_bands_std, transform=transformTest, standardization= standardization )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:16:00.009720Z","iopub.execute_input":"2025-05-18T20:16:00.010289Z","iopub.status.idle":"2025-05-18T20:16:00.028031Z","shell.execute_reply.started":"2025-05-18T20:16:00.010263Z","shell.execute_reply":"2025-05-18T20:16:00.027503Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"trainLoader = DataLoader(merged_ds,\n                        batch_size=batch_size, \n                        shuffle=True,  #num_workers=4, \n                        #pin_memory=True,\n                        #prefetch_factor=2,\n                        collate_fn=custom_collate_fn,\n                        #worker_init_fn=worker_init_fn,\n                        #generator=torch.Generator().manual_seed(seed) \n                        )\n\n\ntestLoader = DataLoader(val_ds, \n                        batch_size=batch_size, \n                        shuffle=False,\n                        collate_fn=custom_collate_fn,\n                        #worker_init_fn=worker_init_fn,\n                        #generator=torch.Generator().manual_seed(seed) \n                        )\n                        \n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:16:00.028856Z","iopub.execute_input":"2025-05-18T20:16:00.029140Z","iopub.status.idle":"2025-05-18T20:16:00.045515Z","shell.execute_reply.started":"2025-05-18T20:16:00.029117Z","shell.execute_reply":"2025-05-18T20:16:00.044874Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"model = ResidualAttentionUNet(11, 2).to(device)\nweight = gen_weights(torch.from_numpy(class_distribution), c = 1.03).to(device)\ncriterion = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='mean', weight=weight.to(torch.float32))\noptimizer = torch.optim.Adam(model.parameters(), lr=8e-4, weight_decay=1e-2)\n\n# assuming about 40 reductions => .9 ** 40 = 1e-2, starting from 8e-4 ending with 8e-6\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.9, patience=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:16:00.046331Z","iopub.execute_input":"2025-05-18T20:16:00.046569Z","iopub.status.idle":"2025-05-18T20:16:00.530712Z","shell.execute_reply.started":"2025-05-18T20:16:00.046545Z","shell.execute_reply":"2025-05-18T20:16:00.530178Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"output_classes = 2\nmetrics_history = []\nepochs = 30\nfor epoch in range(1, epochs+1):\n    model.train()\n    pb = tqdm(trainLoader, desc=f\"epoch {epoch}/{epochs}: \")\n    yTrue = []\n    yPredicted = []\n\n    bg_yTrue = []\n    bg_yPredicted = []\n    for image, target, _ in pb:\n        image, target = image.to(device), target.to(device)\n        optimizer.zero_grad()\n\n        logits = model(image)\n        # print(f'logits shape : {logits.shape}')\n        # print(f'target shape : {target.shape}')\n        # print(f'image dtype {image.dtype}')\n        # print(f'logits dtype {logits.dtype}')\n        # print(f'target dtype {target.dtype}')\n        loss = criterion(logits, target)\n\n        loss.backward()\n        optimizer.step()\n        pb.set_postfix(loss=loss.item())\n\n        if epoch % 10 == 0:\n            with torch.no_grad():\n                logits = logits.detach()\n                logits = torch.movedim(logits, (0,1,2,3), (0,3,1,2))\n                logits = logits.reshape((-1,output_classes))\n                target = target.reshape(-1)\n                ###################################################################################\n                mask = target != -1\n                ###################################################################################\n                \n                # bg_logits = logits[~mask]\n                # bg_target = target[~mask]\n    \n                # only considering annotated pixels\n                logits = logits[mask]\n                target = target[mask]\n    \n                probs = F.softmax(logits, dim=1).cpu().numpy()\n                target = target.cpu().numpy()\n                yPredicted += probs.argmax(1).tolist()\n                yTrue += target.tolist()\n        \n                \n                # bg_probs = torch.nn.functional.softmax(bg_logits, dim=1).cpu().numpy()\n                # bg_target = bg_target.cpu().numpy()\n                \n                # bg_yPredicted += bg_probs.argmax(1).tolist()\n                # bg_yTrue += bg_target.tolist()\n\n\n    if epoch % 10 == 0:\n        yPredicted = np.asarray(yPredicted)\n        yTrue = np.asarray(yTrue)\n        acc = Evaluation(yPredicted, yTrue)\n        print(acc)\n    \n        # bg_yPredicted = np.asarray(bg_yPredicted)\n        # bg_yTrue = np.asarray(bg_yTrue)\n        # bg_acc = Evaluation(bg_yPredicted, bg_yTrue)\n        # print(\"background:\", bg_acc)\n\n\n    model.eval()\n    yTrue = []\n    yPredicted = []\n    testLossF = []\n    \n    # bg_yTrue = []\n    # bg_yPredicted = []\n    with torch.no_grad():\n        for image, target, _ in testLoader:\n\n            image, target = image.to(device), target.to(device)\n            logits = model(image)\n            # print(f'image dtype {image.dtype}')\n            # print(f'logits dtype {logits.dtype}')\n            # print(f'target dtype {target.dtype}')\n            # print(f'test - target shape {target.shape}')\n            # print(f'test - logit shape {logits.shape}')\n            loss = criterion(logits, target)\n\n            logits = torch.movedim(logits, (0,1,2,3), (0,3,1,2))\n            logits = logits.reshape((-1,output_classes))\n            target = target.reshape(-1)\n            ###################################################################################\n            mask = target != -1\n            ###################################################################################\n            \n            # bg_logits = logits[~mask]\n            # bg_target = target[~mask]\n            \n            logits = logits[mask]\n            target = target[mask]\n            \n\n            probs = F.softmax(logits, dim=1).cpu().numpy()\n            target = target.cpu().numpy()\n            # testBatches += target.shape[0]\n            testLossF.append((loss.data*target.shape[0]).tolist())\n            yPredicted += probs.argmax(1).tolist()\n            yTrue += target.tolist()\n\n\n            # bg_probs = torch.nn.functional.softmax(bg_logits, dim=1).cpu().numpy()\n            # bg_target = bg_target.cpu().numpy()\n\n            # bg_yPredicted += bg_probs.argmax(1).tolist()\n            # bg_yTrue += bg_target.tolist()\n        \n        yPredicted = np.asarray(yPredicted)\n        yTrue = np.asarray(yTrue)\n        print('########### Validation Set Evaluation : #############')\n        acc = Evaluation(yPredicted, yTrue)\n        metrics_history.append(acc)\n\n        # bg_yPredicted = np.asarray(bg_yPredicted)\n        # bg_yTrue = np.asarray(bg_yTrue)\n        # bg_acc = Evaluation(bg_yPredicted, bg_yTrue)\n        print(acc)\n        # print(\"background:\", bg_acc)\n    scheduler.step(sum(testLossF) / len(testLoader.dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T20:16:03.911565Z","iopub.execute_input":"2025-05-18T20:16:03.912181Z","iopub.status.idle":"2025-05-18T21:57:53.433909Z","shell.execute_reply.started":"2025-05-18T20:16:03.912156Z","shell.execute_reply":"2025-05-18T21:57:53.433261Z"}},"outputs":[{"name":"stderr","text":"epoch 1/30:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 104/120 [03:39<00:33,  2.11s/it, loss=0.318]/tmp/ipykernel_31/2493405400.py:5: RuntimeWarning: invalid value encountered in greater\n  invalid_mask |= image > 1\nepoch 1/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [04:11<00:00,  2.10s/it, loss=0.486]\n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\n{'macroPrec': 0.5880462650937845, 'microPrec': 0.908698085273679, 'weightPrec': 0.9794917433645293, 'macroRec': 0.8992861264924438, 'microRec': 0.908698085273679, 'weightRec': 0.908698085273679, 'macroF1': 0.6244537412354473, 'microF1': 0.908698085273679, 'weightF1': 0.9369564383805971, 'subsetAcc': 0.908698085273679, 'IoU': 0.5408997398567341}\n","output_type":"stream"},{"name":"stderr","text":"epoch 2/30:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 95/120 [02:04<00:32,  1.30s/it, loss=0.163] /tmp/ipykernel_31/2493405400.py:5: RuntimeWarning: invalid value encountered in greater\n  invalid_mask |= image > 1\nepoch 2/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:36<00:00,  1.31s/it, loss=0.245] \n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\n{'macroPrec': 0.8519043421647148, 'microPrec': 0.9872139067358982, 'weightPrec': 0.9870861589209086, 'macroRec': 0.844754637131489, 'microRec': 0.9872139067358982, 'weightRec': 0.9872139067358982, 'macroF1': 0.8482907456724691, 'microF1': 0.9872139067358982, 'weightF1': 0.9871482960307061, 'subsetAcc': 0.9872139067358982, 'IoU': 0.7645872008155867}\n","output_type":"stream"},{"name":"stderr","text":"epoch 3/30:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 64/120 [01:24<01:14,  1.33s/it, loss=0.157] /tmp/ipykernel_31/2493405400.py:5: RuntimeWarning: invalid value encountered in greater\n  invalid_mask |= image > 1\nepoch 3/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:37<00:00,  1.32s/it, loss=0.171] \n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\n{'macroPrec': 0.6612411592807774, 'microPrec': 0.960189560030885, 'weightPrec': 0.9805717927950888, 'macroRec': 0.8753448571033854, 'microRec': 0.960189560030885, 'weightRec': 0.960189560030885, 'macroF1': 0.7208372643674215, 'microF1': 0.960189560030885, 'weightF1': 0.9680800656642571, 'subsetAcc': 0.960189560030885, 'IoU': 0.6300890759257904}\n","output_type":"stream"},{"name":"stderr","text":"epoch 4/30:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 99/120 [02:10<00:27,  1.31s/it, loss=0.0784]/tmp/ipykernel_31/2493405400.py:5: RuntimeWarning: invalid value encountered in greater\n  invalid_mask |= image > 1\nepoch 4/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:37<00:00,  1.31s/it, loss=0.0868]\n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\n{'macroPrec': 0.8301358609111384, 'microPrec': 0.9868115521112992, 'weightPrec': 0.9882112512545314, 'macroRec': 0.8926760584382254, 'microRec': 0.9868115521112992, 'weightRec': 0.9868115521112992, 'macroF1': 0.8585351927600802, 'microF1': 0.9868115521112992, 'weightF1': 0.9873819975229571, 'subsetAcc': 0.9868115521112992, 'IoU': 0.7768819566435514}\n","output_type":"stream"},{"name":"stderr","text":"epoch 5/30:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 106/120 [02:19<00:18,  1.32s/it, loss=0.109] /tmp/ipykernel_31/2493405400.py:5: RuntimeWarning: invalid value encountered in greater\n  invalid_mask |= image > 1\nepoch 5/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:36<00:00,  1.31s/it, loss=0.395] \n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\n{'macroPrec': 0.7214726966330601, 'microPrec': 0.9733369224121443, 'weightPrec': 0.9859412784651994, 'macroRec': 0.9428920351584213, 'microRec': 0.9733369224121443, 'weightRec': 0.9733369224121443, 'macroF1': 0.7920630398361085, 'microF1': 0.9733369224121443, 'weightF1': 0.9777620538882531, 'subsetAcc': 0.9733369224121443, 'IoU': 0.6996222427301875}\n","output_type":"stream"},{"name":"stderr","text":"epoch 6/30:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 45/120 [00:59<01:40,  1.34s/it, loss=0.0868]/tmp/ipykernel_31/2493405400.py:5: RuntimeWarning: invalid value encountered in greater\n  invalid_mask |= image > 1\nepoch 6/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:37<00:00,  1.31s/it, loss=0.0703]\n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\n{'macroPrec': 0.8304474079465927, 'microPrec': 0.9870028122321473, 'weightPrec': 0.9885815135307731, 'macroRec': 0.900985975046958, 'microRec': 0.9870028122321473, 'weightRec': 0.9870028122321473, 'macroF1': 0.8621068182507771, 'microF1': 0.9870028122321473, 'weightF1': 0.9876293773438131, 'subsetAcc': 0.9870028122321473, 'IoU': 0.7813278858339494}\n","output_type":"stream"},{"name":"stderr","text":"epoch 7/30:  16%|â–ˆâ–Œ        | 19/120 [00:25<02:15,  1.34s/it, loss=0.116] /tmp/ipykernel_31/2493405400.py:5: RuntimeWarning: invalid value encountered in greater\n  invalid_mask |= image > 1\nepoch 7/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:37<00:00,  1.32s/it, loss=0.16]  \n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\n{'macroPrec': 0.8062619582648308, 'microPrec': 0.9849357861853523, 'weightPrec': 0.9875604951655406, 'macroRec': 0.9008525449932168, 'microRec': 0.9849357861853523, 'weightRec': 0.9849357861853523, 'macroF1': 0.8468175873130594, 'microF1': 0.9849357861853523, 'weightF1': 0.9859428637066713, 'subsetAcc': 0.9849357861853523, 'IoU': 0.762369886393308}\n","output_type":"stream"},{"name":"stderr","text":"epoch 8/30:  22%|â–ˆâ–ˆâ–       | 26/120 [00:34<02:04,  1.33s/it, loss=0.0836]/tmp/ipykernel_31/2493405400.py:5: RuntimeWarning: invalid value encountered in greater\n  invalid_mask |= image > 1\nepoch 8/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:38<00:00,  1.32s/it, loss=0.193] \n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\n{'macroPrec': 0.8432240808519205, 'microPrec': 0.986862554810192, 'weightPrec': 0.9870410673488468, 'macroRec': 0.8523734218764348, 'microRec': 0.986862554810192, 'weightRec': 0.986862554810192, 'macroF1': 0.8477351174919954, 'microF1': 0.986862554810192, 'weightF1': 0.98694895425445, 'subsetAcc': 0.986862554810192, 'IoU': 0.7638550390242833}\n","output_type":"stream"},{"name":"stderr","text":"epoch 9/30:   8%|â–Š         | 10/120 [00:13<02:24,  1.32s/it, loss=0.0976]/tmp/ipykernel_31/2493405400.py:5: RuntimeWarning: invalid value encountered in greater\n  invalid_mask |= image > 1\nepoch 9/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:37<00:00,  1.32s/it, loss=0.0891]\n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\n{'macroPrec': 0.741203985610704, 'microPrec': 0.9768745262770155, 'weightPrec': 0.9873918827589045, 'macroRec': 0.9579096630522819, 'microRec': 0.9768745262770155, 'weightRec': 0.9768745262770155, 'macroF1': 0.8132194949499056, 'microF1': 0.9768745262770155, 'weightF1': 0.9804465677096167, 'subsetAcc': 0.9768745262770155, 'IoU': 0.7226175899997852}\n","output_type":"stream"},{"name":"stderr","text":"epoch 10/30:  31%|â–ˆâ–ˆâ–ˆ       | 37/120 [00:49<01:50,  1.33s/it, loss=0.0526]/tmp/ipykernel_31/2493405400.py:5: RuntimeWarning: invalid value encountered in greater\n  invalid_mask |= image > 1\nepoch 10/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:39<00:00,  1.33s/it, loss=0.128] \n","output_type":"stream"},{"name":"stdout","text":"{'macroPrec': 0.7683597270969219, 'microPrec': 0.9795061537278222, 'weightPrec': 0.9877115484649577, 'macroRec': 0.9619192300174098, 'microRec': 0.9795061537278222, 'weightRec': 0.9795061537278222, 'macroF1': 0.8373729716366622, 'microF1': 0.9795061537278222, 'weightF1': 0.9822152445771444, 'subsetAcc': 0.9795061537278222, 'IoU': 0.7501708312381097}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/2493405400.py:5: RuntimeWarning: invalid value encountered in greater\n  invalid_mask |= image > 1\n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\n{'macroPrec': 0.8423053091116409, 'microPrec': 0.9881305385743329, 'weightPrec': 0.9895120593392289, 'macroRec': 0.9110159129570583, 'microRec': 0.9881305385743329, 'weightRec': 0.9881305385743329, 'macroF1': 0.873339280442998, 'microF1': 0.9881305385743329, 'weightF1': 0.9886717303818092, 'subsetAcc': 0.9881305385743329, 'IoU': 0.7957258883532594}\n","output_type":"stream"},{"name":"stderr","text":"epoch 11/30:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 55/120 [01:12<01:25,  1.31s/it, loss=0.227] /tmp/ipykernel_31/2493405400.py:5: RuntimeWarning: invalid value encountered in greater\n  invalid_mask |= image > 1\nepoch 11/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:38<00:00,  1.32s/it, loss=0.57]  \n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\n{'macroPrec': 0.759683118093358, 'microPrec': 0.979750511797916, 'weightPrec': 0.983101487854572, 'macroRec': 0.8427542469868469, 'microRec': 0.979750511797916, 'weightRec': 0.979750511797916, 'macroF1': 0.7950770727716553, 'microF1': 0.979750511797916, 'weightF1': 0.9811459033154277, 'subsetAcc': 0.979750511797916, 'IoU': 0.7042812078728516}\n","output_type":"stream"},{"name":"stderr","text":"epoch 12/30:  24%|â–ˆâ–ˆâ–       | 29/120 [00:38<02:00,  1.32s/it, loss=0.166] /tmp/ipykernel_31/2493405400.py:5: RuntimeWarning: invalid value encountered in greater\n  invalid_mask |= image > 1\nepoch 12/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:38<00:00,  1.32s/it, loss=0.216] \n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\n{'macroPrec': 0.825029672825959, 'microPrec': 0.9860875971353484, 'weightPrec': 0.9872708941652028, 'macroRec': 0.8757862202333815, 'microRec': 0.9860875971353484, 'weightRec': 0.9860875971353484, 'macroF1': 0.8484574719839837, 'microF1': 0.9860875971353484, 'weightF1': 0.9865913328256588, 'subsetAcc': 0.9860875971353484, 'IoU': 0.7645546607024434}\n","output_type":"stream"},{"name":"stderr","text":"epoch 13/30:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 57/120 [01:15<01:23,  1.32s/it, loss=0.0837]/tmp/ipykernel_31/2493405400.py:5: RuntimeWarning: invalid value encountered in greater\n  invalid_mask |= image > 1\nepoch 13/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:37<00:00,  1.31s/it, loss=0.202] \n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\n{'macroPrec': 0.7588902806018363, 'microPrec': 0.979734927639921, 'weightPrec': 0.9857138809202162, 'macroRec': 0.9036053851818977, 'microRec': 0.979734927639921, 'weightRec': 0.979734927639921, 'macroF1': 0.8142543352227103, 'microF1': 0.979734927639921, 'weightF1': 0.9819450480509633, 'subsetAcc': 0.979734927639921, 'IoU': 0.7244005684423471}\n","output_type":"stream"},{"name":"stderr","text":"epoch 14/30:  29%|â–ˆâ–ˆâ–‰       | 35/120 [00:46<01:51,  1.32s/it, loss=0.0749]/tmp/ipykernel_31/2493405400.py:5: RuntimeWarning: invalid value encountered in greater\n  invalid_mask |= image > 1\nepoch 14/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:37<00:00,  1.31s/it, loss=0.141] \n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\n{'macroPrec': 0.8126813452750962, 'microPrec': 0.9861343496093335, 'weightPrec': 0.9895072920974555, 'macroRec': 0.9396294170294288, 'microRec': 0.9861343496093335, 'weightRec': 0.9861343496093335, 'macroF1': 0.8647419513600556, 'microF1': 0.9861343496093335, 'weightF1': 0.9873032739093467, 'subsetAcc': 0.9861343496093335, 'IoU': 0.7844476556952917}\n","output_type":"stream"},{"name":"stderr","text":"epoch 15/30:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 52/120 [01:09<01:30,  1.33s/it, loss=0.0995]/tmp/ipykernel_31/2493405400.py:5: RuntimeWarning: invalid value encountered in greater\n  invalid_mask |= image > 1\nepoch 15/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:39<00:00,  1.33s/it, loss=0.0929]\n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\n{'macroPrec': 0.7603655264621089, 'microPrec': 0.9799701067514822, 'weightPrec': 0.9881345105064392, 'macroRec': 0.9573274368460899, 'microRec': 0.9799701067514822, 'weightRec': 0.9799701067514822, 'macroF1': 0.8297417747835893, 'microF1': 0.9799701067514822, 'weightF1': 0.9827117188077206, 'subsetAcc': 0.9799701067514822, 'IoU': 0.741550668336961}\n","output_type":"stream"},{"name":"stderr","text":"epoch 16/30:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 83/120 [01:50<00:49,  1.33s/it, loss=0.102] /tmp/ipykernel_31/2493405400.py:5: RuntimeWarning: invalid value encountered in greater\n  invalid_mask |= image > 1\nepoch 16/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:38<00:00,  1.32s/it, loss=0.0815]\n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\n{'macroPrec': 0.8257375217633102, 'microPrec': 0.9874760039385417, 'weightPrec': 0.9903303601588304, 'macroRec': 0.9460764095016057, 'microRec': 0.9874760039385417, 'weightRec': 0.9874760039385417, 'macroF1': 0.8759282437864566, 'microF1': 0.9874760039385417, 'weightF1': 0.9884516903743165, 'subsetAcc': 0.9874760039385417, 'IoU': 0.7989503191006054}\n","output_type":"stream"},{"name":"stderr","text":"epoch 17/30:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 44/120 [00:58<01:41,  1.33s/it, loss=0.0986]/tmp/ipykernel_31/2493405400.py:5: RuntimeWarning: invalid value encountered in greater\n  invalid_mask |= image > 1\nepoch 17/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:38<00:00,  1.32s/it, loss=0.102] \n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\n{'macroPrec': 0.8829037482498396, 'microPrec': 0.9885144755576649, 'weightPrec': 0.9878661630627272, 'macroRec': 0.8319234344639976, 'microRec': 0.9885144755576649, 'weightRec': 0.9885144755576649, 'macroF1': 0.8555017513719353, 'microF1': 0.9885144755576649, 'weightF1': 0.9881049560424653, 'subsetAcc': 0.9885144755576649, 'IoU': 0.7735138232668023}\n","output_type":"stream"},{"name":"stderr","text":"epoch 18/30:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 117/120 [02:35<00:04,  1.34s/it, loss=0.0824]/tmp/ipykernel_31/2493405400.py:5: RuntimeWarning: invalid value encountered in greater\n  invalid_mask |= image > 1\nepoch 18/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:38<00:00,  1.32s/it, loss=0.102] \n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\n{'macroPrec': 0.8480879087417952, 'microPrec': 0.9882382109386622, 'weightPrec': 0.989171065366306, 'macroRec': 0.8973840143631757, 'microRec': 0.9882382109386622, 'weightRec': 0.9882382109386622, 'macroF1': 0.8710105243768678, 'microF1': 0.9882382109386622, 'weightF1': 0.9886270564441362, 'subsetAcc': 0.9882382109386622, 'IoU': 0.792763752411922}\n","output_type":"stream"},{"name":"stderr","text":"epoch 19/30:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 64/120 [01:25<01:15,  1.35s/it, loss=0.106] /tmp/ipykernel_31/2493405400.py:5: RuntimeWarning: invalid value encountered in greater\n  invalid_mask |= image > 1\nepoch 19/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:39<00:00,  1.33s/it, loss=0.0428]\n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\n{'macroPrec': 0.8112746897738243, 'microPrec': 0.9861471002840567, 'weightPrec': 0.9898750451113504, 'macroRec': 0.9494395973102969, 'microRec': 0.9861471002840567, 'weightRec': 0.9861471002840567, 'macroF1': 0.8669796627834931, 'microF1': 0.9861471002840567, 'weightF1': 0.98740391459943, 'subsetAcc': 0.9861471002840567, 'IoU': 0.7872626844428303}\n","output_type":"stream"},{"name":"stderr","text":"epoch 20/30:  10%|â–ˆ         | 12/120 [00:16<02:25,  1.34s/it, loss=0.135] /tmp/ipykernel_31/2493405400.py:5: RuntimeWarning: invalid value encountered in greater\n  invalid_mask |= image > 1\nepoch 20/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:39<00:00,  1.33s/it, loss=0.0985]\n","output_type":"stream"},{"name":"stdout","text":"{'macroPrec': 0.7812816967179355, 'microPrec': 0.9814361002472006, 'weightPrec': 0.9885163295453285, 'macroRec': 0.9668835210093809, 'microRec': 0.9814361002472006, 'weightRec': 0.9814361002472006, 'macroF1': 0.8492302269283183, 'microF1': 0.9814361002472006, 'weightF1': 0.9837337759677555, 'subsetAcc': 0.9814361002472006, 'IoU': 0.7645263751478732}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/2493405400.py:5: RuntimeWarning: invalid value encountered in greater\n  invalid_mask |= image > 1\n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\n{'macroPrec': 0.7623249269986121, 'microPrec': 0.9802704559782955, 'weightPrec': 0.9887168055340849, 'macroRec': 0.9687169684182605, 'microRec': 0.9802704559782955, 'weightRec': 0.9802704559782955, 'macroF1': 0.8341477331333056, 'microF1': 0.9802704559782955, 'weightF1': 0.9830480830119825, 'subsetAcc': 0.9802704559782955, 'IoU': 0.7466256100037287}\n","output_type":"stream"},{"name":"stderr","text":"epoch 21/30:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 90/120 [01:59<00:39,  1.32s/it, loss=0.0522]/tmp/ipykernel_31/2493405400.py:5: RuntimeWarning: invalid value encountered in greater\n  invalid_mask |= image > 1\nepoch 21/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:37<00:00,  1.32s/it, loss=0.131] \n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\n{'macroPrec': 0.7833013852744681, 'microPrec': 0.9827412533913253, 'weightPrec': 0.9871310698461134, 'macroRec': 0.9130040125919762, 'microRec': 0.9827412533913253, 'weightRec': 0.9827412533913253, 'macroF1': 0.8352258887161801, 'microF1': 0.9827412533913253, 'weightF1': 0.984347041308781, 'subsetAcc': 0.9827412533913253, 'IoU': 0.7483955116000515}\n","output_type":"stream"},{"name":"stderr","text":"epoch 22/30:   6%|â–Œ         | 7/120 [00:09<02:31,  1.34s/it, loss=0.0843]/tmp/ipykernel_31/2493405400.py:5: RuntimeWarning: invalid value encountered in greater\n  invalid_mask |= image > 1\nepoch 22/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:41<00:00,  1.34s/it, loss=0.111] \n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\n{'macroPrec': 0.7954423243423113, 'microPrec': 0.9845688501016512, 'weightPrec': 0.9896370993885479, 'macroRec': 0.9589776935243659, 'microRec': 0.9845688501016512, 'weightRec': 0.9845688501016512, 'macroF1': 0.8582553269822328, 'microF1': 0.9845688501016512, 'weightF1': 0.9862389592819394, 'subsetAcc': 0.9845688501016512, 'IoU': 0.7760992880625648}\n","output_type":"stream"},{"name":"stderr","text":"epoch 23/30:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 44/120 [00:58<01:41,  1.34s/it, loss=0.163] /tmp/ipykernel_31/2493405400.py:5: RuntimeWarning: invalid value encountered in greater\n  invalid_mask |= image > 1\nepoch 23/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:39<00:00,  1.33s/it, loss=0.0525]\n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\n{'macroPrec': 0.8163941547721898, 'microPrec': 0.986543787942112, 'weightPrec': 0.9897964600980873, 'macroRec': 0.9427352267528569, 'microRec': 0.986543787942112, 'weightRec': 0.986543787942112, 'macroF1': 0.8683671396571463, 'microF1': 0.986543787942112, 'weightF1': 0.9876626577556429, 'subsetAcc': 0.986543787942112, 'IoU': 0.7890905325510985}\n","output_type":"stream"},{"name":"stderr","text":"epoch 24/30:   3%|â–Ž         | 4/120 [00:05<02:33,  1.32s/it, loss=0.0939]/tmp/ipykernel_31/2493405400.py:5: RuntimeWarning: invalid value encountered in greater\n  invalid_mask |= image > 1\nepoch 24/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:38<00:00,  1.32s/it, loss=0.0798]\n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\n{'macroPrec': 0.7696632727579982, 'microPrec': 0.9813273452386856, 'weightPrec': 0.9887925685265664, 'macroRec': 0.9637187345721362, 'microRec': 0.9813273452386856, 'weightRec': 0.9813273452386856, 'macroF1': 0.83909336961762, 'microF1': 0.9813273452386856, 'weightF1': 0.9837921194561936, 'subsetAcc': 0.9813273452386856, 'IoU': 0.7525528679962301}\n","output_type":"stream"},{"name":"stderr","text":"epoch 25/30:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 62/120 [01:22<01:17,  1.33s/it, loss=0.177] /tmp/ipykernel_31/2493405400.py:5: RuntimeWarning: invalid value encountered in greater\n  invalid_mask |= image > 1\nepoch 25/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:37<00:00,  1.31s/it, loss=0.0734]\n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\n{'macroPrec': 0.841994444541895, 'microPrec': 0.9887709057937649, 'weightPrec': 0.9907879457327673, 'macroRec': 0.9409451884987441, 'microRec': 0.9887709057937649, 'weightRec': 0.9887709057937649, 'macroF1': 0.8848353671299538, 'microF1': 0.9887709057937649, 'weightF1': 0.9894801867059186, 'subsetAcc': 0.9887709057937649, 'IoU': 0.8108871328021097}\n","output_type":"stream"},{"name":"stderr","text":"epoch 26/30:   8%|â–Š         | 9/120 [00:11<02:24,  1.30s/it, loss=0.077] /tmp/ipykernel_31/2493405400.py:5: RuntimeWarning: invalid value encountered in greater\n  invalid_mask |= image > 1\nepoch 26/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:37<00:00,  1.31s/it, loss=1.61]  \n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\n{'macroPrec': 0.8685085457079849, 'microPrec': 0.9898192946043395, 'weightPrec': 0.9903941252439711, 'macroRec': 0.9057676986880282, 'microRec': 0.9898192946043395, 'weightRec': 0.9898192946043395, 'macroF1': 0.8861912449202807, 'microF1': 0.9898192946043395, 'weightF1': 0.990064234889695, 'subsetAcc': 0.9898192946043395, 'IoU': 0.8128749370382383}\n","output_type":"stream"},{"name":"stderr","text":"epoch 27/30:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 111/120 [02:26<00:11,  1.33s/it, loss=0.102] /tmp/ipykernel_31/2493405400.py:5: RuntimeWarning: invalid value encountered in greater\n  invalid_mask |= image > 1\nepoch 27/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:37<00:00,  1.31s/it, loss=0.0824]\n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\n{'macroPrec': 0.8233153000386644, 'microPrec': 0.9867591326707705, 'weightPrec': 0.9890569850326878, 'macroRec': 0.918972737826548, 'microRec': 0.9867591326707705, 'weightRec': 0.9867591326707705, 'macroF1': 0.8645915368593295, 'microF1': 0.9867591326707705, 'weightF1': 0.9876119134996764, 'subsetAcc': 0.9867591326707705, 'IoU': 0.7843779247115112}\n","output_type":"stream"},{"name":"stderr","text":"epoch 28/30:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 55/120 [01:13<01:26,  1.34s/it, loss=0.0744]/tmp/ipykernel_31/2493405400.py:5: RuntimeWarning: invalid value encountered in greater\n  invalid_mask |= image > 1\nepoch 28/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:37<00:00,  1.31s/it, loss=0.0494]\n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\n{'macroPrec': 0.8244799430946932, 'microPrec': 0.9871402361708307, 'weightPrec': 0.9897592365686588, 'macroRec': 0.9340958333864748, 'microRec': 0.9871402361708307, 'weightRec': 0.9871402361708307, 'macroF1': 0.8708655165189632, 'microF1': 0.9871402361708307, 'weightF1': 0.988068832624309, 'subsetAcc': 0.9871402361708307, 'IoU': 0.7923770949963399}\n","output_type":"stream"},{"name":"stderr","text":"epoch 29/30:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 46/120 [01:00<01:39,  1.34s/it, loss=0.0545]/tmp/ipykernel_31/2493405400.py:5: RuntimeWarning: invalid value encountered in greater\n  invalid_mask |= image > 1\nepoch 29/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:38<00:00,  1.32s/it, loss=0.00737]\n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\n{'macroPrec': 0.8027491491842487, 'microPrec': 0.9852219679958064, 'weightPrec': 0.989389201946361, 'macroRec': 0.9463885058757027, 'microRec': 0.9852219679958064, 'weightRec': 0.9852219679958064, 'macroF1': 0.8598674673522029, 'microF1': 0.9852219679958064, 'weightF1': 0.9866372032020722, 'subsetAcc': 0.9852219679958064, 'IoU': 0.7782119172697235}\n","output_type":"stream"},{"name":"stderr","text":"epoch 30/30:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 80/120 [01:46<00:53,  1.33s/it, loss=0.0559]/tmp/ipykernel_31/2493405400.py:5: RuntimeWarning: invalid value encountered in greater\n  invalid_mask |= image > 1\nepoch 30/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:38<00:00,  1.32s/it, loss=0.162] \n","output_type":"stream"},{"name":"stdout","text":"{'macroPrec': 0.7949022986753064, 'microPrec': 0.9832922954996636, 'weightPrec': 0.9892278653011504, 'macroRec': 0.9691895384469061, 'microRec': 0.9832922954996636, 'weightRec': 0.9832922954996636, 'macroF1': 0.8606463963789573, 'microF1': 0.9832922954996636, 'weightF1': 0.9851944216858431, 'subsetAcc': 0.9832922954996636, 'IoU': 0.7788010027953466}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/2493405400.py:5: RuntimeWarning: invalid value encountered in greater\n  invalid_mask |= image > 1\n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\n{'macroPrec': 0.8364416811801099, 'microPrec': 0.9881404557657842, 'weightPrec': 0.9902034364782886, 'macroRec': 0.9327927532217986, 'microRec': 0.9881404557657842, 'weightRec': 0.9881404557657842, 'macroF1': 0.8782114963513497, 'microF1': 0.9881404557657842, 'weightF1': 0.9888828800754125, 'subsetAcc': 0.9881404557657842, 'IoU': 0.8020389634856595}\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"\n\n# Save everything in a checkpoint\ncheckpoint = {\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'scheduler_state_dict': scheduler.state_dict(),\n    'epoch': 10  # Optional: Save the epoch number\n}\n\ntorch.save(checkpoint, 'model_checkpoint_30_epochs.pth')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T22:00:28.600679Z","iopub.execute_input":"2025-05-18T22:00:28.600968Z","iopub.status.idle":"2025-05-18T22:00:28.951552Z","shell.execute_reply.started":"2025-05-18T22:00:28.600950Z","shell.execute_reply":"2025-05-18T22:00:28.950993Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"marida_test_df = create_marida_df(MARIDA_path, 'test')\nempty_df =  pd.DataFrame(columns=marida_test_df.columns)\nmarida_test_ds = MergedSegmentationDataset(marida_test_df, empty_df, global_bands_mean, global_bands_std, transform=transformTest, standardization= standardization )\n\nmarida_testLoader = DataLoader(marida_test_ds, \n                        batch_size=batch_size, \n                        shuffle=False,\n                        collate_fn=custom_collate_fn,\n                        #worker_init_fn=worker_init_fn,\n                        #generator=torch.Generator().manual_seed(seed) \n                        )\n\ntest_metrics_history = []\nmodel.eval()\nyTrue = []\nyPredicted = []\ntestLossF = []\nwith torch.no_grad():\n    for image, target, _ in marida_testLoader:\n\n        image, target = image.to(device), target.to(device)\n        logits = model(image)\n        # print(f'image dtype {image.dtype}')\n        # print(f'logits dtype {logits.dtype}')\n        # print(f'target dtype {target.dtype}')\n        # print(f'test - target shape {target.shape}')\n        #print(f'test - logit shape {logits.shape}')\n        loss = criterion(logits, target)\n\n        logits = torch.movedim(logits, (0,1,2,3), (0,3,1,2))\n        logits = logits.reshape((-1,output_classes))\n        target = target.reshape(-1)\n        ###################################################################################\n        mask = target != -1\n        ###################################################################################\n        \n        # bg_logits = logits[~mask]\n        # bg_target = target[~mask]\n        \n        logits = logits[mask]\n        target = target[mask]\n        \n\n        probs = F.softmax(logits, dim=1).cpu().numpy()\n        print(f'test - probs shape {probs.shape}')\n        target = target.cpu().numpy()\n        # testBatches += target.shape[0]\n        testLossF.append((loss.data*target.shape[0]).tolist())\n        yPredicted += probs.argmax(1).tolist()\n        yTrue += target.tolist()\n\n\n        # bg_probs = torch.nn.functional.softmax(bg_logits, dim=1).cpu().numpy()\n        # bg_target = bg_target.cpu().numpy()\n\n        # bg_yPredicted += bg_probs.argmax(1).tolist()\n        # bg_yTrue += bg_target.tolist()\n    \n    yPredicted = np.asarray(yPredicted)\n    yTrue = np.asarray(yTrue)\n    acc = Evaluation(yPredicted, yTrue)\n    test_metrics_history.append(acc)\n\n    # bg_yPredicted = np.asarray(bg_yPredicted)\n    # bg_yTrue = np.asarray(bg_yTrue)\n    # bg_acc = Evaluation(bg_yPredicted, bg_yTrue)\n    print(acc)\n                    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T22:00:36.964029Z","iopub.execute_input":"2025-05-18T22:00:36.964348Z","iopub.status.idle":"2025-05-18T22:01:28.292807Z","shell.execute_reply.started":"2025-05-18T22:00:36.964328Z","shell.execute_reply":"2025-05-18T22:01:28.292112Z"}},"outputs":[{"name":"stdout","text":"test - probs shape (7177, 2)\ntest - probs shape (17871, 2)\ntest - probs shape (2032, 2)\ntest - probs shape (4769, 2)\ntest - probs shape (3834, 2)\ntest - probs shape (3491, 2)\ntest - probs shape (18137, 2)\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/2493405400.py:5: RuntimeWarning: invalid value encountered in greater\n  invalid_mask |= image > 1\n","output_type":"stream"},{"name":"stdout","text":"test - probs shape (21602, 2)\ntest - probs shape (9102, 2)\ntest - probs shape (7595, 2)\ntest - probs shape (5344, 2)\ntest - probs shape (14574, 2)\ntest - probs shape (1183, 2)\ntest - probs shape (19516, 2)\ntest - probs shape (6836, 2)\ntest - probs shape (3626, 2)\ntest - probs shape (1100, 2)\ntest - probs shape (11923, 2)\ntest - probs shape (23276, 2)\ntest - probs shape (5089, 2)\ntest - probs shape (1839, 2)\ntest - probs shape (4423, 2)\ntest - probs shape (524, 2)\n{'macroPrec': 0.7237010091998346, 'microPrec': 0.9850459040454063, 'weightPrec': 0.9908632264056609, 'macroRec': 0.907481110001789, 'microRec': 0.9850459040454063, 'weightRec': 0.9850459040454063, 'macroF1': 0.7875735559452557, 'microF1': 0.9850459040454063, 'weightF1': 0.9872194988371132, 'subsetAcc': 0.9850459040454063, 'IoU': 0.6980411337174643}\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"# All black\n# /kaggle/input/litter-windrows-patches/patches/S2A_MSIL1C_20180916T101021_R022_T33TUL/S2A_MSIL1C_20180916T101021_R022_T33TUL_366560_5053920.tif","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T15:18:36.369445Z","iopub.status.idle":"2025-05-18T15:18:36.369742Z","shell.execute_reply.started":"2025-05-18T15:18:36.369574Z","shell.execute_reply":"2025-05-18T15:18:36.369590Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Lightning implementation. To be used later.\n\n# class BinaryClassificationModel(pl.LightningModule):\n#     def __init__(self, hparams):\n#         super().__init__()\n#         self.save_hyperparameters(hparams)\n\n#         # Model selection\n#         if hparams.model_name == \"resattunet\":\n#             self.model = ResidualAttentionUNet(11, 11)\n#             # Modify for binary classification\n#             self.model.decoder = nn.Sequential(\n#                 self.model.decoder,\n#                 nn.AdaptiveAvgPool2d(1),\n#                 nn.Flatten(),\n#                 nn.Linear(11, 2)  # Binary output\n#             )\n#         elif hparams.model_name == \"attunet\":\n#             self.model = AttentionUNet(11, 11)\n#             self.model.decoder = nn.Sequential(\n#                 self.model.decoder,\n#                 nn.AdaptiveAvgPool2d(1),\n#                 nn.Flatten(),\n#                 nn.Linear(11, 2)\n#             )\n#         elif hparams.model_name == \"unet\":\n#             self.model = UNet(11, 11)\n#             self.model.decoder = nn.Sequential(\n#                 self.model.decoder,\n#                 nn.AdaptiveAvgPool2d(1),\n#                 nn.Flatten(),\n#                 nn.Linear(11, 2)\n#             )\n#         else:\n#             raise ValueError(\"Invalid model name\")\n\n#         # Loss function\n#         if hparams.focal_loss:\n#             self.criterion = FocalLoss()\n#         else:\n#             weight = gen_weights(class_distr, c=1.03)[:2]  # Binary classes\n#             self.criterion = nn.CrossEntropyLoss(weight=weight, ignore_index=-1)\n\n#         # Track best metrics\n#         self.best_macro_f1 = 0.0\n#         self.best_micro_f1 = 0.0\n#         self.best_weight_f1 = 0.0\n\n#     def forward(self, x):\n#         return self.model(x)\n\n#     def training_step(self, batch, batch_idx):\n#         images, labels, _ = batch\n#         logits = self(images)\n#         loss = self.criterion(logits, labels)\n#         self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n#         return loss\n\n#     def validation_step(self, batch, batch_idx):\n#         images, labels, _ = batch\n#         logits = self(images)\n#         loss = self.criterion(logits, labels)\n#         probs = torch.softmax(logits, dim=1).cpu().numpy()\n#         labels = labels.cpu().numpy()\n#         preds = probs.argmax(1)\n#         return {\"loss\": loss, \"preds\": preds.tolist(), \"labels\": labels.tolist()}\n\n#     def validation_epoch_end(self, outputs):\n#         preds = np.concatenate([o[\"preds\"] for o in outputs])\n#         labels = np.concatenate([o[\"labels\"] for o in outputs])\n#         loss = torch.stack([o[\"loss\"] for o in outputs]).mean()\n#         acc = Evaluation(preds, labels)\n\n#         self.log(\"val_loss\", loss, prog_bar=True)\n#         self.log(\"val_macro_precision\", acc[\"macroPrec\"], prog_bar=True)\n#         self.log(\"val_macro_recall\", acc[\"macroRec\"])\n#         self.log(\"val_macro_f1\", acc[\"macroF1\"])\n#         self.log(\"val_micro_precision\", acc[\"microPrec\"])\n#         self.log(\"val_micro_recall\", acc[\"microRec\"])\n#         self.log(\"val_micro_f1\", acc[\"microF1\"])\n#         self.log(\"val_weight_precision\", acc[\"weightPrec\"])\n#         self.log(\"val_weight_recall\", acc[\"weightRec\"])\n#         self.log(\"val_weight_f1\", acc[\"weightF1\"])\n#         self.log(\"val_iou\", acc[\"IoU\"])\n\n#         # Update best metrics\n#         if acc[\"macroF1\"] > self.best_macro_f1:\n#             self.best_macro_f1 = acc[\"macroF1\"]\n#         if acc[\"microF1\"] > self.best_micro_f1:\n#             self.best_micro_f1 = acc[\"microF1\"]\n#         if acc[\"weightF1\"] > self.best_weight_f1:\n#             self.best_weight_f1 = acc[\"weightF1\"]\n\n#     def configure_optimizers(self):\n#         optimizer = optim.Adam(\n#             self.parameters(),\n#             lr=self.hparams.initial_lr,\n#             weight_decay=self.hparams.decay_lr\n#         )\n#         if self.hparams.scheduler_lr == \"rop\":\n#             scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n#                 optimizer, mode=\"min\", factor=0.1, patience=10, verbose=True\n#             )\n#             return {\n#                 \"optimizer\": optimizer,\n#                 \"lr_scheduler\": scheduler,\n#                 \"monitor\": \"val_loss\"\n#             }\n#         else:\n#             scheduler = optim.lr_scheduler.MultiStepLR(\n#                 optimizer, milestones=[40, 80, 120, 160], gamma=0.5, verbose=True\n#             )\n#             return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n\n#     def train_dataloader(self):\n#         transform = transforms.Compose([\n#             transforms.ToTensor(),\n#             RandomRotationTransform([-90, 0, 90, 180]),\n#             transforms.RandomHorizontalFlip(),\n#             transforms.Normalize(bands_mean, bands_std)\n#         ])\n#         dataset = MergedSegmentationDataset(\n#             dataset1_paths=(\"path/to/dataset1/images\", \"path/to/dataset1/masks\"),\n#             dataset2_paths=(\"path/to/dataset2/images\", \"path/to/dataset2/masks\"),\n#             transform=transform\n#         )\n#         return DataLoader(\n#             dataset,\n#             batch_size=self.hparams.train_batch_size,\n#             shuffle=True,\n#             num_workers=4,\n#             worker_init_fn=seed_worker,\n#             generator=torch.Generator().manual_seed(0)\n#         )\n\n#     def val_dataloader(self):\n#         transform = transforms.Compose([\n#             transforms.ToTensor(),\n#             transforms.Normalize(bands_mean, bands_std)\n#         ])\n#         dataset = MergedSegmentationDataset(\n#             dataset1_paths=(\"path/to/dataset1/images\", \"path/to/dataset1/masks\"),\n#             dataset2_paths=(\"path/to/dataset2/images\", \"path/to/dataset2/masks\"),\n#             transform=transform\n#         )\n#         return DataLoader(\n#             dataset,\n#             batch_size=self.hparams.test_batch_size,\n#             shuffle=False,\n#             num_workers=4,\n#             worker_init_fn=seed_worker,\n#             generator=torch.Generator().manual_seed(0)\n#         )\n\n# def seed_worker(worker_id):\n#     worker_seed = torch.initial_seed() % 2**32\n#     np.random.seed(worker_seed)\n#     random.seed(worker_seed)\n\n# def main():\n#     parser = argparse.ArgumentParser()\n#     parser.add_argument('--train_batch_size', type=int, default=8)\n#     parser.add_argument('--test_batch_size', type=int, default=4)\n#     parser.add_argument('--total_epochs', type=int, default=50)\n#     parser.add_argument('--experiment_name', type=str, required=True)\n#     parser.add_argument('--initial_lr', type=float, default=1e-3)\n#     parser.add_argument('--decay_lr', type=float, default=0)\n#     parser.add_argument('--scheduler_lr', type=str, default=\"ms\")\n#     parser.add_argument('--focal_loss', type=bool, default=False)\n#     parser.add_argument('--model_name', type=str, default=\"resattunet\")\n#     args = parser.parse_args()\n\n#     # Set seeds for reproducibility\n#     pl.seed_everything(0, workers=True)\n\n#     # Initialize model\n#     model = BinaryClassificationModel(args)\n\n#     # Logger\n#     logger = TensorBoardLogger(save_dir=args.experiment_name, name=\"logs\")\n\n#     # Callbacks for saving best models\n#     checkpoint_macro = ModelCheckpoint(\n#         dirpath=args.experiment_name,\n#         filename=\"bestMacroF1Model\",\n#         monitor=\"val_macro_f1\",\n#         mode=\"max\",\n#         save_top_k=1\n#     )\n#     checkpoint_micro = ModelCheckpoint(\n#         dirpath=args.experiment_name,\n#         filename=\"bestMicroF1Model\",\n#         monitor=\"val_micro_f1\",\n#         mode=\"max\",\n#         save_top_k=1\n#     )\n#     checkpoint_weight = ModelCheckpoint(\n#         dirpath=args.experiment_name,\n#         filename=\"bestWeightF1Model\",\n#         monitor=\"val_weight_f1\",\n#         mode=\"max\",\n#         save_top_k=1\n#     )\n\n#     # Trainer\n#     trainer = pl.Trainer(\n#         max_epochs=args.total_epochs,\n#         accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n#         devices=1,\n#         logger=logger,\n#         callbacks=[checkpoint_macro, checkpoint_micro, checkpoint_weight],\n#         deterministic=True\n#     )\n\n#     # Train\n#     trainer.fit(model)\n\n# # if __name__ == \"__main__\":\n# #     main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}